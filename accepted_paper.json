{"notes":[{"content":{"title":{"value":"MAS-GPT: Training LLMs To Build LLM-Based Multi-Agent Systems"},"authors":{"value":["Rui Ye","Shuo Tang","Rui Ge","Yaxin Du","Zhenfei Yin","Jing Shao","Siheng Chen"]},"authorids":{"value":["~Rui_Ye1","~Shuo_Tang2","~Rui_Ge1","~Yaxin_Du1","~Zhenfei_Yin2","~Jing_Shao3","~Siheng_Chen1"]},"keywords":{"value":["LLMs","Multi-Agent Systems"]},"abstract":{"value":"LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 4 LLMs show that the proposed MAS-GPT consistently outperforms 1O+ baseline MAS methods on diverse settings, indicating MAS-GPT's high effectiveness, efficiency and strong generalization ability."},"pdf":{"value":"/pdf/33d8043314a5af8541f92e4c90c18c753ff5cf40.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nye2025masgpt,\ntitle={{MAS}-{GPT}: Training {LLM}s To Build {LLM}-Based Multi-Agent Systems},\nauthor={Rui Ye and Shuo Tang and Rui Ge and Yaxin Du and Zhenfei Yin and Jing Shao and Siheng Chen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=TqHoQIlumy}\n}"},"paperhash":{"value":"ye|masgpt_training_llms_to_build_llmbased_multiagent_systems"}},"id":"TqHoQIlumy","forum":"TqHoQIlumy","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission186/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission186/Authors"],"number":186,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1739033913269,"cdate":1739033913269,"tmdate":1741166413935,"mdate":1741166413935,"pdate":1741166413916,"odate":1741166413916,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers (Abridged)"},"authors":{"value":["Shalev Lifshitz","Sheila A. McIlraith","Yilun Du"]},"authorids":{"value":["~Shalev_Lifshitz1","~Sheila_A._McIlraith1","~Yilun_Du1"]},"keywords":{"value":["large language models","test-time compute","verification","scaling"]},"TLDR":{"value":"We explore scaling the number of verifier models as a novel test-time scaling dimension and introduce an algorithm that enables simple scaling along this dimension."},"abstract":{"value":"By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses *verifiers* to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: *scaling the number of verifiers*. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-*n* sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time."},"pdf":{"value":"/pdf/f7236b51731046dbeec0e2d9990ba0faadda9624.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nlifshitz2025multiagent,\ntitle={Multi-Agent Verification: Scaling Test-Time Compute with Goal Verifiers},\nauthor={Shalev Lifshitz and Sheila A. McIlraith and Yilun Du},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=H22e93wnMe}\n}"},"paperhash":{"value":"lifshitz|multiagent_verification_scaling_testtime_compute_with_multiple_verifiers_abridged"}},"id":"H22e93wnMe","forum":"H22e93wnMe","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission183/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission183/Authors"],"number":183,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission183/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738951175821,"cdate":1738951175821,"tmdate":1742439127963,"mdate":1742439127963,"pdate":1741166413705,"odate":1741166413705,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving"},"authors":{"value":["Sara Rajaee","Kumar Pratik","Gabriele Cesa","Arash Behboodi"]},"authorids":{"value":["~Sara_Rajaee2","~Kumar_Pratik1","~Gabriele_Cesa1","~Arash_Behboodi1"]},"keywords":{"value":["Theorem Proving","Large Language Model","Mathematical Reasoning"]},"abstract":{"value":"The most promising recent methods for AI reasoning require applying variants of reinforcement learning (RL) either on rolled out trajectories from the model, even for the step-wise rewards, or large quantities of human annotated trajectory data. The reliance on the rolled-out trajectory renders the compute cost and time prohibitively high. In particular, the correctness of a reasoning trajectory can typically only be judged at its completion, leading to sparse rewards in RL or requiring expensive synthetic data generation in expert iteration-like methods.\nIn this work, we focus on the Automatic Theorem Proving (ATP) task and propose a novel verifier-in-the-loop design, which unlike existing approaches that leverage feedback on the entire reasoning trajectory, employs an automated verifier to give intermediate feedback at each step of the reasoning process. Using Lean as the verifier, we empirically show that the step-by-step local verification produces a global improvement in the model's reasoning accuracy and efficiency."},"pdf":{"value":"/pdf/d5ccd12393ecbaaac77ee2938f910886f4a8a517.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nrajaee2025local,\ntitle={Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving},\nauthor={Sara Rajaee and Kumar Pratik and Gabriele Cesa and Arash Behboodi},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=e6VJwdnd8s}\n}"},"paperhash":{"value":"rajaee|local_lookahead_guidance_via_verifierintheloop_for_automated_theorem_proving"}},"id":"e6VJwdnd8s","forum":"e6VJwdnd8s","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission179/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission179/Authors"],"number":179,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission179/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738929132435,"cdate":1738929132435,"tmdate":1742300133685,"mdate":1742300133685,"pdate":1741166413511,"odate":1741166413511,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Strategic LLM Decoding through Bayesian Games"},"authors":{"value":["Weitong Zhang","Chengqi Zang","Bernhard Kainz"]},"authorids":{"value":["~Weitong_Zhang3","~Chengqi_Zang1","~Bernhard_Kainz1"]},"keywords":{"value":["Multi-agent System","Game Theory","Mechanism Design"]},"TLDR":{"value":"Strategic LLM Decoding through Bayesian Games"},"abstract":{"value":"Large Language Models (LLMs) often produce outputs that --  though plausible -- can lack consistency and reliability, particularly in ambiguous or complex scenarios. Challenges arise from ensuring that outputs align with both factual correctness and human intent. This is problematic in existing approaches that trade improved consistency for lower accuracy. To mitigate these challenges, we propose a novel game-theoretic approach to enhance consistency and reliability during the decoding stage of LLM output generation. Our method models the decoding process as a multistage Bayesian Decoding Game.\nThe strategic decoding process dynamically converges to a consensus on the most reliable outputs without human feedback or additional training. Remarkably, our game design allows smaller models to outperform much larger models through game mechanisms (\\textit{e.g.} 78.1 LLaMA13B \\textit{vs} 76.6 PaLM540B), as well as integrating various LLM strategies and models, demonstrating the potential of game-theoretic tools to improve the truthfulness and reliability of LLMs."},"pdf":{"value":"/pdf/41893cf1581654f4da3b3a1200b9f53a3c82bb17.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhang2025strategic,\ntitle={Strategic {LLM} Decoding through Bayesian Games},\nauthor={Weitong Zhang and Chengqi Zang and Bernhard Kainz},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=K6ZWCpGrGG}\n}"},"paperhash":{"value":"zhang|strategic_llm_decoding_through_bayesian_games"}},"id":"K6ZWCpGrGG","forum":"K6ZWCpGrGG","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission177/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission177/Authors"],"number":177,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission177/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738928508577,"cdate":1738928508577,"tmdate":1741709102771,"mdate":1741709102771,"pdate":1741166413427,"odate":1741166413427,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Revealing chemical reasoning in LLMs through search on complex planning tasks"},"authors":{"value":["Andres M Bran","Théo A. Neukomm","Daniel P Armstrong","Zlatko Jončev","Philippe Schwaller"]},"authorids":{"value":["~Andres_M_Bran1","~Théo_A._Neukomm1","~Daniel_P_Armstrong1","~Zlatko_Jončev1","~Philippe_Schwaller1"]},"keywords":{"value":["Machine Learning","Large language models","Chemistry","Retrosynthesis","Search"]},"abstract":{"value":"Large language models (LLMs) have been the focal point of enormous development in artificial intelligence over the past half decade, recently achieving human level performance on mathematics and programming benchmarks. In-spite of this, performance improvements on chemical tasks have emerged at a somewhat slower pace. In this work we investigate the capabilities of large language models (LLMs) in chemical search to address two central problems in AI-driven synthesis: retrosynthetic planning and mechanism elucidation. In our approach, the search environment builds options and the LLM serves as a guidance function to evaluate the validity and potential of a partially constructed solution. This is advantageous as LLMs can digest arbitrary inputs and optimize for arbitrary requirements. In this work, we show that LLMs can analyze and reason about chemical entities like molecules and reactions. We then leverage these capabilities in the context of two central problems in organic chemistry: retrosynthetic planning and mechanistic elucidation. Our results show that LLMs can accurately reason about chemical entities in both local and global terms, analyzing single reactions but also whole synthetic routes, and that such capabilities can be exploited through search algorithms for solving chemical problems in more flexible terms."},"pdf":{"value":"/pdf/2bf6e1368eaa5b06d55eada9e54b6231a5fee725.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nbran2025revealing,\ntitle={Revealing chemical reasoning in {LLM}s through search on complex planning tasks},\nauthor={Andres M Bran and Th{\\'e}o A. Neukomm and Daniel P Armstrong and Zlatko Jon{\\v{c}}ev and Philippe Schwaller},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=HnCWeXgJtZ}\n}"},"paperhash":{"value":"bran|revealing_chemical_reasoning_in_llms_through_search_on_complex_planning_tasks"}},"id":"HnCWeXgJtZ","forum":"HnCWeXgJtZ","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission174/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission174/Authors"],"number":174,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738926821194,"cdate":1738926821194,"tmdate":1741166413302,"mdate":1741166413302,"pdate":1741166413180,"odate":1741166413180,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"MastermindEval: A Simple But Scalable Reasoning Benchmark"},"authors":{"value":["Jonas Golde","Patrick Haller","Fabio Barth","Alan Akbik"]},"authorids":{"value":["~Jonas_Golde1","~Patrick_Haller2","~Fabio_Barth1","~Alan_Akbik2"]},"keywords":{"value":["Deductive Reasoning","Board Games"]},"TLDR":{"value":"We introduce MastermindEval, a benchmark based on the boardgame Mastermind in which language models need to deduce a concealed code by interacting"},"abstract":{"value":"Recent advancements in large language models (LLMs) have led to remarkable performance across a wide range of language understanding and mathematical tasks. As a result, increasing attention has been given to assessing the true reasoning capabilities of LLMs, driving research into commonsense, numerical, logical, and qualitative reasoning. However, with the rapid progress of reasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been a growing demand for reasoning benchmarks that can keep pace with ongoing model developments. In this paper, we introduce MastermindEval, a simple, scalable, and interpretable deductive reasoning benchmark inspired by the board game Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic evaluation, in which the model autonomously plays the game, and (2) deductive reasoning evaluation, in which the model is given a pre-played game state with only one possible valid code to infer. In our experimental results we (1) find that even easy Mastermind instances are difficult for current models and (2) demonstrate that the benchmark is scalable to possibly more advanced models in the future Furthermore, we investigate possible reasons why models cannot deduce the final solution and find that current models are limited in deducing the concealed code as the number of statement to combine information from is increasing."},"pdf":{"value":"/pdf/02eaa463a61e0c3e0de2f10d9f50055a8b4f55ba.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\ngolde2025mastermindeval,\ntitle={MastermindEval: A Simple But Scalable Reasoning Benchmark},\nauthor={Jonas Golde and Patrick Haller and Fabio Barth and Alan Akbik},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=H4donosutm}\n}"},"paperhash":{"value":"golde|mastermindeval_a_simple_but_scalable_reasoning_benchmark"}},"id":"H4donosutm","forum":"H4donosutm","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission173/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission173/Authors"],"number":173,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission173/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738926611005,"cdate":1738926611005,"tmdate":1741877902433,"mdate":1741877902433,"pdate":1741166413128,"odate":1741166413128,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws"},"authors":{"value":["Prasanna Mayilvahanan","Thaddäus Wiedemer","Sayak Mallick","Matthias Bethge","Wieland Brendel"]},"authorids":{"value":["~Prasanna_Mayilvahanan2","~Thaddäus_Wiedemer1","~Sayak_Mallick1","~Matthias_Bethge1","~Wieland_Brendel1"]},"keywords":{"value":["scaling laws","robustness"]},"TLDR":{"value":"LLM architecture struggle to go beyond the data"},"abstract":{"value":"Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance.\nIn this work, we investigate which factors most strongly influence loss-to-loss scaling.\nOur experiments reveal that the pretraining data and tokenizer determine the scaling trend.\nIn contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact.\nConsequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency."},"pdf":{"value":"/pdf/cdc8b2ddd2510f86e415e5640a14f802ac9f9deb.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nmayilvahanan2025llms,\ntitle={{LLM}s on the Line: Data Determines Loss-to-Loss Scaling Laws},\nauthor={Prasanna Mayilvahanan and Thadd{\\\"a}us Wiedemer and Sayak Mallick and Matthias Bethge and Wieland Brendel},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=m74HDYarEt}\n}"},"paperhash":{"value":"mayilvahanan|llms_on_the_line_data_determines_losstoloss_scaling_laws"}},"id":"m74HDYarEt","forum":"m74HDYarEt","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission172/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission172/Authors"],"number":172,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738926229234,"cdate":1738926229234,"tmdate":1742177655500,"mdate":1742177655500,"pdate":1742177655469,"odate":1742177655469,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Advancing Multimodal In-Context Learning in Large Vision-Language Models with Task-aware Demonstrations"},"authors":{"value":["Yanshu Li"]},"authorids":{"value":["~Yanshu_Li1"]},"keywords":{"value":["Multimodal","Large Vision-language Models","In-context learning"]},"abstract":{"value":"Multimodal in-context learning (ICL) has emerged as a key capability of Large Vision-Language Models (LVLMs), driven by their increasing scale and applicability. Despite its promise, effective ICL in the multimodal setting remains challenging due to the inherent complexity of image-text inputs and the high sensitivity of ICL performance to input configurations. In this work, we shed light on the core mechanism underlying multimodal ICL, identifying task mapping as a crucial factor in configuring robust in-context demonstration (ICD) sequences. Building on these insights, we propose \\textit{SabER}, a lightweight yet powerful decoder-only transformer equipped with task-aware attention, which intelligently selects and arranges ICDs from a demonstration library in an autoregressive fashion. This design enables fine-grained feature extraction and cross-modal reasoning, iteratively refining task mapping to generate high-quality ICD sequences. Through extensive experiments covering five LVLMs and nine benchmark datasets, SabER not only demonstrates strong empirical performance, but also provides deeper understanding of how task semantics interact with multimodal ICDs. Our findings highlight the importance of principled ICD sequence configuration and open new avenues to enhance multimodal ICL in a wide range of real-world scenarios."},"pdf":{"value":"/pdf/783ecbd1aad48864119ad46d748f8a001c74e571.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nli2025unveiling,\ntitle={Unveiling and Enhancing Multimodal In-context Learning of Large Vision-language Models},\nauthor={Yanshu Li},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=8ae6Mqr4Nr}\n}"},"paperhash":{"value":"li|advancing_multimodal_incontext_learning_in_large_visionlanguage_models_with_taskaware_demonstrations"}},"id":"8ae6Mqr4Nr","forum":"8ae6Mqr4Nr","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission171/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission171/Authors"],"number":171,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission171/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738926059467,"cdate":1738926059467,"tmdate":1742448867879,"mdate":1742448867879,"pdate":1741166413066,"odate":1741166413066,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Understanding Reasoning in Thinking Language Models via Steering Vectors"},"authors":{"value":["Constantin Venhoff","Iván Arcuschin","Philip Torr","Arthur Conmy","Neel Nanda"]},"authorids":{"value":["~Constantin_Venhoff1","~Iván_Arcuschin1","~Philip_Torr1","~Arthur_Conmy1","~Neel_Nanda1"]},"keywords":{"value":["Steering","Reasoning","Mechanistic Interpretability","Thinking Models"]},"TLDR":{"value":"We analyze reasoning behaviors in thinking LLMs like DeepSeek-R1, identifying steering vectors for their reasoning processes."},"abstract":{"value":"Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, the underlying mechanisms enabling their reasoning capabilities remain poorly understood. This work studies the particular reasoning processes of thinking LLMs by analyzing DeepSeek-R1-Distill models and comparing them with non-thinking models like GPT-4o. Through a systematic experiment on 300 tasks across 10 diverse categories, we identify key behavioral patterns that characterize thinking models, including expressing their own uncertainty, coming up with examples for validating their working hypothesis, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our findings not only advance the understanding of how thinking models reason but also offer practical tools for steering their reasoning processes in a controlled and interpretable manner. We validate our approach using two DeepSeek-R1-Distill models, showing consistent results across different model architectures."},"pdf":{"value":"/pdf/93dd6b545cf6de9a2ac92fcd6f31706a98bc1dfa.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nvenhoff2025understanding,\ntitle={Understanding Reasoning in Thinking Language Models via Steering Vectors},\nauthor={Constantin Venhoff and Iv{\\'a}n Arcuschin and Philip Torr and Arthur Conmy and Neel Nanda},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=OwhVWNOBcz}\n}"},"paperhash":{"value":"venhoff|understanding_reasoning_in_thinking_language_models_via_steering_vectors"}},"id":"OwhVWNOBcz","forum":"OwhVWNOBcz","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission170/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission170/Authors"],"number":170,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission170/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738925953198,"cdate":1738925953198,"tmdate":1742471529783,"mdate":1742471529783,"pdate":1741166412993,"odate":1741166412993,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs"},"authors":{"value":["Benjamin Estermann","Roger Wattenhofer"]},"authorids":{"value":["~Benjamin_Estermann1","~Roger_Wattenhofer1"]},"keywords":{"value":["Reasoning","scaling","large language model"]},"abstract":{"value":"Large Language Models (LLMs) have demonstrated remarkable text generation capabilities, and recent advances in training paradigms have led to breakthroughs in their reasoning performance.\nIn this work, we investigate how the reasoning effort of such models scales with problem complexity.\nWe use the infinitely scalable Tents puzzle, which has a known linear-time solution, to analyze this scaling behavior.\nOur results show that reasoning effort scales with problem size, but only up to a critical problem complexity.\nBeyond this threshold, the reasoning effort does not continue to increase, and may even decrease.\nThis observation highlights a critical limitation in the logical coherence of current LLMs as problem complexity increases, and underscores the need for strategies to improve reasoning scalability.\nFurthermore, our results reveal significant performance differences between current state-of-the-art reasoning models when faced with increasingly complex logical puzzles."},"pdf":{"value":"/pdf/c7fff047bbe7c355aaf4fe48735737f426aff0b1.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nestermann2025reasoning,\ntitle={Reasoning Effort and Problem Complexity: A Scaling Analysis in {LLM}s},\nauthor={Benjamin Estermann and Roger Wattenhofer},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=wfybjqEmx5}\n}"},"paperhash":{"value":"estermann|reasoning_effort_and_problem_complexity_a_scaling_analysis_in_llms"}},"id":"wfybjqEmx5","forum":"wfybjqEmx5","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission168/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission168/Authors"],"number":168,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission168/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738925853622,"cdate":1738925853622,"tmdate":1742320702957,"mdate":1742320702957,"pdate":1741166412880,"odate":1741166412880,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"LookPlanGraph: Embodied instruction following method with VLM graph augmentation"},"authors":{"value":["Anatoly Onishchenko","Alexey Kovalev","Aleksandr Panov"]},"authorids":{"value":["~Anatoly_Onishchenko1","~Alexey_Kovalev3","~Aleksandr_Panov1"]},"keywords":{"value":["Embodied instruction following","Scene-graphs","LLM-based planning"]},"TLDR":{"value":"We propose LookPlanGraph -- a novel approach that leverages hierarchical scene graphs and dynamically augments them during task execution."},"abstract":{"value":"Recently, approaches using Large Language Models (LLM) as planners for robotic tasks have become widespread. In such systems, the LLM must be grounded in the environment in which the robot is operating in order to successfully complete tasks. One way to achieve this grounding is to use a scene graph that contains all the information necessary to complete the task, including the presence and location of objects. In this paper, we propose an approach that works with a scene graph containing only immobile static objects, and augments the scene graph with the necessary movable objects during instruction following using a visual language model and an image from the agent's camera. We conduct thorough experiments on the compiled GRASIF dataset that contain tasks from SayPlan Office, Behaviour-1K, and RobotHow datasets, and demonstrate that the proposed approach effectively handles the task, bypassing approaches that use pre-created scene graphs."},"pdf":{"value":"/pdf/11d810e3e7763cabc8256f8dc840374f908a289c.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nonishchenko2025lookplangraph,\ntitle={LookPlanGraph: Embodied instruction following method with {VLM} graph augmentation},\nauthor={Anatoly Onishchenko and Alexey Kovalev and Aleksandr Panov},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=B47cCZfJFa}\n}"},"paperhash":{"value":"onishchenko|lookplangraph_embodied_instruction_following_method_with_vlm_graph_augmentation"}},"id":"B47cCZfJFa","forum":"B47cCZfJFa","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission165/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission165/Authors"],"number":165,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission165/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738924827394,"cdate":1738924827394,"tmdate":1742472593170,"mdate":1742472593170,"pdate":1741166412736,"odate":1741166412736,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"PHYSICS: Benchmarking Foundation Models for Problem Solving in Physics"},"authors":{"value":["Kaiyue Feng","Yilun Zhao","Yixin Liu","Tianyu Yang","Chen Zhao","John Sous","Arman Cohan"]},"authorids":{"value":["~Kaiyue_Feng1","~Yilun_Zhao1","~Yixin_Liu2","~Tianyu_Yang6","~Chen_Zhao2","~John_Sous1","~Arman_Cohan1"]},"keywords":{"value":["LLMs","Benchmark","physics","math","reasoning"]},"abstract":{"value":"We introduce PHYSICS, a comprehensive benchmark for PhD-qualifying exam physics problem solving. It contains 1,297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements"},"pdf":{"value":"/pdf/37b750fd4e754090bee4095d7c34449dc89015bf.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nfeng2025physics,\ntitle={{PHYSICS}: Benchmarking Foundation Models for PhD-Qualifying Exam Physics Problem Solving},\nauthor={Kaiyue Feng and Yilun Zhao and Yixin Liu and Tianyu Yang and Chen Zhao and John Sous and Arman Cohan},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ssCw35Jl44}\n}"},"paperhash":{"value":"feng|physics_benchmarking_foundation_models_for_problem_solving_in_physics"}},"id":"ssCw35Jl44","forum":"ssCw35Jl44","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission163/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission163/Authors"],"number":163,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission163/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738924277374,"cdate":1738924277374,"tmdate":1742426701706,"mdate":1742426701706,"pdate":1741166412532,"odate":1741166412532,"version":2,"details":{"writable":true,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Rethinking Fine-tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning"},"authors":{"value":["Feng Chen","Allan Raventos","Nan Cheng","Surya Ganguli","Shaul Druckmann"]},"authorids":{"value":["~Feng_Chen13","~Allan_Raventos1","~Nan_Cheng3","~Surya_Ganguli1","~Shaul_Druckmann1"]},"keywords":{"value":["Test-time compute","inference-time compute","coverage","pass@N","reasoning","large language model","formal math","finetuning","overfitting","overconfidence"]},"TLDR":{"value":"We show that limiting a model's confidence during training can improve test-time scaling in mathematical reasoning."},"abstract":{"value":"Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget?  To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be _misaligned_ with pass@N in that pass@N accuracy _decreases_ with longer training.  We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance.  Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees.  Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies."},"pdf":{"value":"/pdf/9486a87f66bb39ba7517e0b50a91beefcaa221aa.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nchen2025rethinking,\ntitle={Rethinking Fine-tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning},\nauthor={Feng Chen and Allan Raventos and Nan Cheng and Surya Ganguli and Shaul Druckmann},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=9L5t04WYAs}\n}"},"paperhash":{"value":"chen|rethinking_finetuning_when_scaling_testtime_compute_limiting_confidence_improves_mathematical_reasoning"}},"id":"9L5t04WYAs","forum":"9L5t04WYAs","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission162/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission162/Authors"],"number":162,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission162/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738921120936,"cdate":1738921120936,"tmdate":1742415575337,"mdate":1742415575337,"pdate":1741166412449,"odate":1741166412449,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"PC-Agent: A Hierarchical Agentic Framework for Complex Task Automation on PC"},"authors":{"value":["Haowei Liu","Xi Zhang","Haiyang Xu","Yuyang Wanyan","Junyang Wang","Ming Yan","Ji Zhang","Chunfeng Yuan","Changsheng Xu","Weiming Hu","Fei Huang"]},"authorids":{"value":["~Haowei_Liu1","~Xi_Zhang11","~Haiyang_Xu1","~Yuyang_Wanyan3","~Junyang_Wang1","~Ming_Yan2","~Ji_Zhang3","~Chunfeng_Yuan1","~Changsheng_Xu1","~Weiming_Hu1","~Fei_Huang2"]},"keywords":{"value":["Multimodal agent","complex task automation","refined perception","reasoning and planning"]},"abstract":{"value":"MLLM-based GUI agents can assist humans in completing various tasks on smart devices automatically, demonstrating significant potential and application value. Unlike smartphones, the PC scenario not only features a more complex interactive environment with denser and more varied UI and text layouts, but also involves more intricate intra- and inter-app workflows, thus posing greater challenges for both perception and decision-making. To address the above issues, we propose a hierarchical agentic framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. The APM integrates intention understanding and OCR to achieve fine-grained perception of the content and location of target text, and utilizes the accessibility (A11y) tree  to obtain interactive element information. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. Alongside the PC-Agent framework, we introduce a new benchmark PC-Eval including 8 widely used applications and 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32\\% absolute improvement of task success rate over previous state-of-the-art methods. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent."},"pdf":{"value":"/pdf/43bef34ab8d4a9ad688a8e1b2d0e5ab031f89fb2.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nliu2025pcagent,\ntitle={{PC}-Agent: A Hierarchical Agentic Framework for Complex Task Automation on {PC}},\nauthor={Haowei Liu and Xi Zhang and Haiyang Xu and Yuyang Wanyan and Junyang Wang and Ming Yan and Ji Zhang and Chunfeng Yuan and Changsheng Xu and Weiming Hu and Fei Huang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=Q20FcJJi4s}\n}"},"paperhash":{"value":"liu|pcagent_a_hierarchical_agentic_framework_for_complex_task_automation_on_pc"}},"id":"Q20FcJJi4s","forum":"Q20FcJJi4s","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission160/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission160/Authors"],"number":160,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission160/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738919434831,"cdate":1738919434831,"tmdate":1742452770844,"mdate":1742452770844,"pdate":1741166412386,"odate":1741166412386,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"FLEX-TRAVELPLANNER: A BENCHMARK FOR FLEXIBLE PLANNING WITH LANGUAGE AGENTS"},"authors":{"value":["Juhyun Oh","Eunsu Kim","Alice Oh"]},"authorids":{"value":["~Juhyun_Oh2","~Eunsu_Kim1","~Alice_Oh1"]},"keywords":{"value":["Evaluation","Planning","Reasoning","LLM"]},"TLDR":{"value":"We introduce a novel evaluation benchmark to assess LLMs' flexible planning ability in changing constraints."},"abstract":{"value":"Real-world planning problems require constant adaptation to changing requirements and balancing of competing constraints. However, current benchmarks for evaluating LLMs’ planning capabilities primarily focus on static, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark that evaluates language models’ ability to reason flexibly in dynamic planning scenarios. Building on the\nTravelPlanner dataset (Xie et al., 2024), we introduce two novel evaluation settings: (1) sequential constraint introduction across multiple turns, and (2) scenarios with explicitly prioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B reveals several key findings: models’ performance on single-turn tasks poorly predicts their ability to adapt plans across multiple turns; constraint introduction order significantly affects performance; and models struggle with constraint prioritization, often incorrectly favoring newly introduced lower priority preferences over existing higher-priority constraints. These findings highlight the importance of evaluating LLMs in more realistic, dynamic planning scenarios and suggest specific directions for improving model performance on complex planning tasks."},"pdf":{"value":"/pdf/65c84a7097841e886056b952bb27602e7bbbc453.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\noh2025flextravelplanner,\ntitle={{FLEX}-{TRAVELPLANNER}: A {BENCHMARK} {FOR} {FLEXIBLE} {PLANNING} {WITH} {LANGUAGE} {AGENTS}},\nauthor={Juhyun Oh and Eunsu Kim and Alice Oh},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=a7unQ5jMx7}\n}"},"paperhash":{"value":"oh|flextravelplanner_a_benchmark_for_flexible_planning_with_language_agents"}},"id":"a7unQ5jMx7","forum":"a7unQ5jMx7","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission159/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission159/Authors"],"number":159,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission159/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738919186782,"cdate":1738919186782,"tmdate":1742364547030,"mdate":1742364547030,"pdate":1741166412290,"odate":1741166412290,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Improving Test-Time Search for LLMs with Backtracking Against In-Context Value Verifiers"},"authors":{"value":["Anikait Singh","Kushal Arora","Sedrick Keh","Jean Mercat","Tatsunori Hashimoto","Chelsea Finn","Aviral Kumar"]},"authorids":{"value":["~Anikait_Singh1","~Kushal_Arora1","~Sedrick_Keh1","~Jean_Mercat1","~Tatsunori_Hashimoto1","~Chelsea_Finn1","~Aviral_Kumar2"]},"keywords":{"value":["LLMs","Reasoning","Test Time Inference","Backtracking","In-Context Verifiers"]},"TLDR":{"value":"Combining multi-step and multi-turn reasoning with preemptive backtracking and value verification optimizes inference efficiency by revising mistakes early in the reasoning process."},"abstract":{"value":"Solving reasoning problems is an iterative multi-step computation, where a reasoning agent progresses through a sequence of steps, with each step logically building upon the previous one to reach a desired conclusion. If the desired solution is not attained, the agent must backtrack and try reasoning chains that are quite different from previous attempts. Though prior work such as test-time search against an outcome verifier can improve performance, most search is done in parallel via Best-of-N reranking, and independently for each attempt at a problem, thus wasting a significant amount of computation in sampling multiple full solutions even beyond the point that is needed. Can we reduce the total amount of computation by sharing information and computation across multiple attempts to a given problem? In this paper, we build a novel approach combining process verifiers that predict likelihoods of success per step with preemptive backtracking to maximize performance per generated token. To do this, the PRM can be used to identify where a problematic step in a solution trace is by using the sensitivity of the predictions of the learned verifier and allowing the model to do focused resampling of the problematic portion of a solution. This approach can significantly reduce the amount of computation by leveraging partial computation from previous revisions. To further enhance the computational efficiency of inference, we introduce in-context process supervision, where the verifier is conditioned on the history of revisions that are attempted, reducing uncertainty in the verification decisions and improving the verifier's confidence with each round of backtracking. This framework for iterative backtracking, leveraging in-context process supervision, enables an effective tradeoff between inference and model performance."},"pdf":{"value":"/pdf/957073a28b689b3d4b367b31b943904895efa5cf.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsingh2025improving,\ntitle={Improving Test-Time Search for {LLM}s with Backtracking Against In-Context Value Verifiers},\nauthor={Anikait Singh and Kushal Arora and Sedrick Keh and Jean Mercat and Tatsunori Hashimoto and Chelsea Finn and Aviral Kumar},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ZXRKOAf7UX}\n}"},"paperhash":{"value":"singh|improving_testtime_search_for_llms_with_backtracking_against_incontext_value_verifiers"}},"id":"ZXRKOAf7UX","forum":"ZXRKOAf7UX","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission158/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission158/Authors"],"number":158,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission158/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738915800909,"cdate":1738915800909,"tmdate":1742416755375,"mdate":1742416755375,"pdate":1741166412288,"odate":1741166412288,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Resolving Ambiguity through Personalization in LLM chat systems"},"authors":{"value":["Sophia Huiwen Sun","Abishek Sankararaman","Balakrishnan Murali Narayanaswamy"]},"authorids":{"value":["~Sophia_Huiwen_Sun1","~Abishek_Sankararaman1","~Balakrishnan_Murali_Narayanaswamy1"]},"keywords":{"value":["personalized generation","reasoning","feedback incorporation","ambiguity"]},"TLDR":{"value":"explores LLMs' behavior to perform consistent personalized generation by reasoning about user feedback. Proposed a memory coreset method to improve"},"abstract":{"value":"This paper explores LLMs' ability to perform consistent personalized generation incorporating user feedback. We first show that it is challenging for LLMs to (1) utilize feedback consistently in long conversations, (2) reason about multiple partial or conflicting feedback, and (3) adapt to changing preferences within a conversation. These challenges show that input information selection is crucial for improving multi-turn LLM performance. We propose a novel solution of building a **CoreSet** of past conversations, a principled approach of personalization. In addition to addressing the long history, conflict, and preference change challenges, coresets are an effective way to reduce input tokens, making these services more cost-effective. We show that our coreset algorithm improves upon state-of-the-art methods on both synthetic and real-world ambiguity datasets compared to memory and personalization benchmarks."},"pdf":{"value":"/pdf/935d620c6c04d319c87a8c5d41016853145ccf3f.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsun2025resolving,\ntitle={Resolving Ambiguity through Personalization in {LLM} chat systems},\nauthor={Sophia Huiwen Sun and Abishek Sankararaman and Balakrishnan Murali Narayanaswamy},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=MUmGcyXVhE}\n}"},"paperhash":{"value":"sun|resolving_ambiguity_through_personalization_in_llm_chat_systems"}},"id":"MUmGcyXVhE","forum":"MUmGcyXVhE","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission157/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission157/Authors"],"number":157,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission157/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738914800586,"cdate":1738914800586,"tmdate":1742376593063,"mdate":1742376593063,"pdate":1741166412231,"odate":1741166412231,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Cutting Through the Noise: Boosting LLM Performance on Math Word Problems"},"authors":{"value":["Ujjwala Anantheswaran","Himanshu Gupta","Kevin Scaria","Shreyas Verma","Chitta Baral","Swaroop Mishra"]},"authorids":{"value":["~Ujjwala_Anantheswaran1","~Himanshu_Gupta5","~Kevin_Scaria1","~Shreyas_Verma1","~Chitta_Baral1","~Swaroop_Mishra1"]},"keywords":{"value":["reasoning","robustness","mathematics","problem solving","adversarial data","data generation","prompting framework","math word problems"]},"abstract":{"value":"Large Language Models (LLMs) excel at various tasks, including solving math word problems (MWPs), but struggle with real-world problems containing irrelevant information. To address this, we propose a prompting framework that generates adversarial variants of MWPs by adding irrelevant variables. We introduce a dataset, ProbleMathic, containing both adversarial and non-adversarial MWPs. Our experiments reveal that LLMs are susceptible to distraction by numerical noise, resulting in an average relative performance drop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Qwen-2, Mistral) on the adversarial samples from our dataset. Fine-tuning on adversarial training instances improves performance on adversarial MWPs by ~8%, indicating increased robustness to noise and improved ability to identify relevant data for reasoning. Finally, to assess the generalizability of our prompting framework, we introduce GSM-8K-Adv, an adversarial variant of the GSM-8K benchmark. LLMs continue to struggle when faced with adversarial information, reducing performance by up to 24%."},"pdf":{"value":"/pdf/3e02a76b285c739ff8395149b24710254e739f73.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nanantheswaran2025cutting,\ntitle={Cutting Through the Noise: Boosting {LLM} Performance on Math Word Problems},\nauthor={Ujjwala Anantheswaran and Himanshu Gupta and Kevin Scaria and Shreyas Verma and Chitta Baral and Swaroop Mishra},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=VnPYbWQjz7}\n}"},"paperhash":{"value":"anantheswaran|cutting_through_the_noise_boosting_llm_performance_on_math_word_problems"}},"id":"VnPYbWQjz7","forum":"VnPYbWQjz7","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission156/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission156/Authors"],"number":156,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission156/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738913103378,"cdate":1738913103378,"tmdate":1742101889038,"mdate":1742101889038,"pdate":1741166412109,"odate":1741166412109,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Training Large Language Models to Reason in a Continuous Latent Space"},"authors":{"value":["Shibo Hao","Sainbayar Sukhbaatar","DiJia Su","Xian Li","Zhiting Hu","Jason E Weston","Yuandong Tian"]},"authorids":{"value":["~Shibo_Hao1","~Sainbayar_Sukhbaatar1","~DiJia_Su1","~Xian_Li1","~Zhiting_Hu3","~Jason_E_Weston1","~Yuandong_Tian1"]},"keywords":{"value":["reasoning","chain of thought"]},"abstract":{"value":"Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research."},"pdf":{"value":"/pdf/95db3600f46cdd73797327c66ee5faafca5dc6e6.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nhao2025training,\ntitle={Training Large Language Models to Reason in a Continuous Latent Space},\nauthor={Shibo Hao and Sainbayar Sukhbaatar and DiJia Su and Xian Li and Zhiting Hu and Jason E Weston and Yuandong Tian},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=KrWSrrYGpT}\n}"},"paperhash":{"value":"hao|training_large_language_models_to_reason_in_a_continuous_latent_space"}},"id":"KrWSrrYGpT","forum":"KrWSrrYGpT","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission155/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission155/Authors"],"number":155,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission155/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738910303558,"cdate":1738910303558,"tmdate":1742459178918,"mdate":1742459178918,"pdate":1741166412107,"odate":1741166412107,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"s1: Simple test-time scaling"},"authors":{"value":["Niklas Muennighoff","Zitong Yang","Weijia Shi","Xiang Lisa Li","Li Fei-Fei","Hannaneh Hajishirzi","Luke Zettlemoyer","Percy Liang","Emmanuel Candes","Tatsunori Hashimoto"]},"authorids":{"value":["~Niklas_Muennighoff1","~Zitong_Yang1","~Weijia_Shi1","~Xiang_Lisa_Li1","~Li_Fei-Fei1","~Hannaneh_Hajishirzi1","~Luke_Zettlemoyer1","~Percy_Liang1","~Emmanuel_Candes1","~Tatsunori_Hashimoto1"]},"keywords":{"value":["test-time scaling","reasoning","large language models"]},"TLDR":{"value":"Training on 1,000 examples and budget forcing lead to test-time scaling and strong reasoning performance matching o1-preview"},"abstract":{"value":"Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending ``Wait'' multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1."},"pdf":{"value":"/pdf/6c1e17ad5415e37696199a46bde1c06bbfa86a3d.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nmuennighoff2025s,\ntitle={s1: Simple test-time scaling},\nauthor={Niklas Muennighoff and Zitong Yang and Weijia Shi and Xiang Lisa Li and Li Fei-Fei and Hannaneh Hajishirzi and Luke Zettlemoyer and Percy Liang and Emmanuel Candes and Tatsunori Hashimoto},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=LdH0vrgAHm}\n}"},"paperhash":{"value":"muennighoff|s1_simple_testtime_scaling"}},"id":"LdH0vrgAHm","forum":"LdH0vrgAHm","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission152/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission152/Authors"],"number":152,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission152/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738907515848,"cdate":1738907515848,"tmdate":1742011285978,"mdate":1742011285978,"pdate":1741166411942,"odate":1741166411942,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Multi-Turn Code Generation Through Single-Step Rewards"},"authors":{"value":["Arnav Kumar Jain","Gonzalo Gonzalez-Pumariega","Wayne Chen","Alexander M Rush","Wenting Zhao","Sanjiban Choudhury"]},"authorids":{"value":["~Arnav_Kumar_Jain2","~Gonzalo_Gonzalez-Pumariega1","~Wayne_Chen4","~Alexander_M_Rush1","~Wenting_Zhao1","~Sanjiban_Choudhury3"]},"keywords":{"value":["Code Generation","Language Models","Reinforcement Learning"]},"abstract":{"value":"We address the problem of code generation from multi-turn execution feedback. \nExisting methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards.\nWe propose a simple yet scalable approach, $\\mu$Code, that solves multi-turn code generation using only single-step rewards.\nOur key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn.\n$\\mu$Code iteratively trains both a generator to provide  code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code.\nExperimental evaluations show that our approach achieves significant improvements over state-of-the-art baselines such as STaR. \nWe provide analysis of the design choices of the reward models and policy, and show the efficacy of $\\mu$Code at utilizing the execution feedback."},"pdf":{"value":"/pdf/10e11b22553ee4df8e7fd1370a84e9efb386a7f6.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\njain2025multiturn,\ntitle={Multi-Turn Code Generation Through Single-Step Rewards},\nauthor={Arnav Kumar Jain and Gonzalo Gonzalez-Pumariega and Wayne Chen and Alexander M Rush and Wenting Zhao and Sanjiban Choudhury},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=B9kbmNtWIp}\n}"},"paperhash":{"value":"jain|multiturn_code_generation_through_singlestep_rewards"}},"id":"B9kbmNtWIp","forum":"B9kbmNtWIp","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission151/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission151/Authors"],"number":151,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission151/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738906422516,"cdate":1738906422516,"tmdate":1742455855824,"mdate":1742455855824,"pdate":1741166411890,"odate":1741166411890,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Teaching Transformers Causal Reasoning through Axiomatic Training"},"authors":{"value":["Aniket Vashishtha","Abhinav Kumar","Atharva Pandey","Abbavaram Gowtham Reddy","Kabir Ahuja","Vineeth N. Balasubramanian","Amit Sharma"]},"authorids":{"value":["~Aniket_Vashishtha1","~Abhinav_Kumar3","~Atharva_Pandey1","~Abbavaram_Gowtham_Reddy1","~Kabir_Ahuja1","~Vineeth_N._Balasubramanian2","~Amit_Sharma3"]},"keywords":{"value":["Causal Reasoning","Causal Axioms","Transformers","Generalization","Large Language Models"]},"abstract":{"value":"For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since active interventions are costly, we study to what extent a system can  learn causal reasoning from symbolic demonstrations of causal axioms. Specifically, we present an axiomatic training method where the system learns from  multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the system  would learn to generalize from the axiom demonstrations to more complex scenarios.  Our results, based on applying axiomatic training to learn the  transitivity axiom and d-separation rule, indicate that such generalization is possible. To avoid data contamination issues, we start with a 67 million parameter transformer model and train it from scratch. On both tasks, we find that a model trained on linear causal chains (along with some noisy variations) can generalize well to complex graphs, including longer causal chains, causal chains with reversed order, and graphs with branching. To handle diverse text inputs, the same method is extended to finetune language models. Finetuning Llama-3.1 8B model on our  axiomatic data leads to significant gains on causal benchmarks such as Corr2Cause and CLEAR, in some cases providing state-of-the-art performance surpassing GPT-4."},"pdf":{"value":"/pdf/77effbf9df9a2fc4343b516d297b12790b631d23.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nvashishtha2025teaching,\ntitle={Teaching Transformers Causal Reasoning through Axiomatic Training},\nauthor={Aniket Vashishtha and Abhinav Kumar and Atharva Pandey and Abbavaram Gowtham Reddy and Kabir Ahuja and Vineeth N. Balasubramanian and Amit Sharma},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ZBN2hY8Gyu}\n}"},"paperhash":{"value":"vashishtha|teaching_transformers_causal_reasoning_through_axiomatic_training"}},"id":"ZBN2hY8Gyu","forum":"ZBN2hY8Gyu","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission150/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission150/Authors"],"number":150,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission150/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738905756588,"cdate":1738905756588,"tmdate":1742400973039,"mdate":1742400973039,"pdate":1741166411852,"odate":1741166411852,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Offline Reinforcement Learning for LLM Multi-Step Reasoning"},"authors":{"value":["Huaijie Wang","Shibo Hao","Hanze Dong","Shenao Zhang","Yilin Bao","Ziran Yang","Yi Wu"]},"authorids":{"value":["~Huaijie_Wang1","~Shibo_Hao1","~Hanze_Dong1","~Shenao_Zhang1","~Yilin_Bao2","~Ziran_Yang1","~Yi_Wu1"]},"keywords":{"value":["Offline Reinforcement Learning","Soft Q-Learning","LLM Reasoning"]},"TLDR":{"value":"We present an offline reinforcement learning algorithm for LLM multi-step reasoning"},"abstract":{"value":"Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward.\n\nIn this work, we propose OREO (Offline REasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH), and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost the performance during test time."},"pdf":{"value":"/pdf/5519a75e1030b539b472da4af7b188c13bb95657.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nwang2025offline,\ntitle={Offline Reinforcement Learning for {LLM} Multi-Step Reasoning},\nauthor={Huaijie Wang and Shibo Hao and Hanze Dong and Shenao Zhang and Yilin Bao and Ziran Yang and Yi Wu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=RtQLtnuiIn}\n}"},"paperhash":{"value":"wang|offline_reinforcement_learning_for_llm_multistep_reasoning"}},"id":"RtQLtnuiIn","forum":"RtQLtnuiIn","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission147/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission147/Authors"],"number":147,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission147/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738902631466,"cdate":1738902631466,"tmdate":1742459462706,"mdate":1742459462706,"pdate":1741166411702,"odate":1741166411702,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Value-Based Deep RL Scales Predictably"},"authors":{"value":["Oleh Rybkin","Michal Nauman","Preston Fu","Charlie Victor Snell","Pieter Abbeel","Sergey Levine","Aviral Kumar"]},"authorids":{"value":["~Oleh_Rybkin1","~Michal_Nauman1","~Preston_Fu1","~Charlie_Victor_Snell1","~Pieter_Abbeel2","~Sergey_Levine1","~Aviral_Kumar2"]},"keywords":{"value":["scaling laws","online reinforcement learning","q-learning"]},"TLDR":{"value":"We establish that value-based online RL can be scaled predictably to larger data, larger compute, or generally larger budget"},"abstract":{"value":"Scaling data and compute is critical to the success of modern ML. However, scaling demands predictability: we want methods to not only perform well with more compute or data, but also have their performance be predictable from small-scale runs, without running the large-scale experiment. In this paper, we show that value-based off-policy RL methods are predictable despite community lore regarding their pathological behavior. First, we show that data and compute requirements to attain a given performance level lie on a Pareto frontier, controlled by the updates-to-data (UTD) ratio. By estimating this frontier, we can predict this data requirement when given more compute, and this compute requirement when given more data. Second, we determine the optimal allocation of a total resource budget across data and compute for a given performance and use it to determine hyperparameters that maximize performance for a given budget. Third, this scaling is enabled by first estimating predictable relationships between hyperparameters, which is used to manage effects of overfitting and plasticity loss unique to RL. We validate our approach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI gym, and IsaacGym, when extrapolating to higher levels of data, compute, budget, or performance."},"pdf":{"value":"/pdf/ece49bd8eec3bdadc054cfae4eb7df5cbdc2ce6b.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nrybkin2025valuebased,\ntitle={Value-Based Deep {RL} Scales Predictably},\nauthor={Oleh Rybkin and Michal Nauman and Preston Fu and Charlie Victor Snell and Pieter Abbeel and Sergey Levine and Aviral Kumar},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=2ogvMWbQ9d}\n}"},"paperhash":{"value":"rybkin|valuebased_deep_rl_scales_predictably"}},"id":"2ogvMWbQ9d","forum":"2ogvMWbQ9d","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission146/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission146/Authors"],"number":146,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission146/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738902383574,"cdate":1738902383574,"tmdate":1742414892689,"mdate":1742414892689,"pdate":1741166411663,"odate":1741166411663,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"InductionBench: LLMs Fail in the Simplest Complexity Class"},"authors":{"value":["Wenyue Hua","Fei Sun","Liangming Pan","Adam Jardine","William Yang Wang"]},"authorids":{"value":["~Wenyue_Hua1","~Fei_Sun3","~Liangming_Pan1","~Adam_Jardine2","~William_Yang_Wang2"]},"keywords":{"value":["large language models","benchmark","inductive reasoning"]},"abstract":{"value":"Large language models (LLMs) have shown remarkable improvements in reasoning, largely due to intensive pretraining and scaling at inference time. Many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, \\textit{inductive reasoning}, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce \\textbf{InductionBench}, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even o3, the most advanced model available, struggles to master the simplest complexity classes within the subregular hierarchy, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities."},"pdf":{"value":"/pdf/9728ac9e600a15ef11641e5755e2169bd8c16e81.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nhua2025inductionbench,\ntitle={InductionBench: {LLM}s Fail in the Simplest Complexity Class},\nauthor={Wenyue Hua and Fei Sun and Liangming Pan and Adam Jardine and William Yang Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=brw11PScQM}\n}"},"paperhash":{"value":"hua|inductionbench_llms_fail_in_the_simplest_complexity_class"}},"id":"brw11PScQM","forum":"brw11PScQM","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission145/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission145/Authors"],"number":145,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission145/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738901836832,"cdate":1738901836832,"tmdate":1742454413311,"mdate":1742454413311,"pdate":1741166411619,"odate":1741166411619,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners"},"authors":{"value":["Daniele Paliotta","Junxiong Wang","Matteo Pagliardini","Kevin Li","Aviv Bick","Albert Gu","François Fleuret","Tri Dao"]},"authorids":{"value":["~Daniele_Paliotta1","~Junxiong_Wang1","~Matteo_Pagliardini1","~Kevin_Li3","~Aviv_Bick1","~Albert_Gu1","~François_Fleuret2","~Tri_Dao1"]},"keywords":{"value":["transformers","llms","large language models","mamba","linear rnns","distillation","test time compute","inference time compute"]},"TLDR":{"value":"Distilling pretrained Transformers such as Llama 3 to pure or hybrid Mamba models lead to better scaling in reasoning tasks due to faster inference time"},"abstract":{"value":"Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. \nA common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. \nThis raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget?\nTo address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers.\nTrained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. \nDespite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teachers under fixed time budgets, opening a new direction for scaling inference compute."},"pdf":{"value":"/pdf/1fa3578ab87e058854fc6f3f060a677cdb7f4697.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\npaliotta2025thinking,\ntitle={Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners},\nauthor={Daniele Paliotta and Junxiong Wang and Matteo Pagliardini and Kevin Li and Aviv Bick and Albert Gu and Fran{\\c{c}}ois Fleuret and Tri Dao},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=0Vwmx0MIRG}\n}"},"paperhash":{"value":"paliotta|thinking_slow_fast_scaling_inference_compute_with_distilled_reasoners"}},"id":"0Vwmx0MIRG","forum":"0Vwmx0MIRG","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission143/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission143/Authors"],"number":143,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission143/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738898987857,"cdate":1738898987857,"tmdate":1742422335907,"mdate":1742422335907,"pdate":1741166411540,"odate":1741166411540,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems"},"authors":{"value":["Anirudh Chari","Suraj Marpadga Reddy","Aditya Tiwari","Richard Lian","Brian Lee Zhou"]},"authorids":{"value":["~Anirudh_Chari1","~Suraj_Marpadga_Reddy1","~Aditya_Tiwari3","~Richard_Lian1","~Brian_Lee_Zhou1"]},"keywords":{"value":["large language models","embodied agents","minecraft"]},"abstract":{"value":"While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage \\textit{mental models} through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience."},"pdf":{"value":"/pdf/33caec4b109a785e831f1c32116e8a5c8af37eda.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nchari2025mindstores,\ntitle={{MINDSTORES}: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems},\nauthor={Anirudh Chari and Suraj Marpadga Reddy and Aditya Tiwari and Richard Lian and Brian Lee Zhou},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=CSxiqflLXg}\n}"},"paperhash":{"value":"chari|mindstores_memoryinformed_neural_decision_synthesis_for_taskoriented_reinforcement_in_embodied_systems"}},"id":"CSxiqflLXg","forum":"CSxiqflLXg","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission141/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission141/Authors"],"number":141,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission141/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738897152153,"cdate":1738897152153,"tmdate":1742221933660,"mdate":1742221933660,"pdate":1741166411385,"odate":1741166411385,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach"},"authors":{"value":["SHUANGYAN DENG","Haizhou Peng","Jiachen Xu","Chunhou Liu","Ciprian Doru Giurcaneanu","Jiamou Liu"]},"authorids":{"value":["~SHUANGYAN_DENG2","~Haizhou_Peng1","~Jiachen_Xu3","~Chunhou_Liu1","~Ciprian_Doru_Giurcaneanu1","~Jiamou_Liu1"]},"keywords":{"value":["Multimodal Reasoning","Financial Reasoning","Multimodal Large Language Model","Error Feedback Learning"]},"TLDR":{"value":"Our paper presents FinMR, a multimodal benchmark for evaluating financial reasoning in AI, and introduces EFL, a method to improve model performance through error feedback learning."},"abstract":{"value":"Effective financial reasoning demands not only textual understanding but also the ability to interpret complex visual data such as charts, tables, and trend graphs. This paper introduces a new benchmark designed to evaluate how well AI models—especially large language and multimodal models—reason in finance-specific contexts. Covering 3,200 expert-level question-answer pairs across 15 core financial topics, the benchmark integrates both textual and visual modalities to reflect authentic analytical challenges in finance. To address limitations in current reasoning approaches, we propose an error-aware learning framework that leverages historical model mistakes and feedback to guide inference, without requiring fine-tuning. Our experiments across state-of-the-art models show that multimodal inputs significantly enhance performance and that incorporating error feedback leads to consistent and measurable improvements. The results highlight persistent challenges in visual understanding and mathematical logic, while also demonstrating the promise of self-reflective reasoning in financial AI systems."},"pdf":{"value":"/pdf/524127ce6e212463b9977de13e70d2948b53c587.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\ndeng2025understanding,\ntitle={Understanding Financial Reasoning in {AI}: A Multimodal Benchmark and Error Learning Approach},\nauthor={SHUANGYAN DENG and Haizhou Peng and Jiachen Xu and Chunhou Liu and Ciprian Doru Giurcaneanu and Jiamou Liu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=W8QSWDWjKC}\n}"},"paperhash":{"value":"deng|understanding_financial_reasoning_in_ai_a_multimodal_benchmark_and_error_learning_approach"}},"id":"W8QSWDWjKC","forum":"W8QSWDWjKC","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission138/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission138/Authors"],"number":138,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission138/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738891894527,"cdate":1738891894527,"tmdate":1742255699449,"mdate":1742255699449,"pdate":1741166411252,"odate":1741166411252,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"IGDA: INTERACTIVE GRAPH DISCOVERY THROUGH LARGE LANGUAGE MODEL AGENTS"},"authors":{"value":["Alexander Havrilla","David Alvarez-Melis","Nicolo Fusi"]},"authorids":{"value":["~Alexander_Havrilla2","~David_Alvarez-Melis1","~Nicolo_Fusi1"]},"keywords":{"value":["Discovery","Graph","Bayesian Optimization","Confidence"]},"TLDR":{"value":"We design a graph discovery algorithm utilizing LLMs as confidence estimators to predict and refine experimenal variable relationships"},"abstract":{"value":"Large language models (\\textbf{LLMs}) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable \\textit{semantic metadata} to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of \\textit{interactive graph discovery}: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose \\textbf{IGDA}, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches."},"pdf":{"value":"/pdf/8a5aadd484b7bcca4dc9dfda9a5d901470016865.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nhavrilla2025igda,\ntitle={{IGDA}: {INTERACTIVE} {GRAPH} {DISCOVERY} {THROUGH} {LARGE} {LANGUAGE} {MODEL} {AGENTS}},\nauthor={Alexander Havrilla and David Alvarez-Melis and Nicolo Fusi},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=cHV3Iw84AC}\n}"},"paperhash":{"value":"havrilla|igda_interactive_graph_discovery_through_large_language_model_agents"}},"id":"cHV3Iw84AC","forum":"cHV3Iw84AC","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission137/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission137/Authors"],"number":137,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission137/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738887705200,"cdate":1738887705200,"tmdate":1742438700300,"mdate":1742438700300,"pdate":1741166411149,"odate":1741166411149,"version":2,"details":{"writable":false,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"UNDERSTANDING INFERENCE SCALING LAWS FOR MIXTURES OF LLMS"},"authors":{"value":["Alexander Havrilla","Srishti Gureja"]},"authorids":{"value":["~Alexander_Havrilla2","~Srishti_Gureja1"]},"keywords":{"value":["Inference time-compute","Scaling","mixture"]},"TLDR":{"value":"We analyze effect of mixing model solutions at test time on inerence scaling laws, finding mixed strategies produce more diverse solutions and improve scaling laws"},"abstract":{"value":"Scaling inference time compute has enabled a significant improvement in model mathematical problem solving ability. However, most inference scaling strategies sample only from a single model. We extend and analyze inference scaling in the mixed model setting, where samples from weak but inexpensive and strong but expensive models can be pooled at test time. We find mixing samples over a distribution of problems can outperform the best pure, single model strategy by over 5\\% when given the same compute budget. Further, model mixing extends the compute regimes for which inference scaling reliably improves performance. However, as part of our analysis, we prove that for a \\textbf{fixed problem} $Q$ a pure strategy sampling only a single model is most efficient. Further, the best model can be identified as having the largest \\textit{compute normalized probability} of success for $Q$. This implies the observed empirical improvements from model mixing stem from an average improvement over the problem distribution as opposed to improvement over the best pure strategy for any single problem. To better understand this result we empirically analyze the distribution of compute normalized probabilities over problems for variously sized models. Our analysis reveals each model is best suited for efficiently solving a non-trivial subset of problems, further motivating the effectiveness of mixing solutions. Somewhat surprisingly, this remains true even for the hardest set of problems, where, for example, the smallest model is most efficient in solving 25\\% of the problem set."},"pdf":{"value":"/pdf/9ad9bd488c2457516bc5ab3240a98494b7754b85.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nhavrilla2025understanding,\ntitle={{UNDERSTANDING} {INFERENCE} {SCALING} {LAWS} {FOR} {MIXTURES} {OF} {LLMS}},\nauthor={Alexander Havrilla and Srishti Gureja},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=LjE6gxMMkP}\n}"},"paperhash":{"value":"havrilla|understanding_inference_scaling_laws_for_mixtures_of_llms"}},"id":"LjE6gxMMkP","forum":"LjE6gxMMkP","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission136/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission136/Authors"],"number":136,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission136/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738885097544,"cdate":1738885097544,"tmdate":1742439127866,"mdate":1742439127866,"pdate":1741166411144,"odate":1741166411144,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"StochasTok: Improving Fine-Grained Subword Understanding in LLMs"},"authors":{"value":["Anya Sims","Cong Lu","Klara Kaleb","Jakob Nicolaus Foerster","Yee Whye Teh"]},"authorids":{"value":["~Anya_Sims1","~Cong_Lu1","~Klara_Kaleb1","~Jakob_Nicolaus_Foerster1","~Yee_Whye_Teh2"]},"keywords":{"value":["language models","pretraining","tokenization"]},"TLDR":{"value":"A simple stochastic tokenization method—randomly splitting tokens before pretraining—that dramatically improves fine-grained, subword-level understanding in language models without any compromise to benchmark performance or increase in training cost."},"abstract":{"value":"Despite impressive performance, large language models (LLMs) still struggle with seemingly simple questions such as \"How many r's are in 'strawberry'?\" This limitation highlights that LLMs are unable to understand how humans `see' language. We attempt to address this by experimenting with stochastic tokenization schemes in which the same text may be tokenized into multiple possible token sequences. We find that using stochastic tokenization during pretraining dramatically alters the representations learned and allows LLMs to capture understanding of fine-grained spelling-level detail in addition to the structure learned with standard tokenization. We demonstrate this by showing that LLMs pretrained with standard deterministic tokenization cannot be fine-tuned to answer language-game type questions, whilst with the minimal addition of stochastic tokenization during pretraining, the corresponding LLMs perform near-perfectly. Crucially, these improvements are achieved without any performance drop on standard benchmarks or any additional training cost — the only change is a single simple, computationally cheap preprocessing step. Overall, our results suggest that embracing stochastic tokenization can help enable LLMs to better understand how humans perceive language."},"pdf":{"value":"/pdf/4e70457646c1560277343f3f80aa1e7a7a8d751a.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsims2025stochastok,\ntitle={StochasTok: Improving Fine-Grained Subword Understanding in {LLM}s},\nauthor={Anya Sims and Cong Lu and Klara Kaleb and Jakob Nicolaus Foerster and Yee Whye Teh},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=E9kcE8uGp3}\n}"},"paperhash":{"value":"sims|stochastok_improving_finegrained_subword_understanding_in_llms"}},"id":"E9kcE8uGp3","forum":"E9kcE8uGp3","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission135/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission135/Authors"],"number":135,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission135/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738884851317,"cdate":1738884851317,"tmdate":1742471487103,"mdate":1742471487103,"pdate":1741166411088,"odate":1741166411088,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation"},"authors":{"value":["Bo Pang","Hanze Dong","Jiacheng Xu","Silvio Savarese","Yingbo Zhou","Caiming Xiong"]},"authorids":{"value":["~Bo_Pang4","~Hanze_Dong1","~Jiacheng_Xu2","~Silvio_Savarese1","~Yingbo_Zhou1","~Caiming_Xiong1"]},"keywords":{"value":["LLM","reasoning","data synthesis"]},"abstract":{"value":"Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM’s LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities."},"pdf":{"value":"/pdf/4253eaad74807684f06e0276b6c535629b7801c4.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\npang2025bolt,\ntitle={{BOLT}: Bootstrap Long Chain-of-Thought in Language Models without Distillation},\nauthor={Bo Pang and Hanze Dong and Jiacheng Xu and Silvio Savarese and Yingbo Zhou and Caiming Xiong},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=9c1AUgv8yb}\n}"},"paperhash":{"value":"pang|bolt_bootstrap_long_chainofthought_in_language_models_without_distillation"}},"id":"9c1AUgv8yb","forum":"9c1AUgv8yb","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission133/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission133/Authors"],"number":133,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission133/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738881988606,"cdate":1738881988606,"tmdate":1742432190802,"mdate":1742432190802,"pdate":1741166410967,"odate":1741166410967,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models"},"authors":{"value":["Sid Bharthulwar","John Rho","Katrina Brown"]},"authorids":{"value":["~Sid_Bharthulwar1","~John_Rho1","~Katrina_Brown1"]},"keywords":{"value":["evolutionary algorithms","chain of thought","vision-language models","VLMs","reasoning","inference-time compute","guided search"]},"TLDR":{"value":"Evolutionary search discovers prompts that let VLMs decompose tasks and do chain-of-thought on image tokens via tool synthesis, improving vision-language reasoning without model updates"},"abstract":{"value":"We present a framework for optimizing prompts in vision-language models to elicit multimodal reasoning without model retraining. Using an evolutionary algorithm to guide prompt updates downstream of visual tasks, our approach improves upon baseline prompt-updating algorithms, which lack evolution-style \"survival of the fittest\" iteration. Crucially, we find this approach enables the language model to independently discover progressive problem-solving techniques across several evolution generations. For example, the model reasons that to \"break down\" visually complex spatial tasks, making a tool call to a Python interpreter to perform tasks (such as cropping, image segmentation, or saturation changes) would improve performance significantly. Our experimentation shows that explicitly evoking this \"tool calling\" call, via system-level XML $...\\texttt{<tool>} ... \\texttt{</tool>}...$ tags, can effectively flag Python interpreter access for the same language model to generate relevant programs, generating advanced multimodal functionality. This functionality can be crystallized into a system-level prompt that induces improved performance at inference time, and our experimentation suggests up to $\\approx 50\\%$ relative improvement across select visual tasks. Downstream performance is trained and evaluated across subtasks from MathVista, M3CoT, and GeoBench-VLM datasets. Importantly, our approach shows that evolutionary prompt optimization guides language models towards self-reasoning discoveries, which result in improved zero-shot generalization across tasks."},"pdf":{"value":"/pdf/f8a23e228cee00e61279bf1774088241644a9a88.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nbharthulwar2025evolutionary,\ntitle={Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models},\nauthor={Sid Bharthulwar and John Rho and Katrina Brown},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=u8BO0NFF21}\n}"},"paperhash":{"value":"bharthulwar|evolutionary_prompt_optimization_discovers_emergent_multimodal_reasoning_strategies_in_visionlanguage_models"}},"id":"u8BO0NFF21","forum":"u8BO0NFF21","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission131/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission131/Authors"],"number":131,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission131/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738877700073,"cdate":1738877700073,"tmdate":1742420020981,"mdate":1742420020981,"pdate":1741166410839,"odate":1741166410839,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Refining Answer Distributions for Improved Large Language Model Reasoning"},"authors":{"value":["Soumyasundar Pal","Didier Chételat","Yingxue Zhang","Mark Coates"]},"authorids":{"value":["~Soumyasundar_Pal1","~Didier_Chételat1","~Yingxue_Zhang1","~Mark_Coates1"]},"keywords":{"value":["Reasoning","Large Language Models"]},"TLDR":{"value":"We present Refined Answer Distributions, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs."},"abstract":{"value":"Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining multiple LLM responses, generated either in parallel in a single query, or via sequential interactions with LLMs throughout the reasoning process. Existing strategies for combination, such as self-consistency and progressive-hint-prompting, make inefficient usage of the LLM responses. We present Refined Answer Distributions, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs. Our approach can be viewed as an iterative sampling strategy for forming a Monte Carlo approximation of an underlying distribution of answers, with the goal of identifying the mode --- the most likely answer. Empirical evaluation on several reasoning benchmarks demonstrates the superiority of the proposed approach."},"pdf":{"value":"/pdf/d2411a2a9c47bb02e290df04d5cd92bef41b2626.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\npal2025refining,\ntitle={Refining Answer Distributions for Improved Large Language Model Reasoning},\nauthor={Soumyasundar Pal and Didier Ch{\\'e}telat and Yingxue Zhang and Mark Coates},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=KVZKtugIsy}\n}"},"paperhash":{"value":"pal|refining_answer_distributions_for_improved_large_language_model_reasoning"}},"id":"KVZKtugIsy","forum":"KVZKtugIsy","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission130/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission130/Authors"],"number":130,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission130/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738876638434,"cdate":1738876638434,"tmdate":1742399240036,"mdate":1742399240036,"pdate":1741166410792,"odate":1741166410792,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations"},"authors":{"value":["Harshita Chopra","Chirag Shah"]},"authorids":{"value":["~Harshita_Chopra1","~Chirag_Shah1"]},"keywords":{"value":["Planning","Large Language Models","Conversational AI","Monte Carlo Tree Search","Efficient Information Seeking"]},"abstract":{"value":"The ability to identify and acquire missing information is a critical component of effective decision making and problem solving. With the rise of conversational artificial intelligence (AI) systems, strategically formulating information-seeking questions becomes crucial and demands efficient methods to guide the search process. We introduce a novel approach to adaptive question-asking through a combination of Large Language Models (LLM) for generating questions that maximize information gain, Monte Carlo Tree Search (MCTS) for building and using a decision tree across multiple samples, and a hierarchical feedback mechanism to learn from past interactions. We present two key innovations: (1) an adaptive MCTS algorithm that balances exploration and exploitation for efficient search over potential questions; and (2) a clustering-based feedback algorithm that leverages prior experience to guide future interactions. Each incoming sample is assigned to a cluster based on its semantic similarity with previously observed samples. Our UCT (Upper Confidence bound for Trees) formulation selects optimal questions by combining expected rewards, a function of information gain, with a cluster-specific bonus that decays with depth, to emphasize the importance of early-stage questions that have proven effective for narrowing the solution space in similar samples. Experiments in medical diagnosis and troubleshooting domains demonstrate that our method leads to an average of 12% improvement in success rates and a 10x reduction in the average number of LLM calls made per conversation for the search process, in comparison to the state of the art."},"pdf":{"value":"/pdf/671ea2ebe5e94070c8eacad717ccf99350112003.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nchopra2025feedbackaware,\ntitle={Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations},\nauthor={Harshita Chopra and Chirag Shah},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=iIQNK2lvN3}\n}"},"TLDR":{"value":"An efficient and effective planning approach for information-seeking tasks using LLMs with MCTS and a feedback-driven decision process."},"paperhash":{"value":"chopra|feedbackaware_monte_carlo_tree_search_for_efficient_information_seeking_in_goaloriented_conversations"}},"id":"iIQNK2lvN3","forum":"iIQNK2lvN3","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission128/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission128/Authors"],"number":128,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission128/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738869150319,"cdate":1738869150319,"tmdate":1742451142060,"mdate":1742451142060,"pdate":1741166410725,"odate":1741166410725,"version":2,"details":{"writable":false,"replyCount":1,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"When More is Less: Understanding Chain-of-Thought Length in LLMs"},"authors":{"value":["Yuyang Wu","Yifei Wang","Tianqi Du","Stefanie Jegelka","Yisen Wang"]},"authorids":{"value":["~Yuyang_Wu2","~Yifei_Wang1","~Tianqi_Du1","~Stefanie_Jegelka3","~Yisen_Wang1"]},"keywords":{"value":["Chain of Thought Reasoning","Large Language Model","Overthinking"]},"abstract":{"value":"Chain-of-thought (CoT) reasoning enhances the multi-step reasoning capabilities of large language models (LLMs) by breaking complex tasks into smaller, manageable sub-tasks. Researchers have been exploring ways to guide models to generate more complex CoT processes to improve the reasoning ability of LLMs, such as long CoT and the test-time scaling law. However, for most models and tasks, does an increase in CoT length consistently lead to improved reasoning accuracy?\nIn this paper, we observe a nuanced relationship: as the number of reasoning steps increases, performance initially improves but eventually decreases. To understand this phenomenon, we provide a piece of evidence that *longer reasoning processes are increasingly susceptible to noise.* We theoretically prove the existence of an optimal reasoning step number and derive a scaling law for this optimal CoT length based on model capability and task difficulty. Inspired by our theory, we propose length-aware majority voting to alleviate the effects of excessively long or short CoTs, which is verified on both synthetic and real-world datasets. Our findings highlight the critical need to calibrate CoT length to align with model capabilities and task demands, offering a principled framework for optimizing multi-step reasoning in LLMs."},"pdf":{"value":"/pdf/cfc93cd793defd24c6fe2153816c5d29e06dab50.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nwu2025when,\ntitle={When More is Less: Understanding Chain-of-Thought Length in {LLM}s},\nauthor={Yuyang Wu and Yifei Wang and Tianqi Du and Stefanie Jegelka and Yisen Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=W8dxn7hBkO}\n}"},"paperhash":{"value":"wu|when_more_is_less_understanding_chainofthought_length_in_llms"}},"id":"W8dxn7hBkO","forum":"W8dxn7hBkO","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission126/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission126/Authors"],"number":126,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission126/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738863126672,"cdate":1738863126672,"tmdate":1742451499975,"mdate":1742451499975,"pdate":1741166410609,"odate":1741166410609,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"MALT: Improving Reasoning with Multi-Agent LLM Training"},"authors":{"value":["Sumeet Ramesh Motwani","Chandler Smith","Rocktim Jyoti Das","Rafael Rafailov","Ivan Laptev","Philip Torr","Fabio Pizzati","Ronald Clark","Christian Schroeder de Witt"]},"authorids":{"value":["~Sumeet_Ramesh_Motwani1","~Chandler_Smith1","~Rocktim_Jyoti_Das2","~Rafael_Rafailov1","~Ivan_Laptev1","~Philip_Torr1","~Fabio_Pizzati1","~Ronald_Clark2","~Christian_Schroeder_de_Witt1"]},"keywords":{"value":["reasoning","multi-agent systems","post-training","reinforcement learning","large language models"]},"TLDR":{"value":"Multi-agent post-training for LLMs to improve reasoning performance"},"abstract":{"value":"Large Language Models (LLMs) often produce answers with a single chain-of-thought, which restricts their ability to explore reasoning paths or self-correct flawed outputs in complex tasks. In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents. During data generation, each agent is repeatedly sampled to form a multi-agent search tree, where final outputs are graded against ground-truth data. We then apply value iteration to propagate reward signals back to each role-conditioned model, automatically producing multi-agent post-training data without human or teacher-model supervision. Our off-policy approach allows each agent to specialize by learning from correct and incorrect trajectories, ultimately improving the end-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same baseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40% respectively, making it an important advance towards multi-agent cooperative training."},"pdf":{"value":"/pdf/a7778587ef87427645857180b3e3fd8acd52541c.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nmotwani2025malt,\ntitle={{MALT}: Improving Reasoning with Multi-Agent {LLM} Training},\nauthor={Sumeet Ramesh Motwani and Chandler Smith and Rocktim Jyoti Das and Rafael Rafailov and Ivan Laptev and Philip Torr and Fabio Pizzati and Ronald Clark and Christian Schroeder de Witt},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=lIf7grAC7n}\n}"},"paperhash":{"value":"motwani|malt_improving_reasoning_with_multiagent_llm_training"}},"id":"lIf7grAC7n","forum":"lIf7grAC7n","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission123/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission123/Authors"],"number":123,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738860295132,"cdate":1738860295132,"tmdate":1742177655295,"mdate":1742177655295,"pdate":1742177655280,"odate":1742177655280,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Chain-of-Timeline: Enhancing LLM Zero-Shot Temporal Reasoning with SQL-Style Timeline Formalization"},"authors":{"value":["Jiaying Wu","Bryan Hooi"]},"authorids":{"value":["~Jiaying_Wu2","~Bryan_Hooi1"]},"keywords":{"value":["temporal reasoning","time-sensitive QA","Large Language Models"]},"abstract":{"value":"Accurate reasoning about time-sensitive facts is essential in today's rapidly evolving knowledge landscape. While Large Language Models (LLMs) possess impressive reasoning capabilities, they struggle with time-sensitive question answering (QA) in long documents due to the presence of (1) irrelevant noisy context and (2) implicit expressions of temporal events. To address these challenges, we introduce Chain-of-Timeline (CoTime), a framework that constructs topic-relevant event timelines through structured code-style formalization. CoTime first extracts a high-level topic from the question (e.g., [subject]'s career history) to identify relevant temporal events in the document. These events are then organized into a temporal SQL-style schema, enabling CoTime to derive answers based on the question's specified time identifiers. Experimental results show that CoTime surpasses representative  baselines."},"pdf":{"value":"/pdf/a5c45c5ed472a3117ce97f64a540fd1566104022.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"TLDR":{"value":"We introduce Chain-of-Timeline (CoTime), a training-free framework that enhances LLM temporal reasoning by distilling and structuring temporal facts into a formalized, SQL-style timeline."},"_bibtex":{"value":"@inproceedings{\nwu2025chainoftimeline,\ntitle={Chain-of-Timeline: Enhancing {LLM} Zero-Shot Temporal Reasoning with {SQL}-Style Timeline Formalization},\nauthor={Jiaying Wu and Bryan Hooi},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ptHtrHwMpw}\n}"},"paperhash":{"value":"wu|chainoftimeline_enhancing_llm_zeroshot_temporal_reasoning_with_sqlstyle_timeline_formalization"}},"id":"ptHtrHwMpw","forum":"ptHtrHwMpw","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission121/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission121/Authors"],"number":121,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission121/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738853974165,"cdate":1738853974165,"tmdate":1742471848267,"mdate":1742471848267,"pdate":1741166410259,"odate":1741166410259,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification"},"authors":{"value":["Hyunseok Lee","Seunghyuk Oh","Jihoon Tack","Jaehyung Kim","Jinwoo Shin"]},"authorids":{"value":["~Hyunseok_Lee1","~Seunghyuk_Oh1","~Jihoon_Tack1","~Jaehyung_Kim1","~Jinwoo_Shin1"]},"keywords":{"value":["Self-refine","Test-time compute","Large Language Model","Self-verify","Self-awareness"]},"abstract":{"value":"Self-awareness, i.e., the ability to assess and correct one's own generation, is a fundamental aspect of human intelligence, making its replication in large language models (LLMs) an important yet challenging task. Previous works tackle this by employing extensive reinforcement learning or rather relying on large external verifiers. In this work, we propose Refine via Intrinsic Self-Verification (ReVISE), an efficient and effective framework that enables LLMs to self-correct their outputs through self-verification. The core idea of ReVISE is to enable LLMs to verify their reasoning processes and continually rethink reasoning trajectories based on its verification. We introduce a structured curriculum based upon online preference learning to implement this efficiently. Specifically, as ReVISE involves two challenging tasks (i.e., self-verification and reasoning correction), we tackle each task sequentially using curriculum learning, collecting both failed and successful reasoning paths to construct preference pairs for efficient training. During inference, our approach enjoys natural test-time scaling by integrating self-verification and correction capabilities, further enhanced by our proposed confidence-aware decoding mechanism. Our experiments on various reasoning tasks demonstrate that ReVISE achieves efficient self-correction and significantly improves reasoning performance."},"pdf":{"value":"/pdf/d26330f067c0596b8ca4a9ecfaeb1b646cfb68fd.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nlee2025revise,\ntitle={Re{VISE}: Learning to Refine at Test-Time via Intrinsic Self-Verification},\nauthor={Hyunseok Lee and Seunghyuk Oh and Jihoon Tack and Jaehyung Kim and Jinwoo Shin},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=0lGvQDPKwh}\n}"},"paperhash":{"value":"lee|revise_learning_to_refine_at_testtime_via_intrinsic_selfverification"}},"id":"0lGvQDPKwh","forum":"0lGvQDPKwh","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission119/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission119/Authors"],"number":119,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission119/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738851676348,"cdate":1738851676348,"tmdate":1742367413218,"mdate":1742367413218,"pdate":1741166410067,"odate":1741166410067,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Implicit Language Models are RNNs: Balancing Parallelization and Expressivity"},"authors":{"value":["Mark Schöne","Babak Rahmani","Heiner Kremer","Fabian Falck","Hitesh Ballani","Jannes Gladrow"]},"authorids":{"value":["~Mark_Schöne1","~Babak_Rahmani1","~Heiner_Kremer1","~Fabian_Falck1","~Hitesh_Ballani1","~Jannes_Gladrow1"]},"keywords":{"value":["State-space models","deep equilibrium models","RNN","transformer","large-language models","sequence models","regular languages","Chomsky hierarchy"]},"TLDR":{"value":"Implicit SSMs bridge RNN expressiveness and transformer parallelization by iterating transformations to approximate fixed points, enabling scalable training and improved performance on state-tracking tasks and large-scale language modeling."},"abstract":{"value":"State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks."},"pdf":{"value":"/pdf/189771ccc52924fbdb2703af45d8c232f7881f4d.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nschone2025implicit,\ntitle={Implicit Language Models are {RNN}s: Balancing Parallelization and Expressivity},\nauthor={Mark Sch{\\\"o}ne and Babak Rahmani and Heiner Kremer and Fabian Falck and Hitesh Ballani and Jannes Gladrow},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=6XwIa48eex}\n}"},"paperhash":{"value":"schöne|implicit_language_models_are_rnns_balancing_parallelization_and_expressivity"}},"id":"6XwIa48eex","forum":"6XwIa48eex","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission118/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission118/Authors"],"number":118,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission118/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738850030263,"cdate":1738850030263,"tmdate":1742313450739,"mdate":1742313450739,"pdate":1741166410007,"odate":1741166410007,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"LLMs Are Not Good Strategists, Yet Memory-Enhanced Agency Boosts Reasoning"},"authors":{"value":["Yi Wu","Zhimin Hu"]},"authorids":{"value":["~Yi_Wu12","~Zhimin_Hu1"]},"keywords":{"value":["LLM Agent","Strategic Reasoning","StarCraft II","Episodic Memory","Cognitive Architecture"]},"TLDR":{"value":"We proposed an LLM-based agent with episodic and working memory to enhance strategic reasoning by balancing coherent strategy and adaptation. We tested it on StarCraft II showing significant improvements over the baseline with fewer token budget."},"abstract":{"value":"Strategic reasoning in dynamic environments, such as games, requires a balance between long-term strategy and short-term adaptations. Although specially trained agents can achieve superhuman performance, they often lack explainability and are highly dependent on extensive data for training. In contrast, approaches that leverage large language models (LLMs) benefit from few-shot learning but struggle to maintain strategic consistency. Drawing inspiration from existing cognitive models of human decision-making, which utilize various forms of memory, we introduce EpicStar, an LLM-based agent with cognitively inspired episodic and working memory modules. Episodic memory enables agents to draw on past experiences to formulate coherent long-term strategies, while working memory modulates active observation and decision variables essential for adaptation. We evaluated EpicStar in the strategy game StarCraft II, where it competes effectively against built-in agents at Level 6 difficulty, surpassing its predecessor at Level 5 with a smaller token budget. Our approach not only enhances adaptability, but also ensures strategic consistency, demonstrating the pivotal role that cognitive memory can play in strategic reasoning."},"pdf":{"value":"/pdf/c5f2798c03175f9d9d638a40e20e39f518876368.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nwu2025llms,\ntitle={{LLM}s Aren't Good Strategists, Yet Can Accumulate Episodes for Improved Planning},\nauthor={Yi Wu and Zhimin Hu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=34NjywGCdx}\n}"},"paperhash":{"value":"wu|llms_are_not_good_strategists_yet_memoryenhanced_agency_boosts_reasoning"}},"id":"34NjywGCdx","forum":"34NjywGCdx","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission117/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission117/Authors"],"number":117,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission117/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738845724022,"cdate":1738845724022,"tmdate":1742470276815,"mdate":1742470276815,"pdate":1741166409970,"odate":1741166409970,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs"},"authors":{"value":["Rohit Saxena","Aryo Pradipta Gema","Pasquale Minervini"]},"authorids":{"value":["~Rohit_Saxena3","~Aryo_Pradipta_Gema1","~Pasquale_Minervini4"]},"keywords":{"value":["Multimodal Large Language Models","Temporal Reasoning","Reasoning with Structured Visual Data","Numerical Reasoning"]},"abstract":{"value":"Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) $ClockQA$, which comprises various types of clock styles—standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks—paired with time-related questions; and 2) $CalendarQA$, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year’s Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs."},"pdf":{"value":"/pdf/7a92a51b30e6700aa59d91419fffa954a4b8c441.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsaxena2025clock,\ntitle={Clock and Calendar Understanding Challenges in Multimodal Large Language Models},\nauthor={Rohit Saxena and Aryo Pradipta Gema and Pasquale Minervini},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=5gfC2BmBw6}\n}"},"paperhash":{"value":"saxena|lost_in_time_clock_and_calendar_understanding_challenges_in_multimodal_llms"}},"id":"5gfC2BmBw6","forum":"5gfC2BmBw6","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission116/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission116/Authors"],"number":116,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission116/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738839958874,"cdate":1738839958874,"tmdate":1742297467768,"mdate":1742297467768,"pdate":1741166409917,"odate":1741166409917,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"GRAPE: Generalizing Robot Policy via Preference Alignment"},"authors":{"value":["Zijian Zhang","Kaiyuan Zheng","Zhaorun Chen","Joel Jang","Yi Li","Siwei Han","Chaoqi Wang","Mingyu Ding","Dieter Fox","Huaxiu Yao"]},"authorids":{"value":["~Zijian_Zhang13","~Kaiyuan_Zheng1","~Zhaorun_Chen1","~Joel_Jang1","~Yi_Li9","~Siwei_Han1","~Chaoqi_Wang1","~Mingyu_Ding1","~Dieter_Fox1","~Huaxiu_Yao1"]},"keywords":{"value":["Vision-Language-Action Model","Preference Optimization","Embodied AI"]},"TLDR":{"value":"We propose a novel guided-reinforced preference tuning algorithm to align vision-language-action models towards arbitrary alignment objectives such as better generalizability, safety, and efficiency."},"abstract":{"value":"Despite the recent advancements of vision-language-action (VLA) models on a variety of robotics tasks, they suffer from critical issues such as poor generalizability to unseen tasks, due to their reliance on behavior cloning exclusively from successful rollouts. Furthermore, they are typically fine-tuned to replicate demonstrations collected by experts under different settings, thus introducing distribution bias and limiting their adaptability to diverse manipulation objectives, such as efficiency, safety, and task completion. To bridge this gap, we introduce GRAPE: Generalizing Robot Policy via Preference Alignment. Specifically, GRAPE aligns VLAs on a trajectory level and implicitly models reward from both successful and failure trials to boost generalizability to diverse tasks. Moreover, \\algname breaks down complex manipulation tasks to independent stages and automatically guides preference modeling through customized spatiotemporal constraints with keypoints proposed by a large vision-language model. Notably, these constraints are flexible and can be customized to align the model with varying objectives, such as safety, efficiency, or task success. We evaluate GRAPE across a diverse array of tasks in both real-world and simulated environments. Experimental results demonstrate that GRAPE enhances the performance of state-of-the-art VLA models, increasing success rates on in-domain and unseen manipulation tasks by 51.79% and 58.20%, respectively. Additionally, GRAPE can be aligned with various objectives, such as safety and efficiency, reducing collision rates by 37.44% and rollout step-length by 11.15%, respectively."},"pdf":{"value":"/pdf/e31a969ff9dd61731b413915c6a679621eaacc5d.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhang2025grape,\ntitle={{GRAPE}: Generalizing Robot Policy via Preference Alignment},\nauthor={Zijian Zhang and Kaiyuan Zheng and Zhaorun Chen and Joel Jang and Yi Li and Siwei Han and Chaoqi Wang and Mingyu Ding and Dieter Fox and Huaxiu Yao},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=XnwyFD1Fvw}\n}"},"paperhash":{"value":"zhang|grape_generalizing_robot_policy_via_preference_alignment"}},"id":"XnwyFD1Fvw","forum":"XnwyFD1Fvw","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission115/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission115/Authors"],"number":115,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission115/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738834125183,"cdate":1738834125183,"tmdate":1742398405330,"mdate":1742398405330,"pdate":1741166409873,"odate":1741166409873,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Large Language Model-Enhanced Multi-Armed Bandits"},"authors":{"value":["Jiahang Sun","Zhiyong Wang","Runhan Yang","Chenjun Xiao","John C.S. Lui","Zhongxiang Dai"]},"authorids":{"value":["~Jiahang_Sun1","~Zhiyong_Wang9","~Runhan_Yang1","~Chenjun_Xiao1","~John_C.S._Lui2","~Zhongxiang_Dai1"]},"keywords":{"value":["LLM-based sequential decision-making","multi-armed bandits","dueling bandits"]},"TLDR":{"value":"We propose to adopt classical MAB algorithms as the high-level framework for sequential decision-making and leverage the in-context learning capability of LLMs for reward prediction."},"abstract":{"value":"Large language models (LLMs) have been adopted to solve sequential decision-making tasks such as multi-armed bandits (MAB), in which an LLM is directly instructed to select the arms to pull in every iteration. However, this paradigm of direct arm selection using LLMs has been shown to be suboptimal in many MAB tasks. Therefore, we propose an alternative approach which combines the strengths of classical MAB and LLMs. Specifically, we adopt a classical MAB algorithm as the high-level framework and leverage the strong in-context learning capability of LLMs to perform the sub-task of reward prediction. Firstly, we incorporate the LLM-based reward predictor into the classical Thompson sampling (TS) algorithm and adopt a decaying schedule for the LLM temperature to ensure a transition from exploration to exploitation. Next, we incorporate the LLM-based reward predictor (with a temperature of 0) into a regression oracle-based MAB algorithm equipped with an explicit exploration mechanism. We also extend our TS-based algorithm to dueling bandits where only the preference feedback between pairs of arms is available, which requires non-trivial algorithmic modifications. We conduct empirical evaluations using both synthetic MAB tasks and experiments designed using real-world text datasets, in which the results show that our algorithms consistently outperform previous baseline methods based on direct arm selection. Interestingly, we also demonstrate that in challenging tasks where the arms lack semantic meanings that can be exploited by the LLM, our approach achieves considerably better performance than LLM-based direct arm selection."},"pdf":{"value":"/pdf/95cbe1a4004a8780fbdef22b4edd9055b78b21fe.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsun2025large,\ntitle={Large Language Model-Enhanced Multi-Armed Bandits},\nauthor={Jiahang Sun and Zhiyong Wang and Runhan Yang and Chenjun Xiao and John C.S. Lui and Zhongxiang Dai},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=aUpWZ3B734}\n}"},"paperhash":{"value":"sun|large_language_modelenhanced_multiarmed_bandits"}},"id":"aUpWZ3B734","forum":"aUpWZ3B734","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission113/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission113/Authors"],"number":113,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission113/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738830725189,"cdate":1738830725189,"tmdate":1742451241503,"mdate":1742451241503,"pdate":1741166409825,"odate":1741166409825,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Reinforcement Learning in Inference Time: A Perspective from Successive Policy Iterations"},"authors":{"value":["Xinnan Zhang","Chenliang Li","Siliang Zeng","Jiaxiang Li","Zhongruo Wang","Songtao Lu","Alfredo Garcia","Mingyi Hong"]},"authorids":{"value":["~Xinnan_Zhang3","~Chenliang_Li3","~Siliang_Zeng1","~Jiaxiang_Li1","~Zhongruo_Wang1","~Songtao_Lu1","~Alfredo_Garcia1","~Mingyi_Hong1"]},"keywords":{"value":["Inference-time Alignment; Reinforcement Learning; RLHF"]},"abstract":{"value":"Aligning Large Language Models (LLMs) to human preferences is essential for their effective deployment in real-world applications. Traditional post-training methods, such as Reinforcement Learning with Human Feedback (RLHF), are resource-intensive and time-consuming, especially as model sizes continue to grow. Recently, inference-time alignment methods have gained significant attention, as they can steer the LLM output without direct fine-tuning, and can be integrated with post-training techniques to further enhance performance. Additionally, these methods enable personalization, allowing models to adapt dynamically to user preferences and specific task requirements. However, these approaches operate in a one-shot manner, limiting policy improvement to a single round. To address this limitation, we introduce inference-time Successive Policy Iterations (SPI), a novel algorithm that enables successive policy improvement at inference time. Specifically, inference-time SPI iteratively learns value functions and leverages them to guide the LLM through a search-based optimization process. Theoretically, our algorithm is equivalent to performing multi-iteration policy optimization on the base model, effectively improving its behavior without direct fine-tuning. Experimental results demonstrate that inference-time SPI significantly improves length-control win rates on challenging instruction-following benchmarks, such as AlpacaEval 2.0, achieving a substantial performance boost (e.g., $30.71\\% \\to 43.80\\%$ for \\texttt{Llama-3-8B-Instruct} compare against GPT-4 responses). Furthermore, inference-time SPI consistently outperforms existing test-time alignment baselines such as Best-of-N (BoN), weak to strong search, which is effective for inference time scaling on different tasks."},"pdf":{"value":"/pdf/a75223e25b4fadb945e3ca05c8a5fe5447cf85d6.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhang2025reinforcement,\ntitle={Reinforcement Learning in Inference Time: A Perspective from Successive Policy Iterations},\nauthor={Xinnan Zhang and Chenliang Li and Siliang Zeng and Jiaxiang Li and Zhongruo Wang and Songtao Lu and Alfredo Garcia and Mingyi Hong},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=7ETrvtvRlU}\n}"},"paperhash":{"value":"zhang|reinforcement_learning_in_inference_time_a_perspective_from_successive_policy_iterations"}},"id":"7ETrvtvRlU","forum":"7ETrvtvRlU","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission112/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission112/Authors"],"number":112,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission112/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738830442742,"cdate":1738830442742,"tmdate":1742350661118,"mdate":1742350661118,"pdate":1741166409789,"odate":1741166409789,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models"},"authors":{"value":["Zhanke Zhou","Xuan Li","Zhaocheng Zhu","Mikhail Galkin","Xiao Feng","Sanmi Koyejo","Jian Tang","Bo Han"]},"authorids":{"value":["~Zhanke_Zhou1","~Xuan_Li9","~Zhaocheng_Zhu1","~Mikhail_Galkin1","~Xiao_Feng2","~Sanmi_Koyejo1","~Jian_Tang1","~Bo_Han1"]},"keywords":{"value":["large language models","multi-step reasoning","explanation","visualization"]},"abstract":{"value":"Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative analysis shows that the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a neural model that predicts any property they observe. We showcase this advantage by adapting our tool to a lightweight verifier, which significantly improves reasoning by evaluating the correctness of reasoning paths."},"pdf":{"value":"/pdf/e065ac4ff7841b893afec0d55014cec4b4da0e59.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhou2025landscape,\ntitle={Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models},\nauthor={Zhanke Zhou and Xuan Li and Zhaocheng Zhu and Mikhail Galkin and Xiao Feng and Sanmi Koyejo and Jian Tang and Bo Han},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ktvYoym0nR}\n}"},"paperhash":{"value":"zhou|landscape_of_thoughts_visualizing_the_reasoning_process_of_large_language_models"}},"id":"ktvYoym0nR","forum":"ktvYoym0nR","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission110/Authors"],"number":110,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission110/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738811090534,"cdate":1738811090534,"tmdate":1742441915023,"mdate":1742441915023,"pdate":1741166409714,"odate":1741166409714,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Learning to Defer for Causal Discovery with Imperfect Experts"},"authors":{"value":["Oscar Clivio","Divyat Mahajan","Perouz Taslakian","Sara Magliacane","Ioannis Mitliagkas","Valentina Zantedeschi","Alexandre Drouin"]},"authorids":{"value":["~Oscar_Clivio1","~Divyat_Mahajan1","~Perouz_Taslakian1","~Sara_Magliacane1","~Ioannis_Mitliagkas1","~Valentina_Zantedeschi2","~Alexandre_Drouin2"]},"keywords":{"value":["causal discovery","large language models","imperfect experts","expert knowledge","background knowledge","learning to defer."]},"abstract":{"value":"Integrating expert knowledge, e.g. from large language models, into causal discovery algorithms can be challenging when the knowledge is not guaranteed to be correct. Expert recommendations may contradict data-driven results, and their reliability can vary significantly depending on the domain or specific query. Existing methods based on soft constraints or inconsistencies in predicted causal relationships fail to account for these variations in expertise. To remedy this, we propose L2D-CD, a method for gauging the correctness of expert recommendations and optimally combining them with data-driven causal discovery results. By adapting learning-to-defer (L2D) algorithms for pairwise causal discovery (CD), we learn a deferral function that selects whether to rely on classical causal discovery methods using numerical data or expert recommendations based on textual meta-data. We evaluate L2D-CD on the canonical Tübingen pairs dataset and demonstrate its superior performance compared to both the causal discovery method and the expert used in isolation. Moreover, our approach identifies domains where the expert's performance is strong or weak. Finally, we outline a strategy for generalizing this approach to causal discovery on graphs with more than two variables, paving the way for further research in this area."},"pdf":{"value":"/pdf/3952e475fb7623b497f1fb7700622e163ed2cebe.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nclivio2025learning,\ntitle={Learning to Defer for Causal Discovery with Imperfect Experts},\nauthor={Oscar Clivio and Divyat Mahajan and Perouz Taslakian and Sara Magliacane and Ioannis Mitliagkas and Valentina Zantedeschi and Alexandre Drouin},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=BD21mAyddj}\n}"},"paperhash":{"value":"clivio|learning_to_defer_for_causal_discovery_with_imperfect_experts"}},"id":"BD21mAyddj","forum":"BD21mAyddj","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission109/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission109/Authors"],"number":109,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission109/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738807799845,"cdate":1738807799845,"tmdate":1742470507059,"mdate":1742470507059,"pdate":1741166409610,"odate":1741166409610,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning"},"authors":{"value":["Pan Lu","Bowen Chen","Sheng Liu","Rahul Thapa","Joseph Boen","James Zou"]},"authorids":{"value":["~Pan_Lu2","~Bowen_Chen3","~Sheng_Liu2","~Rahul_Thapa1","~Joseph_Boen1","~James_Zou1"]},"keywords":{"value":["LLM Agents","Complex Reasoning","LLM Planning","Tool-augmented LLMs","Large Language Models"]},"TLDR":{"value":"We introduce OctoTools, a training-free, user-friendly, and extensible agentic framework designed to tackle complex reasoning across diverse domains."},"abstract":{"value":"Solving complex reasoning tasks involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools’ generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3\\% over GPT-4o. Furthermore, OctoTools also outperforms AutoGen, GPT-Functions and LangChain by up to 10.6\\% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving. Code and interactive demos are available at https://octotools.github.io/."},"pdf":{"value":"/pdf/2e2a0c9b807f62c85dad4260bcc8f033e6e681ce.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nlu2025octotools,\ntitle={OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning},\nauthor={Pan Lu and Bowen Chen and Sheng Liu and Rahul Thapa and Joseph Boen and James Zou},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=YzKOW0zD1d}\n}"},"paperhash":{"value":"lu|octotools_an_agentic_framework_with_extensible_tools_for_complex_reasoning"}},"id":"YzKOW0zD1d","forum":"YzKOW0zD1d","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission108/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission108/Authors"],"number":108,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission108/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738802410244,"cdate":1738802410244,"tmdate":1742265868945,"mdate":1742265868945,"pdate":1741166409607,"odate":1741166409607,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst"},"authors":{"value":["Hongru WANG","Deng Cai","Wanjun Zhong","Shijue Huang","Jeff Z. Pan","Zeming Liu","Kam-Fai Wong"]},"authorids":{"value":["~Hongru_WANG1","~Deng_Cai1","~Wanjun_Zhong1","~Shijue_Huang1","~Jeff_Z._Pan1","~Zeming_Liu1","~Kam-Fai_Wong2"]},"keywords":{"value":["Large Language Models","Self-Reasoning Language Models"]},"TLDR":{"value":"We introduce Self-Reasoning Language Models  (SRLM), which is capable to act as both: a response generation model; and 2) reasoning models to refine its own reasoning rationales."},"abstract":{"value":"Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce \\textit{Self-Reasoning Language Model} (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model's initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than $+2.5$ points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute $+7.89$ average improvement with $64$ sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline."},"pdf":{"value":"/pdf/0ebcc7889c12f8ba090b266c3c34790869daeb4a.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nwang2025selfreasoning,\ntitle={Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst},\nauthor={Hongru WANG and Deng Cai and Wanjun Zhong and Shijue Huang and Jeff Z. Pan and Zeming Liu and Kam-Fai Wong},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=p4wXiD8FX1}\n}"},"paperhash":{"value":"wang|selfreasoning_language_models_unfold_hidden_reasoning_chains_with_few_reasoning_catalyst"}},"id":"p4wXiD8FX1","forum":"p4wXiD8FX1","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission107/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission107/Authors"],"number":107,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission107/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738795535653,"cdate":1738795535653,"tmdate":1741976721454,"mdate":1741976721454,"pdate":1741166409548,"odate":1741166409548,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration"},"authors":{"value":["Qinglin Zhu","Runcong Zhao","Hanqi Yan","Yulan He","Yudong Chen","Lin Gui"]},"authorids":{"value":["~Qinglin_Zhu1","~Runcong_Zhao1","~Hanqi_Yan2","~Yulan_He1","~Yudong_Chen8","~Lin_Gui3"]},"keywords":{"value":["Large Language Models","Reasoning","Embedding Perturbation","Bayesian Optimisation"]},"TLDR":{"value":"We propose an embedding-based search framework that optimises the first token's embedding through controlled perturbation and Bayesian refinement to enhance reasoning accuracy and coherence in large language models with minimal computation."},"abstract":{"value":"Large Language Models (LLMs) struggle with reasoning due to limited diversity and inefficient search. We propose an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) Embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution."},"pdf":{"value":"/pdf/eb012cc8eaccd6e89bc9c6f943fd5d47af5a764f.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhu2025navigating,\ntitle={Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration},\nauthor={Qinglin Zhu and Runcong Zhao and Hanqi Yan and Yulan He and Yudong Chen and Lin Gui},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=Pp90xRxITT}\n}"},"paperhash":{"value":"zhu|navigating_solution_spaces_in_large_language_models_through_controlled_embedding_exploration"}},"id":"Pp90xRxITT","forum":"Pp90xRxITT","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission106/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission106/Authors"],"number":106,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission106/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738778698710,"cdate":1738778698710,"tmdate":1742383934738,"mdate":1742383934738,"pdate":1741166409466,"odate":1741166409466,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Chain-of-Thought Reasoning in the Wild is not Always Faithful"},"authors":{"value":["Iván Arcuschin","Jett Janiak","Robert Krzyzanowski","Senthooran Rajamanoharan","Neel Nanda","Arthur Conmy"]},"authorids":{"value":["~Iván_Arcuschin1","~Jett_Janiak1","~Robert_Krzyzanowski1","~Senthooran_Rajamanoharan1","~Neel_Nanda1","~Arthur_Conmy1"]},"keywords":{"value":["Chain-of-Thought","Faithfulness","AI Safety"]},"TLDR":{"value":"We show that Chain-of-Thought reasoning is not faithful is frontier models, in unbiased contexts"},"abstract":{"value":"Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal non-negligible rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and ChatGPT-4o (7.0%) all answer a notable proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (\"implicit post-hoc rationalization\"). For example, when separately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior."},"pdf":{"value":"/pdf/054ce351d773fda02a12cd05550a72cc091efb18.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\narcuschin2025chainofthought,\ntitle={Chain-of-Thought Reasoning in the Wild is not Always Faithful},\nauthor={Iv{\\'a}n Arcuschin and Jett Janiak and Robert Krzyzanowski and Senthooran Rajamanoharan and Neel Nanda and Arthur Conmy},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=L8094Whth0}\n}"},"paperhash":{"value":"arcuschin|chainofthought_reasoning_in_the_wild_is_not_always_faithful"}},"id":"L8094Whth0","forum":"L8094Whth0","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission105/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission105/Authors"],"number":105,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission105/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738775452426,"cdate":1738775452426,"tmdate":1742411416024,"mdate":1742411416024,"pdate":1741166409415,"odate":1741166409415,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures"},"authors":{"value":["Fu-Chieh Chang","You-Chen Lin","Pei-Yuan Wu"]},"authorids":{"value":["~Fu-Chieh_Chang1","~You-Chen_Lin1","~Pei-Yuan_Wu1"]},"keywords":{"value":["large language model","mathematical learning"]},"TLDR":{"value":"We show that large language model can learn the algebraic structure such as commutativity and identity."},"abstract":{"value":"The reasoning abilities of large language models (LLMs) have improved with chain-of-thought (CoT) prompting, allowing models to solve complex tasks stepwise. However, training CoT capabilities requires detailed reasoning data, which is often scarce. The self-taught reasoner (STaR) framework addresses this by using reinforcement learning to automatically generate reasoning steps, reducing reliance on human-labeled data. Although STaR and its variants have demonstrated empirical success, a theoretical foundation explaining these improvements is lacking. \nLarge language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks. In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties. Since these structures are observable through input-output relationships, they can generalize to unseen data. We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformer-based LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements. Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance."},"pdf":{"value":"/pdf/ed3670a170cb532cc7b8d9b0fb216293486dcfd0.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nchang2025unraveling,\ntitle={Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures},\nauthor={Fu-Chieh Chang and You-Chen Lin and Pei-Yuan Wu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=aNmbQ4kGSQ}\n}"},"paperhash":{"value":"chang|unraveling_arithmetic_in_large_language_models_the_role_of_algebraic_structures"}},"id":"aNmbQ4kGSQ","forum":"aNmbQ4kGSQ","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission103/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission103/Authors"],"number":103,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission103/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738770118028,"cdate":1738770118028,"tmdate":1742382818171,"mdate":1742382818171,"pdate":1741166409374,"odate":1741166409374,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner"},"authors":{"value":["Fu-Chieh Chang","Yu-Ting Lee","Hui-Ying Shih","Yi Hsuan Tseng","Pei-Yuan Wu"]},"authorids":{"value":["~Fu-Chieh_Chang1","~Yu-Ting_Lee2","~Hui-Ying_Shih1","~Yi_Hsuan_Tseng1","~Pei-Yuan_Wu1"]},"keywords":{"value":["large language model","reinforcement learning","chain of thought"]},"TLDR":{"value":"We provide theoretical analysis for self-taught reasoner."},"abstract":{"value":"The reasoning abilities of large language models (LLMs) have improved with chain-of-thought (CoT) prompting, allowing models to solve complex tasks stepwise. However, training CoT capabilities requires detailed reasoning data, which is often scarce. The self-taught reasoner (STaR) framework addresses this by using reinforcement learning to automatically generate reasoning steps, reducing reliance on human-labeled data. Although STaR and its variants have demonstrated empirical success, a theoretical foundation explaining these improvements is lacking. This work provides a theoretical framework for understanding the effectiveness of reinforcement learning on CoT reasoning and STaR. Our contributions are: (1) criteria for the quality of pre-trained models necessary to initiate effective reasoning improvement; (2) an analysis of policy improvement, showing why LLM reasoning improves iteratively with STaR; (3) conditions for convergence to an optimal reasoning policy; and (4) an examination of STaR’s robustness, explaining how it can improve reasoning even when incorporating occasional incorrect steps; This framework aims to bridge empirical findings with theoretical insights, advancing reinforcement learning approaches for reasoning in LLMs."},"pdf":{"value":"/pdf/a592592cabc0bfa59c2d2b71c16f4ad76cd12b0c.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nchang2025rlstar,\ntitle={{RL}-{ST}aR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner},\nauthor={Fu-Chieh Chang and Yu-Ting Lee and Hui-Ying Shih and Yi Hsuan Tseng and Pei-Yuan Wu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=Oo2XthxKB9}\n}"},"paperhash":{"value":"chang|rlstar_theoretical_analysis_of_reinforcement_learning_frameworks_for_selftaught_reasoner"}},"id":"Oo2XthxKB9","forum":"Oo2XthxKB9","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission102/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission102/Authors"],"number":102,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission102/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738767494043,"cdate":1738767494043,"tmdate":1742396888672,"mdate":1742396888672,"pdate":1741166409301,"odate":1741166409301,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Enhancing Mathematical Reasoning in Language Models Through Focused Differentiation Training"},"authors":{"value":["Zhiyu Zhao","Yongcheng Zeng","Ning Yang","Zihan Zhao","Haifeng Zhang","Jun Wang","Guoqing Liu"]},"authorids":{"value":["~Zhiyu_Zhao1","~Yongcheng_Zeng1","~Ning_Yang5","~Zihan_Zhao1","~Haifeng_Zhang3","~Jun_Wang2","~Guoqing_Liu3"]},"keywords":{"value":["large language model","alignment"]},"TLDR":{"value":"The paper proposes a method to improve LLMs' math capabilities by analyzing hidden state differences between correct and incorrect solutions, outperforming existing approaches without external models or supervision."},"abstract":{"value":"Enhancing the mathematical capabilities of large language models (LLMs) is crucial for applications requiring precise and rigorous mathematical reasoning. Current models, even when trained with methods like Direct Preference Optimization (DPO), often struggle to effectively differentiate between correct and erroneous mathematical responses, especially when errors occur in multi-step solutions. Traditional approaches focusing on token or logit-level analysis fail to capture the nuanced semantic differences in mathematical reasoning. To address this challenge, we propose leveraging the rich semantic information embedded in the hidden state space of LLMs. Our novel approach, Focused Differentiation Training (FDT), fine-tunes the model by emphasizing the differences between the hidden states of correct and incorrect responses, rather than their common features. Unlike other methods that detect errors at the token or logits level and often rely on human input or more powerful models, our approach enhances mathematical reasoning capabilities using only the model's inherent abilities. This methodology promotes a more accurate alignment with mathematical correctness, thereby improving the model's ability to evaluate and generate precise mathematical responses. Experimental results demonstrate that our algorithm substantially outperforms traditional alignment methods in mathematical tasks, offering a robust solution for enhancing the mathematical reasoning capabilities of language models."},"pdf":{"value":"/pdf/0fc865f7e7e9c418f05838d3e5106e2418b3de55.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhao2025enhancing,\ntitle={Enhancing Mathematical Reasoning in Language Models Through Focused Differentiation Training},\nauthor={Zhiyu Zhao and Yongcheng Zeng and Ning Yang and Zihan Zhao and Haifeng Zhang and Jun Wang and Guoqing Liu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=UH8kT7gVjj}\n}"},"paperhash":{"value":"zhao|enhancing_mathematical_reasoning_in_language_models_through_focused_differentiation_training"}},"id":"UH8kT7gVjj","forum":"UH8kT7gVjj","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission101/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission101/Authors"],"number":101,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission101/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738766672785,"cdate":1738766672785,"tmdate":1742369101609,"mdate":1742369101609,"pdate":1741166409283,"odate":1741166409283,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use"},"authors":{"value":["Kaixin Li","Meng ziyang","Hongzhan Lin","Ziyang Luo","Yuchen Tian","Jing Ma","Zhiyong Huang","Tat-Seng Chua"]},"authorids":{"value":["~Kaixin_Li1","~Meng_ziyang1","~Hongzhan_Lin1","~Ziyang_Luo2","~Yuchen_Tian3","~Jing_Ma4","~Zhiyong_Huang1","~Tat-Seng_Chua2"]},"keywords":{"value":["Visual Reasoning","Visual Search","GUI Agent","GUI Grouding"]},"TLDR":{"value":"We propose a visual search method to improve GUI grounding in professional high-resolution computer use, almost tripling the performance of previous SOTA model."},"abstract":{"value":"Multi-modal large language models (MLLMs) are rapidly advancing in visual understanding and reasoning, enhancing GUI agents for tasks such as web browsing and mobile interactions. However, these agents depend on reasoning skills for action planning but only rely on the model capability for UI grounding (localizing the target element). These grounding models struggle with high-resolution displays, small targets, and complex environments. In this work, we introduce a novel method to improve MLLMs’ grounding performance in high-resolution, complex UI environments using a visual search approach based on visual reasoning. Additionally, we create a new benchmark, dubbed ScreenSpot-Pro, designed to comprehensively evaluate model capabilities in professional high-resolution settings. This benchmark consists of real-world high-resolution images and expert-annotated tasks from diverse professional domains. Our experiments show that existing GUI grounding models perform poorly on this dataset, with the best achieving only 18.9\\%, whereas our visual-reasoning strategy significantly improves performance, reaching 48.1\\% without any additional training."},"pdf":{"value":"/pdf/adcc235fac27c7b157f6b3c3c13f96fa76878e7e.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nli2025screenspotpro,\ntitle={ScreenSpot-Pro: {GUI} Grounding for Professional High-Resolution Computer Use},\nauthor={Kaixin Li and Meng ziyang and Hongzhan Lin and Ziyang Luo and Yuchen Tian and Jing Ma and Zhiyong Huang and Tat-Seng Chua},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=XaKNDIAHas}\n}"},"paperhash":{"value":"li|screenspotpro_gui_grounding_for_professional_highresolution_computer_use"}},"id":"XaKNDIAHas","forum":"XaKNDIAHas","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission100/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission100/Authors"],"number":100,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission100/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738765257343,"cdate":1738765257343,"tmdate":1742456771849,"mdate":1742456771849,"pdate":1741166409247,"odate":1741166409247,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"RuleArena: A Benchmark for LLM Rule-Guided Reasoning in Real-World Scenarios"},"authors":{"value":["Ruiwen Zhou","Wenyue Hua","Liangming Pan","Sitao Cheng","Xiaobao Wu","En Yu","William Yang Wang"]},"authorids":{"value":["~Ruiwen_Zhou1","~Wenyue_Hua1","~Liangming_Pan1","~Sitao_Cheng1","~Xiaobao_Wu1","~En_Yu1","~William_Yang_Wang2"]},"keywords":{"value":["Large Language Models","Complex Reasoning","Rule Following"]},"abstract":{"value":"This paper introduces RuleArena, a challenging benchmark to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains---airline baggage fees, NBA transactions, and tax regulations---RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate math computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. We also observe a significant performance boost when LLMs are provided with external tools. These results highlight significant challenges and promising directions in advancing LLMs' rule-guided reasoning capabilities in real-life applications."},"pdf":{"value":"/pdf/18e2217535b1587af6b2d9989b8d424da0f2e3e3.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhou2025rulearena,\ntitle={RuleArena: A Benchmark for {LLM} Rule-Guided Reasoning in Real-World Scenarios},\nauthor={Ruiwen Zhou and Wenyue Hua and Liangming Pan and Sitao Cheng and Xiaobao Wu and En Yu and William Yang Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=hbzz5aIC3m}\n}"},"paperhash":{"value":"zhou|rulearena_a_benchmark_for_llm_ruleguided_reasoning_in_realworld_scenarios"}},"id":"hbzz5aIC3m","forum":"hbzz5aIC3m","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission99/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission99/Authors"],"number":99,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission99/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738764692146,"cdate":1738764692146,"tmdate":1742392298687,"mdate":1742392298687,"pdate":1741166409142,"odate":1741166409142,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Meta-Prompt Optimization for LLM-Based Sequential Decision Making"},"authors":{"value":["Mingze Kong","Zhiyong Wang","Yao Shu","Zhongxiang Dai"]},"authorids":{"value":["~Mingze_Kong1","~Zhiyong_Wang9","~Yao_Shu1","~Zhongxiang_Dai1"]},"keywords":{"value":["LLM-based sequential decision-making","multi-armed bandits","prompt optimization"]},"TLDR":{"value":"We propose an adversarial bandit-based approach to automatically optimize the meta-prompt for LLM-based sequential decision-making."},"abstract":{"value":"Large language models (LLMs) have recently been employed as agents to solve sequential decision-making tasks such as Bayesian optimization and multi-armed bandits (MAB). These works usually adopt an LLM for sequential action selection by providing it with a fixed, manually designed meta-prompt. However, numerous previous works have found that the prompt has a significant impact on the performance of the LLM, which calls for a method to automatically optimize the meta-prompt for LLM-based agents. Unfortunately, the non-stationarity in the reward observations during LLM-based sequential decision-making makes meta-prompt optimization highly challenging. To address this challenge, we draw inspirations from adversarial bandit algorithms, which are inherently capable of handling non-stationary reward observations. Building on this foundation, we propose our EXPonential-weight algorithm for prompt Optimization (EXPO) to automatically optimize the task description and meta-instruction in the meta-prompt for LLM-based agents. We also extend EXPO to additionally optimize the exemplars (i.e., history of interactions) in the meta-prompt to further enhance the performance, hence introducing our EXPO-ES algorithm. We use extensive experiments to show that our algorithms significantly improve the performance of LLM-based sequential decision-making."},"pdf":{"value":"/pdf/6fd5f2ba860a2d1641427c3f284b65e5435bd6f7.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nkong2025metaprompt,\ntitle={Meta-Prompt Optimization for {LLM}-Based Sequential Decision Making},\nauthor={Mingze Kong and Zhiyong Wang and Yao Shu and Zhongxiang Dai},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=JPYOjDuZg8}\n}"},"paperhash":{"value":"kong|metaprompt_optimization_for_llmbased_sequential_decision_making"}},"id":"JPYOjDuZg8","forum":"JPYOjDuZg8","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission98/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission98/Authors"],"number":98,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission98/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738763321965,"cdate":1738763321965,"tmdate":1742451191038,"mdate":1742451191038,"pdate":1741166409031,"odate":1741166409031,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"When Debate Fails: Bias Reinforcement in Large Language Models"},"authors":{"value":["Jihwan Oh","Minchan Jeong","Jongwoo Ko","Se-Young Yun"]},"authorids":{"value":["~Jihwan_Oh1","~Minchan_Jeong1","~Jongwoo_Ko1","~Se-Young_Yun1"]},"keywords":{"value":["Large Language Models","Debate","Reasoning","Bias"]},"TLDR":{"value":"We show that Multi-Agent Debate (MAD) reinforces inherent biases due to strong self-consistency, and propose DReaMAD, which refines prior knowledge and promotes diverse reasoning, outperforming MAD in MetaNIM Arena."},"abstract":{"value":"Large Language Models (LLMs) solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate (MAD) has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD’s limitations, we propose $\\texttt{\\textbf{DReaMAD}}$ ($\\textbf{D}$iverse $\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate with Refined Prompt), a novel framework that (1) refines LLMs’ strategic prior knowledge to improve reasoning quality and (2) promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\\texttt{\\textbf{DReaMAD}}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making."},"pdf":{"value":"/pdf/ff85ebe040cdbc4a8e671056957e4e770d3bc52f.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\noh2025when,\ntitle={When Debate Fails: Bias Reinforcement in Large Language Models},\nauthor={Jihwan Oh and Minchan Jeong and Jongwoo Ko and Se-Young Yun},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=c5bjw7hqix}\n}"},"paperhash":{"value":"oh|when_debate_fails_bias_reinforcement_in_large_language_models"}},"id":"c5bjw7hqix","forum":"c5bjw7hqix","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission97/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission97/Authors"],"number":97,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission97/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738761745287,"cdate":1738761745287,"tmdate":1742471002129,"mdate":1742471002129,"pdate":1741166409028,"odate":1741166409028,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems"},"authors":{"value":["Kaixin Li","Yuchen Tian","Qisheng Hu","Ziyang Luo","Zhiyong Huang","Jing Ma"]},"authorids":{"value":["~Kaixin_Li1","~Yuchen_Tian3","~Qisheng_Hu1","~Ziyang_Luo2","~Zhiyong_Huang1","~Jing_Ma4"]},"keywords":{"value":["Visual Reasoning","Large Multimodal Models","Code Generation"]},"TLDR":{"value":"We present MMCode, the first multi-modal coding dataset for evaluating algorithmic problem-solving skills in visually rich contexts."},"abstract":{"value":"Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models have demonstrated remarkable abilities in visual reasoning and mathematical tasks, there is little work on investigating whether these models can effectively interpret visual elements for code generation. To this end, we present MMCode, the first multi-modal coding dataset for evaluating algorithmic problem-solving skills in visually rich contexts. MMCode contains 3,548 questions and 6,620 images collected from real-world programming challenges harvested from 10 code competition websites, presenting significant challenges due to the extreme demand for reasoning abilities. Our experiment results show that current state-of-the-art models struggle to solve these problems. The results highlight the lack of powerful vision-code models, and we hope MMCode can serve as an inspiration for future works in this domain. The data and code are publicly available."},"pdf":{"value":"/pdf/65e40418e76004fc53b6a26af8b5874a70aa8164.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nli2025mmcode,\ntitle={{MMC}ode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems},\nauthor={Kaixin Li and Yuchen Tian and Qisheng Hu and Ziyang Luo and Zhiyong Huang and Jing Ma},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=aXt81rtgTB}\n}"},"paperhash":{"value":"li|mmcode_benchmarking_multimodal_large_language_models_in_code_generation_with_visually_rich_programming_problems"}},"id":"aXt81rtgTB","forum":"aXt81rtgTB","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission96/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission96/Authors"],"number":96,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission96/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738755185633,"cdate":1738755185633,"tmdate":1742456324447,"mdate":1742456324447,"pdate":1741166409026,"odate":1741166409026,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory"},"authors":{"value":["Nikola Zubic","Federico Soldà","Aurelio Sulser","Davide Scaramuzza"]},"authorids":{"value":["~Nikola_Zubic1","~Federico_Soldà1","~Aurelio_Sulser1","~Davide_Scaramuzza1"]},"keywords":{"value":["theory","complexity theory","state space models","deep learning architectures","logic in computer science"]},"TLDR":{"value":"We theoretically prove the shortcomings of current deep learning models with a specific focus on State Space Models."},"abstract":{"value":"Despite their successes, deep learning models struggle with tasks requiring complex reasoning and function composition. We present a theoretical and empirical investigation into the limitations of Structured State Space Models (SSMs) and Transformers in such tasks. We prove that one-layer SSMs cannot efficiently perform function composition over large domains without impractically large state sizes, and even with Chain-of-Thought prompting, they require a number of steps that scale unfavorably with the complexity of the function composition. Also, the language of a finite-precision SSM is within the class of regular languages. Our experiments corroborate these theoretical findings. Evaluating models on tasks including various function composition settings, multi-digit multiplication, dynamic programming, and Einstein's puzzle, we find significant performance degradation even with advanced prompting techniques. Models often resort to shortcuts, leading to compounding errors. These findings highlight fundamental barriers within current deep learning architectures rooted in their computational capacities. We underscore the need for innovative solutions to transcend these constraints and achieve reliable multi-step reasoning and compositional task-solving, which is critical for advancing toward general artificial intelligence."},"pdf":{"value":"/pdf/19623b0a0c5f2a385e934cc2c7c4984c9e4cbe84.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzubic2025limits,\ntitle={Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory},\nauthor={Nikola Zubic and Federico Sold{\\`a} and Aurelio Sulser and Davide Scaramuzza},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=sBwQzsyeYX}\n}"},"paperhash":{"value":"zubic|limits_of_deep_learning_sequence_modeling_through_the_lens_of_complexity_theory"}},"id":"sBwQzsyeYX","forum":"sBwQzsyeYX","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission95/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission95/Authors"],"number":95,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission95/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738749556543,"cdate":1738749556543,"tmdate":1742043141692,"mdate":1742043141692,"pdate":1741166408883,"odate":1741166408883,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage"},"authors":{"value":["Zhi Gao","Bofei Zhang","Pengxiang Li","Xiaojian Ma","Tao Yuan","Yue Fan","Yuwei Wu","Yunde Jia","Song-Chun Zhu","Qing Li"]},"authorids":{"value":["~Zhi_Gao5","~Bofei_Zhang1","~Pengxiang_Li1","~Xiaojian_Ma1","~Tao_Yuan6","~Yue_Fan2","~Yuwei_Wu1","~Yunde_Jia1","~Song-Chun_Zhu1","~Qing_Li1"]},"keywords":{"value":["Agent","Multimodal Learning","Tool Learning"]},"abstract":{"value":"The advancement of large language models (LLMs) prompts the development of multi-modal agents, providing a feasible way to solve practical tasks by using tools. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o model to separately generate queries, files, and trajectories, followed by a query-file verifier and trajectory verifier. Based on the data synthesis pipeline, we collect the MM-traj dataset with 20k tasks using 10 tools. Then, we build the T3-agent that uses MiniCPM-V as the controller Trajectory Tuning for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-agent has achieved remarkable improvements and outperforms GPT-4 driven agents by 10%, showing the effectiveness of the proposed data synthesis pipeline that leads to better reasoning capabilities in tool usage."},"pdf":{"value":"/pdf/6a336fd6d17243723024a2ff641958825daf2168.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\ngao2025multimodal,\ntitle={Multi-modal Agent Tuning: Building a {VLM}-Driven Agent for Efficient Tool Usage},\nauthor={Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaojian Ma and Tao Yuan and Yue Fan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=BYUJycKQUy}\n}"},"paperhash":{"value":"gao|multimodal_agent_tuning_building_a_vlmdriven_agent_for_efficient_tool_usage"}},"id":"BYUJycKQUy","forum":"BYUJycKQUy","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission93/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission93/Authors"],"number":93,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission93/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738725913356,"cdate":1738725913356,"tmdate":1742175251394,"mdate":1742175251394,"pdate":1741166408828,"odate":1741166408828,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"WebWalker: Benchmarking LLMs in Web Traversal"},"authors":{"value":["Jialong Wu","Wenbiao Yin","Yong Jiang","Zhenglin Wang","Zekun Xi","Runnan Fang","Linhai Zhang","Yulan He","Deyu Zhou","Pengjun Xie","Fei Huang"]},"authorids":{"value":["~Jialong_Wu3","~Wenbiao_Yin1","~Yong_Jiang1","~Zhenglin_Wang1","~Zekun_Xi2","~Runnan_Fang1","~Linhai_Zhang1","~Yulan_He1","~Deyu_Zhou1","~Pengjun_Xie2","~Fei_Huang2"]},"keywords":{"value":["Web Agent","Benchmarking","RAG","Multi-Agent"]},"abstract":{"value":"Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering.\nHowever, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information.\nTo address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal.\nIt evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. \nWe propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm.\nExtensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios."},"pdf":{"value":"/pdf/2b352174d4fd3d1aa111f8e857a4ce0e4d166c3c.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nwu2025webwalker,\ntitle={WebWalker: Benchmarking {LLM}s in Web Traversal},\nauthor={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Linhai Zhang and Yulan He and Deyu Zhou and Pengjun Xie and Fei Huang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=cVI9lAfkuK}\n}"},"paperhash":{"value":"wu|webwalker_benchmarking_llms_in_web_traversal"}},"id":"cVI9lAfkuK","forum":"cVI9lAfkuK","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission92/Authors"],"number":92,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission92/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738724080407,"cdate":1738724080407,"tmdate":1741704895959,"mdate":1741704895959,"pdate":1741166408737,"odate":1741166408737,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"MFC-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models"},"authors":{"value":["Shengkang Wang","Hongzhan Lin","Ziyang Luo","Zhen Ye","Guang Chen","Jing Ma"]},"authorids":{"value":["~Shengkang_Wang1","~Hongzhan_Lin1","~Ziyang_Luo2","~Zhen_Ye2","~Guang_Chen6","~Jing_Ma4"]},"keywords":{"value":["benchmarking","evaluation","cross-modal application","multimodality"]},"abstract":{"value":"Large vision-language models (LVLMs) have significantly improved multimodal reasoning tasks, such as visual question answering and image captioning. These models embed multimodal facts within their parameters, rather than relying on external knowledge bases to store factual information explicitly. However, the content discerned by LVLMs may deviate from factuality due to inherent bias or incorrect inference. In this work, we introduce MFC-Bench, a rigorous and comprehensive benchmark designed to evaluate the factual accuracy of LVLMs across three stages of verdict prediction for multimodal fact-checking (MFC): Manipulation, Out-of-Context, and Veracity Classification. Through our evaluation on MFC-Bench, we benchmarked a dozen diverse and representative LVLMs, uncovering that current models still fall short in MFC and demonstrate insensitivity to various forms of manipulated content. We hope that MFC-Bench could raise attention to the trustworthy AI potentially assisted by LVLMs in the future."},"pdf":{"value":"/pdf/f63112a3f95bfe76f5fdf5efa31e254cc1e049ed.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nwang2025mfcbench,\ntitle={{MFC}-Bench: Benchmarking Multimodal Fact-Checking with Large Vision-Language Models},\nauthor={Shengkang Wang and Hongzhan Lin and Ziyang Luo and Zhen Ye and Guang Chen and Jing Ma},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=llGi9CqfSr}\n}"},"paperhash":{"value":"wang|mfcbench_benchmarking_multimodal_factchecking_with_large_visionlanguage_models"}},"id":"llGi9CqfSr","forum":"llGi9CqfSr","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission91/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission91/Authors"],"number":91,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission91/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738723619788,"cdate":1738723619788,"tmdate":1741840404765,"mdate":1741840404765,"pdate":1741166408675,"odate":1741166408675,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance"},"authors":{"value":["Yongchao Chen","Yilun Hao","Yueying Liu","Yang Zhang","Chuchu Fan"]},"authorids":{"value":["~Yongchao_Chen1","~Yilun_Hao1","~Yueying_Liu3","~Yang_Zhang3","~Chuchu_Fan2"]},"keywords":{"value":["Large Language Models","Code Interpreter","Code/text generation","Symbolic computing","Model fine-tuning","Model reasoning and planning"]},"TLDR":{"value":"Method CodeSteer to augment LLM capabilities by guiding LLM code/text generation, and SymBench for evaluation of symbolic tasks."},"abstract":{"value":"Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks."},"pdf":{"value":"/pdf/33c67ac53adce572a564d4ccec68f338f7ded6d1.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nchen2025codesteer,\ntitle={CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance},\nauthor={Yongchao Chen and Yilun Hao and Yueying Liu and Yang Zhang and Chuchu Fan},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=EoPHz3r9Mj}\n}"},"paperhash":{"value":"chen|codesteer_symbolicaugmented_language_models_via_codetext_guidance"}},"id":"EoPHz3r9Mj","forum":"EoPHz3r9Mj","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission89/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission89/Authors"],"number":89,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission89/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738706022703,"cdate":1738706022703,"tmdate":1742239522301,"mdate":1742239522301,"pdate":1741166408607,"odate":1741166408607,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"ARIES: Stimulating Self-Refinement of Large Language Models with and for Iterative Preference Optimization"},"authors":{"value":["Yongcheng Zeng","Xuanfa Jin","Guoqing Liu","Quan He","Dong Li","Jianye HAO","Haifeng Zhang","Jun Wang"]},"authorids":{"value":["~Yongcheng_Zeng1","~Xuanfa_Jin1","~Guoqing_Liu3","~Quan_He1","~Dong_Li18","~Jianye_HAO1","~Haifeng_Zhang3","~Jun_Wang2"]},"keywords":{"value":["Reinforcement Learning from Human Feedback (RLHF)","Large language models (LLMs)，Iterative Preference Training","Self-Improvement"]},"TLDR":{"value":"We present ARIES, a novel framework designed to stimulate and enhance the self-refinement capability of large language models (LLMs), which in turn facilitates iterative preference optimization for further improvement of their performance."},"abstract":{"value":"A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions. However, even the most advanced models often face challenges in improving their outputs. In this paper, we explore how to cultivate LLMs with the self-refinement capability through iterative preference training, and how this ability can be leveraged to improve model performance during inference. To this end, we introduce a novel post-training and inference framework, called ARIES: Adaptive Refinement and Iterative Enhancement Structure. This method iteratively performs preference training and self-refinement-based data collection. During training, ARIES strengthen the model's direct question-answering capability while simultaneously unlocking its self-refinement potential. During inference, ARIES harnesses this self-refinement capability to generate a series of progressively refined responses, which are then filtered using either the Reward Model Scoring or a simple yet effective Rule-Based Selection mechanism, specifically tailored to our approach, to construct a dataset for the next round of preference training. Experimental results demonstrate the remarkable performance of ARIES. When applied to the Llama-3.1-8B model and under the self-refinement setting, ARIES surpasses powerful models such as GPT-4o, achieving 62.3% length-controlled (LC) and a 63.3% raw win rates on AlpacaEval 2, outperforming Iterative DPO by 27.8% and 35.5% respectively, as well as a 50.3% win rate on Arena-Hard, surpassing Iterative DPO by 26.6%. Furthermore, ARIES consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH."},"pdf":{"value":"/pdf/a2efaa1be97374c822308de0c33db25a9c57515d.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzeng2025aries,\ntitle={{ARIES}: Stimulating Self-Refinement of Large Language Models with and for Iterative Preference Optimization},\nauthor={Yongcheng Zeng and Xuanfa Jin and Guoqing Liu and Quan He and Dong Li and Jianye HAO and Haifeng Zhang and Jun Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=TAoxjwt4KP}\n}"},"paperhash":{"value":"zeng|aries_stimulating_selfrefinement_of_large_language_models_with_and_for_iterative_preference_optimization"}},"id":"TAoxjwt4KP","forum":"TAoxjwt4KP","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission88/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission88/Authors"],"number":88,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission88/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738699424444,"cdate":1738699424444,"tmdate":1742388018116,"mdate":1742388018116,"pdate":1741166408549,"odate":1741166408549,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Re-Imagine: Symbolic Benchmark Synthesis for Reasoning Evaluation"},"authors":{"value":["Xinnuo Xu","Rachel Lawrence","Kshitij Dubey","Atharva Pandey","Fabian Falck","Risa Ueno","Aditya V. Nori","Rahul Sharma","Amit Sharma","Javier Gonzalez"]},"authorids":{"value":["~Xinnuo_Xu1","~Rachel_Lawrence1","~Kshitij_Dubey1","~Atharva_Pandey1","~Fabian_Falck1","~Risa_Ueno1","~Aditya_V._Nori1","~Rahul_Sharma5","~Amit_Sharma3","~Javier_Gonzalez2"]},"keywords":{"value":["Reasoning","Large Language Models","Causality","Evaluation","Benchmarks"]},"TLDR":{"value":"We introduce RE-IMAGINE, a framework and automated benchmark synthesis pipeline to measure reasoning ability in LLMs."},"abstract":{"value":"Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true “reasoning” or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE: a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy."},"pdf":{"value":"/pdf/3d2e0f8ae05b41ec8616d4f19f89e2840bdded16.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nxu2025reimagine,\ntitle={Re-Imagine: Symbolic Benchmark Synthesis for Reasoning Evaluation},\nauthor={Xinnuo Xu and Rachel Lawrence and Kshitij Dubey and Atharva Pandey and Fabian Falck and Risa Ueno and Aditya V. Nori and Rahul Sharma and Amit Sharma and Javier Gonzalez},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=XZ2Cu6F5sO}\n}"},"paperhash":{"value":"xu|reimagine_symbolic_benchmark_synthesis_for_reasoning_evaluation"}},"id":"XZ2Cu6F5sO","forum":"XZ2Cu6F5sO","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission87/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission87/Authors"],"number":87,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission87/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738692053165,"cdate":1738692053165,"tmdate":1742464042864,"mdate":1742464042864,"pdate":1741166408547,"odate":1741166408547,"version":2,"details":{"writable":false,"replyCount":4,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind"},"authors":{"value":["Zhining Zhang","Chuanyang Jin","Mung Yao Jia","Tianmin Shu"]},"authorids":{"value":["~Zhining_Zhang1","~Chuanyang_Jin2","~Mung_Yao_Jia1","~Tianmin_Shu1"]},"keywords":{"value":["Theory of Mind","Bayesian inference","large language models","inverse planning"]},"TLDR":{"value":"We propose AutoToM, a method that automates inverse planning and model discovery to achieve open-ended Theory of Mind."},"abstract":{"value":"Theory of Mind (ToM), the ability to understand people's mental variables based on their behavior, is key to developing socially intelligent agents. Current approaches to Theory of Mind reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use rigid, handcrafted Bayesian Theory of Mind (BToM) models, which are more robust but cannot generalize across different domains. In this work, we introduce *AutoToM*, an automated Bayesian Theory of Mind method for achieving open-ended machine Theory of Mind. *AutoToM* can operate in any domain, infer any mental variable, and conduct robust Theory of Mind reasoning of any order. Given a Theory of Mind inference problem, *AutoToM* first proposes an initial BToM model. It then conducts automated Bayesian inverse planning based on the proposed model, leveraging an LLM as the backend. Based on the uncertainty of the inference, it iteratively refines the model, by introducing additional mental variables and/or incorporating more timesteps in the context. Empirical evaluations across multiple Theory of Mind benchmarks demonstrate that *AutoToM* consistently achieves state-of-the-art performance, offering a scalable, robust, and interpretable approach to machine Theory of Mind."},"pdf":{"value":"/pdf/a15898e665b81cccec2e432ed87cd6185ebc6f98.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhang2025autotom,\ntitle={AutoToM: Automated Bayesian Inverse Planning and Model Discovery for Open-ended Theory of Mind},\nauthor={Zhining Zhang and Chuanyang Jin and Mung Yao Jia and Tianmin Shu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=EwdEdgCewj}\n}"},"paperhash":{"value":"zhang|autotom_automated_bayesian_inverse_planning_and_model_discovery_for_openended_theory_of_mind"}},"id":"EwdEdgCewj","forum":"EwdEdgCewj","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission86/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission86/Authors"],"number":86,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission86/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738691236284,"cdate":1738691236284,"tmdate":1742396129413,"mdate":1742396129413,"pdate":1741166408475,"odate":1741166408475,"version":2,"details":{"writable":false,"replyCount":1,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations"},"authors":{"value":["Kaixuan Huang","Jiacheng Guo","Zihao Li","Xiang Ji","Jiawei Ge","Wenzhe Li","Yingqing Guo","Tianle Cai","Hui Yuan","Runzhe Wang","Yue Wu","Ming Yin","Shange Tang","Yangsibo Huang","Chi Jin","Xinyun Chen","Chiyuan Zhang","Mengdi Wang"]},"authorids":{"value":["~Kaixuan_Huang1","~Jiacheng_Guo1","~Zihao_Li3","~Xiang_Ji3","~Jiawei_Ge3","~Wenzhe_Li2","~Yingqing_Guo1","~Tianle_Cai1","~Hui_Yuan2","~Runzhe_Wang2","~Yue_Wu12","~Ming_Yin4","~Shange_Tang1","~Yangsibo_Huang2","~Chi_Jin1","~Xinyun_Chen1","~Chiyuan_Zhang1","~Mengdi_Wang1"]},"keywords":{"value":["mathematical reasoning","benchmark","robustness"]},"TLDR":{"value":"We construct MATH-P-Simple and MATH-P-Hard to benchmark LLM's math reasoning against simple and hard perturbations, and examine memorization issues."},"abstract":{"value":"A genuinely robust reasoning model should be able to function correctly when the problem statement is modified out-of-training-distribution. Prior work has shown that language models struggle on mathematical benchmarks when questions undergo simple perturbations -- modifications that still preserve the underlying reasoning patterns of the solutions. However, no work has explored hard perturbations, which fundamentally change the nature of the problem so that the original solution steps do not apply. To bridge the gap, we construct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard perturbation, respectively. Each consists of 279 perturbed math problems derived from level-5 (hardest) problems in the MATH dataset (Hendrycks et. al., 2021). We observe significant performance drops on MATH-P-Hard across various models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking (-12.9%). We also raise concerns about a novel form of memorization where models blindly apply learned problem-solving skills without assessing their applicability to modified contexts. This issue is amplified when using original problems for in-context learning. We call for research efforts to address this challenge, which is critical for developing more robust and reliable reasoning models."},"pdf":{"value":"/pdf/c9d072994ed8dd1c35001d5485987b5ab036967c.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nhuang2025mathperturb,\ntitle={{MATH}-Perturb: Benchmarking {LLM}s' Math Reasoning Abilities against Hard Perturbations},\nauthor={Kaixuan Huang and Jiacheng Guo and Zihao Li and Xiang Ji and Jiawei Ge and Wenzhe Li and Yingqing Guo and Tianle Cai and Hui Yuan and Runzhe Wang and Yue Wu and Ming Yin and Shange Tang and Yangsibo Huang and Chi Jin and Xinyun Chen and Chiyuan Zhang and Mengdi Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=M8OLGgYK7e}\n}"},"paperhash":{"value":"huang|mathperturb_benchmarking_llms_math_reasoning_abilities_against_hard_perturbations"}},"id":"M8OLGgYK7e","forum":"M8OLGgYK7e","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission85/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission85/Authors"],"number":85,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission85/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738689121168,"cdate":1738689121168,"tmdate":1742430027640,"mdate":1742430027640,"pdate":1741166408410,"odate":1741166408410,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"LM2: Large Memory Models for Long Context Reasoning"},"authors":{"value":["Jikun Kang","Wenqi Wu","Filippos Christianos","Alex James Chan","Fraser David Greenlee","George Thomas","Marvin Purtorab","Andrew Toulis"]},"authorids":{"value":["~Jikun_Kang1","~Wenqi_Wu4","~Filippos_Christianos1","~Alex_James_Chan1","~Fraser_David_Greenlee1","~George_Thomas1","~Marvin_Purtorab1","~Andrew_Toulis1"]},"keywords":{"value":["LLM","Memory Model","Long Context","Reasoning"]},"abstract":{"value":"This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures."},"pdf":{"value":"/pdf/759189ac74693408611e89075570e98a49638287.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nkang2025lm,\ntitle={{LM}2: Large Memory Models for Long Context Reasoning},\nauthor={Jikun Kang and Wenqi Wu and Filippos Christianos and Alex James Chan and Fraser David Greenlee and George Thomas and Marvin Purtorab and Andrew Toulis},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=SWpv2bRkoj}\n}"},"paperhash":{"value":"kang|lm2_large_memory_models_for_long_context_reasoning"}},"id":"SWpv2bRkoj","forum":"SWpv2bRkoj","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission84/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission84/Authors"],"number":84,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission84/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738686494978,"cdate":1738686494978,"tmdate":1742324462876,"mdate":1742324462876,"pdate":1741166408341,"odate":1741166408341,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection"},"authors":{"value":["Yibo Yan","Shen Wang","Jiahao Huo","Hang Li","BOYAN LI","Jiamin Su","Xiong Gao","YiFan Zhang","Tianlong Xu","Zhendong Chu","Aoxiao Zhong","Kun Wang","Hui Xiong","Philip S. Yu","Xuming Hu","Qingsong Wen"]},"authorids":{"value":["~Yibo_Yan1","~Shen_Wang2","~Jiahao_Huo2","~Hang_Li10","~BOYAN_LI7","~Jiamin_Su2","~Xiong_Gao1","~YiFan_Zhang8","~Tianlong_Xu3","~Zhendong_Chu1","~Aoxiao_Zhong1","~Kun_Wang15","~Hui_Xiong1","~Philip_S._Yu1","~Xuming_Hu1","~Qingsong_Wen2"]},"keywords":{"value":["Multimodal Large Language Model","Complex Reasoning","Error Detection"]},"TLDR":{"value":"This paper introduces ErrorRadar, the first benchmark for assessing Multimodal Large Language Models (MLLMs) in multimodal error detection task within K-12 mathematical question sets with real-world student problem-solving data."},"abstract":{"value":"As the field of Multimodal Large Language Models (MLLMs) continues to evolve, their potential to handle mathematical reasoning tasks is promising, as they can handle multimodal questions via cross-modal understanding capabilities compared to text-only LLMs. Current mathematical benchmarks predominantly focus on evaluating MLLMs' problem-solving ability, yet there is a crucial gap in addressing more complex scenarios such as error detection, for enhancing reasoning capability in complicated settings. To fill this gap, we formally formulate the new task — **multimodal error detection**, and introduce **ErrorRadar**, the **first benchmark designed to assess MLLMs' capabilities in such a task**. ErrorRadar evaluates two sub-tasks: error step identification and error categorization, providing a framework for evaluating MLLMs' complex mathematical reasoning ability. It consists of 2,500 high-quality multimodal K-12 mathematical problems, collected from real-world student interactions in an educational organization, with expert-based annotation and metadata such as problem type and error category. Through extensive experiments, we evaluated both open-source and closed-source representative MLLMs, benchmarking their performance against educational expert evaluators. Results indicate challenges still remain, as GPT-4o with best model performance is still around 10% behind human evaluation."},"pdf":{"value":"/pdf/f807fd75d66631ddd591f308c2a0dc2af8759dfd.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nyan2025errorradar,\ntitle={ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection},\nauthor={Yibo Yan and Shen Wang and Jiahao Huo and Hang Li and BOYAN LI and Jiamin Su and Xiong Gao and YiFan Zhang and Tianlong Xu and Zhendong Chu and Aoxiao Zhong and Kun Wang and Hui Xiong and Philip S. Yu and Xuming Hu and Qingsong Wen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ydeV4pkNpR}\n}"},"paperhash":{"value":"yan|errorradar_benchmarking_complex_mathematical_reasoning_of_multimodal_large_language_models_via_error_detection"}},"id":"ydeV4pkNpR","forum":"ydeV4pkNpR","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission82/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission82/Authors"],"number":82,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission82/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738682014128,"cdate":1738682014128,"tmdate":1742139765320,"mdate":1742139765320,"pdate":1741166408240,"odate":1741166408239,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Plan$^\\ast$RAG: Efficient Test-Time Planning for Retrieval Augmented Generation"},"authors":{"value":["Prakhar Verma","Sukruta Prakash Midigeshi","Gaurav Sinha","Arno Solin","Nagarajan Natarajan","Amit Sharma"]},"authorids":{"value":["~Prakhar_Verma1","~Sukruta_Prakash_Midigeshi1","~Gaurav_Sinha2","~Arno_Solin1","~Nagarajan_Natarajan2","~Amit_Sharma3"]},"keywords":{"value":["Language Models","Retrieval Augmented Generation","LLM","RAG","Reasoning"]},"TLDR":{"value":"Plan$^\\ast$RAG generates test-time reasoning plans as DAGs, enabling parallel retrieval and systematic verification for improved accuracy in RAG systems"},"abstract":{"value":"We introduce Plan$^\\ast$RAG, a novel framework that enables structured multi-hop reasoning in retrieval-augmented generation (RAG) through test-time reasoning plan generation. While existing approaches such as ReAct maintain reasoning chains within the language model's context window, we observe that this often leads to plan fragmentation and execution failures. Our key insight is that by isolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's working memory, we can enable *(1)* systematic *exploration* of reasoning paths, *(2)* *atomic* subqueries enabling precise retrievals and grounding, and *(3)* *efficiency* through parallel execution and bounded context window utilization. Moreover, Plan$^\\ast$RAG's modular design allows it to be integrated with existing RAG methods, thus providing a practical solution to improve current RAG systems. On standard multi-hop reasoning benchmarks, Plan$^\\ast$RAG consistently achieves improvements over recently proposed methods such as RQ-RAG and Self-RAG, while maintaining comparable computational costs."},"pdf":{"value":"/pdf/1078a62f376baa611e5337bcad4670e98cbe6beb.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nverma2025planastrag,\ntitle={Plan\\${\\textasciicircum}{\\textbackslash}ast\\${RAG}: Efficient Test-Time Planning for Retrieval Augmented Generation},\nauthor={Prakhar Verma and Sukruta Prakash Midigeshi and Gaurav Sinha and Arno Solin and Nagarajan Natarajan and Amit Sharma},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=gi9aqlYdBk}\n}"},"paperhash":{"value":"verma|plan^\\astrag_efficient_testtime_planning_for_retrieval_augmented_generation"}},"id":"gi9aqlYdBk","forum":"gi9aqlYdBk","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission78/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission78/Authors"],"number":78,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission78/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738668909610,"cdate":1738668909610,"tmdate":1742286877973,"mdate":1742286877973,"pdate":1741166408048,"odate":1741166408048,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Think to Ground: Improving Spatial Reasoning in LLMs for better Visual Grounding"},"authors":{"value":["Karun Sharma","Vidushee Vats"]},"authorids":{"value":["~Karun_Sharma1","~Vidushee_Vats1"]},"keywords":{"value":["Visual Grounding","Thinking and Reasoning","LLM","RL"]},"TLDR":{"value":"Improving Spatial Reasoning in open-source LLMs for better Visual Grounding and making them comparable to closed-source LLMs like GPT4o using Thinking and Reasoning at inference."},"abstract":{"value":"Visual grounding tasks involve identifying objects and references in an image based on text input. A model is required to locate the objects and their relationships, as well as to understand the image to accurately ground the target. Specialized models like Owl-ViT and Grounding DINO often fail to predict correct results for queries involving complex spatial information. In this paper, we propose a Spatial Thinking and Reasoning Dataset for visual grounding and a framework that uses existing detection models to identify candidate objects. These models provide coordinates and other attributes to a large language model (LLM) for spatial reasoning to determine the correct target. Recent closed-source models like GPT-4o achieve approximately 86% accuracy, while open-source models perform significantly worse, reaching only about 60% accuracy in our experiments. To improve this, we use reinforcement learning to fine-tune a 3B open-source model on our dataset, achieving 77% accuracy, comparable to closed-source models."},"pdf":{"value":"/pdf/6c7eebedebb4814de12d70f44083a67d33f46613.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsharma2025think,\ntitle={Think to Ground: Improving Spatial Reasoning in {LLM}s for better Visual Grounding},\nauthor={Karun Sharma and Vidushee Vats},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=T2IHuIib74}\n}"},"paperhash":{"value":"sharma|think_to_ground_improving_spatial_reasoning_in_llms_for_better_visual_grounding"}},"id":"T2IHuIib74","forum":"T2IHuIib74","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission77/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission77/Authors"],"number":77,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission77/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738660637145,"cdate":1738660637145,"tmdate":1742470744310,"mdate":1742470744310,"pdate":1741166407951,"odate":1741166407951,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with Effortless Adaptation"},"authors":{"value":["Xuan Zhang","Fengzhuo Zhang","Cunxiao Du","Chao Du","Tianyu Pang","Wei Gao","Min Lin"]},"authorids":{"value":["~Xuan_Zhang22","~Fengzhuo_Zhang1","~Cunxiao_Du3","~Chao_Du1","~Tianyu_Pang1","~Wei_Gao1","~Min_Lin1"]},"keywords":{"value":["Hybrid Model","Long-Context Understanding","o1-like Long Reasoning Generation"]},"TLDR":{"value":"We propose a lightweight method that transfers vanilla transformer into a hybrid architecture."},"abstract":{"value":"Scaling language models to handle longer contexts introduces substantial memory challenges due to the growing cost of key-value (KV) caches. Motivated by the efficiency gains of hybrid models and the broad availability of pretrained large transformer backbones, we explore transitioning transformer models into hybrid architectures for a more efficient generation. In this work, we propose LightTransfer, a lightweight method that transforms models such as LLaMA into hybrid variants. Our approach identifies lazy layers—those focusing on recent or initial tokens—and replaces their full attention with streaming attention. This transformation can be performed without any training for long-context understanding tasks or with minimal fine-tuning for o1-like long reasoning generation tasks that require stronger reasoning capabilities. Experiments across diverse benchmarks and models (e.g., LLaMA, Mistral, QwQ-STILL) demonstrate that, even when half of the layers are identified as lazy, LightTransfer achieves up to 2.17\n throughput improvement with minimal performance loss (\n on LongBench) and achieves 53.3% on math benchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."},"pdf":{"value":"/pdf/9fafec3077774ee1b5a9586f106166b81e7a6557.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhang2025lighttransfer,\ntitle={LightTransfer: Your Long-Context {LLM} is Secretly a Hybrid Model with Effortless Adaptation},\nauthor={Xuan Zhang and Fengzhuo Zhang and Cunxiao Du and Chao Du and Tianyu Pang and Wei Gao and Min Lin},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=DfgfGTfObm}\n}"},"paperhash":{"value":"zhang|lighttransfer_your_longcontext_llm_is_secretly_a_hybrid_model_with_effortless_adaptation"}},"id":"DfgfGTfObm","forum":"DfgfGTfObm","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission76/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission76/Authors"],"number":76,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission76/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738659680813,"cdate":1738659680813,"tmdate":1742460505773,"mdate":1742460505773,"pdate":1741166407894,"odate":1741166407894,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"DEDUCE: DEDUCTIVE CONSISTENCY AS A FRAMEWORK TO EVALUATE LLM REASONING"},"authors":{"value":["Atharva Pandey","Kshitij Dubey","Rahul Sharma","Amit Sharma"]},"authorids":{"value":["~Atharva_Pandey1","~Kshitij_Dubey1","~Rahul_Sharma5","~Amit_Sharma3"]},"keywords":{"value":["Reasoning","Synthetic Dataset","Deductive","Large Language Models","Evaluation","Benchmarks"]},"abstract":{"value":"Despite great performance on Olympiad-level reasoning problems, frontier large language models can still struggle on high school math.\nWe study the nature of language models' (LM) reasoning by analyzing their chain-of-thought traces. \nTo avoid memorization issues, we present a framework that can evaluate reasoning of LMs over novel, perturbed versions of benchmark problems. Formally, we compare LMs to ideal deductive reasoners that given a set of premises, can provide valid conclusions over any number of reasoning hops. To assess reasoning performance beyond final accuracy, we introduce *deductive consistency*, a metric that evaluates the correctness of system’s reasoning across varying input premise lengths and the number of solution hops. Using this metric, we examine potential explanations for language models’ failures on novel problems. Through experiments on GSM-8K and a synthetic dataset, we find that the failure is not primarily due to shifts in language style or the propagation of early errors. Instead, it stems from a fundamental limitation: as the number of reasoning hops increases, language models exhibit a decline in deductive consistency, \n which was  masked by memorization for existing benchmark problems. Our analysis provides a new view to characterize LM reasoning---as computations over a window of input premises and reasoning hops---that can provide unified evaluation across problem domains."},"pdf":{"value":"/pdf/e0e89a704f8a65b9bc1438f073d3facb816ba8f3.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\npandey2025deduce,\ntitle={{DEDUCE}: {DEDUCTIVE} {CONSISTENCY} {AS} A {FRAMEWORK} {TO} {EVALUATE} {LLM} {REASONING}},\nauthor={Atharva Pandey and Kshitij Dubey and Rahul Sharma and Amit Sharma},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=9n0LxTavt9}\n}"},"paperhash":{"value":"pandey|deduce_deductive_consistency_as_a_framework_to_evaluate_llm_reasoning"}},"id":"9n0LxTavt9","forum":"9n0LxTavt9","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission75/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission75/Authors"],"number":75,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission75/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738654282280,"cdate":1738654282280,"tmdate":1742471291309,"mdate":1742471291309,"pdate":1741166407892,"odate":1741166407892,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Automating Evaluation of Creativity in LLMs with Semantic Entropy and Efficient Multi-Agent Judge"},"authors":{"value":["Tan Min Sen","Zachary Choy Kit Chun","Swaagat Bikash Saikia","Syed Ali Redha Alsagoff","Banerjee Mohor","Nadya Yuki Wangsajaya","Alvin Chan"]},"authorids":{"value":["~Tan_Min_Sen1","~Zachary_Choy_Kit_Chun1","~Swaagat_Bikash_Saikia1","~Syed_Ali_Redha_Alsagoff1","~Banerjee_Mohor1","~Nadya_Yuki_Wangsajaya1","~Alvin_Chan1"]},"keywords":{"value":["LLMs","Reasoning","Creativity","Benchmark","LLM-as-a-judge"]},"TLDR":{"value":"This paper automates creativity evaluation in LLMs using semantic entropy for novelty and a new retrieval-based framework for efficient multi-agent judging of solution quality."},"abstract":{"value":"Large Language Models (LLMs) have achieved remarkable progress in natural language comprehension, reasoning, and generation, sparking interest in their creative potential. Automating creativity evaluation in LLMs, particularly in physical reasoning tasks, presents a transformative opportunity to accelerate scientific discovery by enabling innovative solutions, uncovering patterns, and automating problem-solving processes. Current creativity evaluation frameworks, however, rely heavily on human annotation, making them subjective, resource-intensive, and impractical for scaling. To address this, we introduce a novel automated evaluation framework rooted in cognitive science principles of divergent and convergent thinking. Divergent creativity is measured using Semantic Entropy, a sampling-based metric that quantifies variability in generated outputs to capture the novelty of ideas. Convergent creativity is assessed using a modified retrieval-based discussion framework—60% more efficient—where autonomous multi-agent systems evaluate task solutions across feasibility, safety, and effectiveness. We implement these methodologies within a benchmark based on the MacGyver dataset, which contains 300 real-world, solvable problems requiring innovative use of everyday objects. Our framework evaluates state-of-the-art LLMs, such as GPT and LLaMA models, while analyzing the effects of key parameters like temperature, model size, and recency. By automating creativity evaluation, we establish a scalable, objective, and reproducible methodology to enhance LLM development, paving the way for breakthroughs in scientific discovery and creative problem-solving across diverse fields."},"pdf":{"value":"/pdf/05a1b6b90aa6a0eca6304a8be8e68027e0b2fd5d.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsen2025think,\ntitle={Think Outside the Bot: Automating Evaluation of Creativity in {LLM}s for Physical Reasoning with Semantic Entropy and Efficient Multi-Agent Judge},\nauthor={Tan Min Sen and Zachary Choy Kit Chun and Swaagat Bikash Saikia and Syed Ali Redha Alsagoff and Banerjee Mohor and Nadya Yuki Wangsajaya and Alvin Chan},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=hocpzqMqB5}\n}"},"paperhash":{"value":"sen|automating_evaluation_of_creativity_in_llms_with_semantic_entropy_and_efficient_multiagent_judge"}},"id":"hocpzqMqB5","forum":"hocpzqMqB5","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission74/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission74/Authors"],"number":74,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission74/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738637929384,"cdate":1738637929384,"tmdate":1742441007137,"mdate":1742441007137,"pdate":1741166407828,"odate":1741166407828,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms"},"authors":{"value":["Xuerui Su","Yue Wang","Jinhua Zhu","Mingyang Yi","Feng Xu","Zhi-Ming Ma","Yuting Liu"]},"authorids":{"value":["~Xuerui_Su1","~Yue_Wang15","~Jinhua_Zhu1","~Mingyang_Yi1","~Feng_Xu11","~Zhi-Ming_Ma1","~Yuting_Liu4"]},"keywords":{"value":["LLMs","RLHF","DPO","Soft Policy Iteration","Boltzmann Distribution","Convergence Rate","Offline Dataset Design"]},"TLDR":{"value":"This paper explores the relationship between DPO, RL and other RLHF algorithms, as well as its connections to typical RL methods."},"abstract":{"value":"With the rapid development of Large Language Models (LLMs), numerous Reinforcement Learning from Human Feedback (RLHF) algorithms have been introduced to improve model safety, and alignment with human preferences. These algorithms can be divided into two main frameworks based on whether they require an explicit reward (or value) function for training: actor-critic-based Proximal Policy Optimization (PPO) and alignment-based Direct Preference Optimization (DPO). The mismatch between DPO and PPO, such as DPO’s use of a classification loss driven by human-preferred data, has raised confusion about whether DPO should be classified as a Reinforcement Learning (RL) algorithm. To address these ambiguities, we focus on three key aspects related to DPO, RL, and other RLHF algorithms: (1) the construction of the loss function; (2) the target distribution at which the algorithm converges; (3) the impact of key components within the loss function. Specifically, we first establish a unified framework named UDRRA connecting these algorithms based on the construction of their loss functions. Next, we uncover their target policy distributions within this framework. Finally, we investigate the critical components of DPO to understand their impact on the convergence rate. Our work provides a deeper understanding of the relationship between DPO, RL, and other RLHF algorithms, offering new insights for improving existing algorithms."},"pdf":{"value":"/pdf/c18cd90e5b0cf4ba2cc2599b6c28c2e1c62fb93e.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsu2025reveal,\ntitle={Reveal the Mystery of {DPO}: The Connection between {DPO} and {RL} Algorithms},\nauthor={Xuerui Su and Yue Wang and Jinhua Zhu and Mingyang Yi and Feng Xu and Zhi-Ming Ma and Yuting Liu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=rkfdSNjO3U}\n}"},"paperhash":{"value":"su|reveal_the_mystery_of_dpo_the_connection_between_dpo_and_rl_algorithms"}},"id":"rkfdSNjO3U","forum":"rkfdSNjO3U","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission73/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission73/Authors"],"number":73,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission73/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738632351393,"cdate":1738632351393,"tmdate":1742390481837,"mdate":1742390481837,"pdate":1741166407739,"odate":1741166407739,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning"},"authors":{"value":["Wanjia Zhao","Mert Yuksekgonul","Shirley Wu","James Zou"]},"authorids":{"value":["~Wanjia_Zhao1","~Mert_Yuksekgonul1","~Shirley_Wu1","~James_Zou1"]},"keywords":{"value":["Multi-Agent","Self-Improving","LLM Reasoning"]},"TLDR":{"value":"We introduce SiriuS, a self-improving framework for multi-agent systems that builds and refines an experience library, enhancing the system's ability to tackle complex reasoning problems with minimal supervision."},"abstract":{"value":"Multi-agent AI systems powered by large language models (LLMs) are increasingly applied to solve complex tasks. However, these systems often rely on fragile, manually designed prompts and heuristics, making optimization difficult. A key challenge in optimizing multi-agent systems is acquiring suitable training data for specialized agents. We introduce SiriuS, a self-improving, reasoning-driven optimization framework for multi-agent systems. Central to our approach is the construction of an experience library: a repository of high-quality reasoning trajectories. The library is built by retaining reasoning steps that lead to successful outcomes, providing a robust training set for optimizing multi-agent system. Additionally, we introduce a library augmentation procedure that refines unsuccessful trajectories, further enriching the library. SiriuS boosts performance by 2.86% to 21.88% on reasoning and biomedical QA and enhances agent negotiation in competitive settings. Our results show that SiriuS enhances multi-agent performance while generating reusable data for self-correction and self-play enhancement in the future."},"pdf":{"value":"/pdf/a8f89c5c3c8bbb9ec84440b91eef8f4562684e66.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhao2025sirius,\ntitle={SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning},\nauthor={Wanjia Zhao and Mert Yuksekgonul and Shirley Wu and James Zou},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=sLBSJr3hH5}\n}"},"paperhash":{"value":"zhao|sirius_selfimproving_multiagent_systems_via_bootstrapped_reasoning"}},"id":"sLBSJr3hH5","forum":"sLBSJr3hH5","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission72/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission72/Authors"],"number":72,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission72/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738623077614,"cdate":1738623077614,"tmdate":1741905794763,"mdate":1741905794763,"pdate":1741166407698,"odate":1741166407698,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs"},"authors":{"value":["Mauricio Soroco","Jialin Song","Mengzhou Xia","Kye Emond","Weiran Sun","Wuyang Chen"]},"authorids":{"value":["~Mauricio_Soroco1","~Jialin_Song3","~Mengzhou_Xia1","~Kye_Emond1","~Weiran_Sun1","~Wuyang_Chen1"]},"keywords":{"value":["AI-for-Math","Large Language Model","Partial Differential Equation"]},"TLDR":{"value":"We build an LLM termed \"PDE-Controller\" that can achieve reasoning and planning on PDE (partial differential equation) control problems."},"abstract":{"value":"While recent AI-for-math has made strides in pure mathematics, areas of applied mathematics, particularly PDEs, remain underexplored despite their significant real-world applications. We present PDE-Controller, a framework that enables large language models (LLMs) to control systems governed by partial differential equations (PDEs). Our approach enables LLMs to transform informal natural language instructions into formal specifications, and then execute reasoning and planning steps to improve the utility of PDE control. We build a holistic solution comprising datasets (both human-written cases and 2 million synthetic samples), math-reasoning models, and novel evaluation metrics, all of which require significant effort. Our PDE-Controller significantly outperforms the latest open-source and GPT models in reasoning, autoformalization, and program synthesis, achieving up to a 62% improvement in utility gain for PDE control. By bridging the gap between language generation and PDE systems, we demonstrate the potential of LLMs in addressing complex scientific and engineering challenges. We promise to release all data, model checkpoints, and code upon acceptance."},"pdf":{"value":"/pdf/c03cd526af99bc19da51fc077a356efa59979c2e.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nsoroco2025pdecontroller,\ntitle={{PDE}-Controller: {LLM}s for Autoformalization and Reasoning of {PDE}s},\nauthor={Mauricio Soroco and Jialin Song and Mengzhou Xia and Kye Emond and Weiran Sun and Wuyang Chen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=e6XPWeOxAy}\n}"},"paperhash":{"value":"soroco|pdecontroller_llms_for_autoformalization_and_reasoning_of_pdes"}},"id":"e6XPWeOxAy","forum":"e6XPWeOxAy","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission69/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission69/Authors"],"number":69,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission69/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738571980306,"cdate":1738571980306,"tmdate":1742383617444,"mdate":1742383617444,"pdate":1741166407520,"odate":1741166407520,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"On the Language of Thoughts in Large Language Models"},"authors":{"value":["Chenxi Liu","Yongqiang Chen","Tongliang Liu","James Cheng","Bo Han","Kun Zhang"]},"authorids":{"value":["~Chenxi_Liu3","~Yongqiang_Chen1","~Tongliang_Liu1","~James_Cheng2","~Bo_Han1","~Kun_Zhang1"]},"keywords":{"value":["Language models","system 2 reasoning"]},"abstract":{"value":"System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process as a causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-scale natural languages. However, in this work, we show that there is a significant gap between the modeling of languages and thoughts. As language is primarily a tool for humans to share knowledge and thinking, modeling human language can easily absorb language biases into LLMs deviated from the chain of thoughts in minds. Furthermore, we show that the biases will mislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of the premise. To this end, we propose a new prompt technique termed Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of directly eliciting the chain of thoughts from partial information, LoT instructs LLMs to adjust the order and token used for the expressions of all the relevant information. We show that the simple strategy significantly reduces the language modeling biases in LLMs and improves the performance of LLMs across a variety of reasoning tasks."},"pdf":{"value":"/pdf/a28e6fe5a52d5e557129bb4c58f67a139a1aaf09.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nliu2025on,\ntitle={On the Language of Thoughts in Large Language Models},\nauthor={Chenxi Liu and Yongqiang Chen and Tongliang Liu and James Cheng and Bo Han and Kun Zhang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=AQpiyVis6R}\n}"},"paperhash":{"value":"liu|on_the_language_of_thoughts_in_large_language_models"}},"id":"AQpiyVis6R","forum":"AQpiyVis6R","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission68/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission68/Authors"],"number":68,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission68/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738569991020,"cdate":1738569991020,"tmdate":1742390843480,"mdate":1742390843480,"pdate":1741166407473,"odate":1741166407473,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Decoupling the components of geometric understanding"},"authors":{"value":["Eliza Kosoy","Annya Dahmani","Andrew Kyle Lampinen","Iulia Maria Comsa","Soojin Jeong","Ishita Dasgupta","Kelsey R Allen"]},"authorids":{"value":["~Eliza_Kosoy1","~Annya_Dahmani1","~Andrew_Kyle_Lampinen1","~Iulia_Maria_Comsa1","~Soojin_Jeong2","~Ishita_Dasgupta1","~Kelsey_R_Allen1"]},"keywords":{"value":["geometry","cognitive science","vlms"]},"TLDR":{"value":"A new methodology for decoupling the components of geometric understanding in foundation models"},"abstract":{"value":"Understanding geometry relies heavily on vision. In this work, we evaluate whether state-of-the-art vision language models (VLMs) can understand simple geometric concepts.We use a paradigm from cognitive science that isolates visual understanding of simple geometry from the many other capabilities it is often conflated with (reasoning, world knowledge, etc.). We compare model performance with human adults from the USA, as well as with prior research on human adults without formal education from an Amazonian indigenous group. We find that VLMs consistently underperform both groups of human adults, although they succeed with some concepts more than others. We also find that VLM geometric understanding is more brittle than human understanding, and is not robust to e.g. mental rotation. This work highlights interesting differences in the origin of geometric understanding in humans and machines -- e.g. from printed materials used in formal education vs. interactions with the physical world or a combination of the two -- and the first steps toward understanding these differences."},"pdf":{"value":"/pdf/68883e690afde51b40b5473cf3cde0b882625635.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nkosoy2025decoupling,\ntitle={Decoupling the components of geometric understanding},\nauthor={Eliza Kosoy and Annya Dahmani and Andrew Kyle Lampinen and Iulia Maria Comsa and Soojin Jeong and Ishita Dasgupta and Kelsey R Allen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=VSLCgwK5Az}\n}"},"paperhash":{"value":"kosoy|decoupling_the_components_of_geometric_understanding"}},"id":"VSLCgwK5Az","forum":"VSLCgwK5Az","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission67/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission67/Authors"],"number":67,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission67/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738560934591,"cdate":1738560934591,"tmdate":1742421823579,"mdate":1742421823579,"pdate":1741166407465,"odate":1741166407465,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Diving into Self-Evolve Training for Multimodal Reasoning"},"authors":{"value":["Wei Liu","Junlong Li","Xiwen Zhang","Fan Zhou","Yu Cheng","Junxian He"]},"authorids":{"value":["~Wei_Liu25","~Junlong_Li1","~Xiwen_Zhang2","~Fan_Zhou6","~Yu_Cheng1","~Junxian_He1"]},"keywords":{"value":["Multimodal Reasoning","Self-Evolving Training","Large Multimodal Models"]},"abstract":{"value":"Self-evolving training—where models iteratively learn from their own outputs—has emerged as a key approach for complex reasoning tasks, addressing the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a domain more intricate than text-only reasoning, remains underexplored, and the understanding of critical factors in this training paradigm remains limited. Furthermore, a central challenge for this training method is performance saturation, which impedes further improvements and scalability. Inspired by reinforcement learning (RL), in this paper, we reframe self-evolving training for multimodal reasoning through the lens of RL, identifying three pivotal factors: $\\textit{Training Method}$, $\\textit{Reward Model}$, and $\\textit{Prompt Variation}$. Through systematic analysis, we establish relatively optimal design principles that significantly enhance multimodal reasoning capabilities. Moreover, delving deeper into training dynamics, we uncover the roots of saturation and propose a new automatic balancing mechanism to mitigate this limitation. Building on these insights, we propose M-STaR (**M**ultimodal **S**elf-evolving **T**r**a**ining for **R**easoning), a framework that achieves consistent performance gains across models of varying sizes and diverse benchmarks. All resources will be made publicly available."},"pdf":{"value":"/pdf/0e8aa65d44a9b1f1aab77289e99a814e2514c577.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nliu2025diving,\ntitle={Diving into Self-Evolve Training for Multimodal Reasoning},\nauthor={Wei Liu and Junlong Li and Xiwen Zhang and Fan Zhou and Yu Cheng and Junxian He},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=Ad8SnHPCHH}\n}"},"paperhash":{"value":"liu|diving_into_selfevolve_training_for_multimodal_reasoning"}},"id":"Ad8SnHPCHH","forum":"Ad8SnHPCHH","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission65/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission65/Authors"],"number":65,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission65/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738551972725,"cdate":1738551972725,"tmdate":1742397472059,"mdate":1742397472059,"pdate":1741166407242,"odate":1741166407242,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Can Large Language Models Reason? A Characterization via 3-SAT"},"authors":{"value":["RISHI HAZRA","Gabriele Venturato","Pedro Zuidberg Dos Martires","Luc De Raedt"]},"authorids":{"value":["~RISHI_HAZRA1","~Gabriele_Venturato1","~Pedro_Zuidberg_Dos_Martires1","~Luc_De_Raedt1"]},"keywords":{"value":["Large Language Models","Logic","Reasoning","Satisfiability","Phase Transitions"]},"TLDR":{"value":"We use phase transitions in random 3-SAT to characterize reasoning abilities of LLMs."},"abstract":{"value":"Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. However, recent works have shown that LLMs often bypass true reasoning using shortcuts, sparking skepticism. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT -- the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of LLMs by varying the inherent hardness of the problem instances. Our experimental evidence shows that LLMs are incapable of performing true reasoning, as required for solving 3-SAT problems. Moreover, we observe significant performance variation based on the inherent hardness of the problems -- performing poorly on harder instances and vice versa. Importantly, we show that integrating external reasoners can considerably enhance LLM performance. By following a principled experimental protocol, our study draws concrete conclusions and moves beyond the anecdotal evidence often found in LLM reasoning research."},"pdf":{"value":"/pdf/c12231dca3a39edfbcf6d7e92eead47d40e9be95.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nhazra2025can,\ntitle={Can Large Language Models Reason? A Characterization via 3-{SAT}},\nauthor={RISHI HAZRA and Gabriele Venturato and Pedro Zuidberg Dos Martires and Luc De Raedt},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=RqvtYEsyo8}\n}"},"paperhash":{"value":"hazra|can_large_language_models_reason_a_characterization_via_3sat"}},"id":"RqvtYEsyo8","forum":"RqvtYEsyo8","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission60/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission60/Authors"],"number":60,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission60/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738521862962,"cdate":1738521862962,"tmdate":1742120474471,"mdate":1742120474471,"pdate":1741166406975,"odate":1741166406975,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Agentic Knowledgeable Self-awareness"},"authors":{"value":["Shuofei Qiao","Zhisong Qiu","Baochang Ren","Xiaobin Wang","Xiangyuan Ru","Ningyu Zhang","Xiang Chen","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"]},"authorids":{"value":["~Shuofei_Qiao1","~Zhisong_Qiu1","~Baochang_Ren1","~Xiaobin_Wang1","~Xiangyuan_Ru1","~Ningyu_Zhang1","~Xiang_Chen5","~Yong_Jiang1","~Pengjun_Xie2","~Fei_Huang2","~Huajun_Chen1"]},"keywords":{"value":["language model agents","knowledgeable self-awareness","agent planning"]},"TLDR":{"value":"We introduce KnowSelf, a data-centric approach that enables agents to have knowledgeable self-awareness similar to humans, selectively self-correcting and querying knowl- edge based on certain situations during the planning process."},"abstract":{"value":"Large language agents have achieved considerable performance across various agentic planning tasks. However, most current agent learning methods are spoon-feeding, with gold trajectories, external feedback and knowledge mindlessly feeding into agent models regardless of their actual needs, resulting in a lack of self-consciousness during the planning process. In this paper, we introduce KnowSelf, a data-centric approach that enables agents to have knowledgeable self-awareness similar to humans, selectively self-correcting and querying knowledge based on certain situations during the planning process. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent’s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. We also present further analysis to examine the effectiveness of agentic knowledgeable self-awareness from different aspects."},"pdf":{"value":"/pdf/f76fbc26be53b97f18b89a3843d0b6a357ee9c8f.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nqiao2025agentic,\ntitle={Agentic Knowledgeable Self-awareness},\nauthor={Shuofei Qiao and Zhisong Qiu and Baochang Ren and Xiaobin Wang and Xiangyuan Ru and Ningyu Zhang and Xiang Chen and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=PGdSLjYwMT}\n}"},"paperhash":{"value":"qiao|agentic_knowledgeable_selfawareness"}},"id":"PGdSLjYwMT","forum":"PGdSLjYwMT","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission57/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission57/Authors"],"number":57,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission57/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738505605401,"cdate":1738505605401,"tmdate":1742390718820,"mdate":1742390718820,"pdate":1741166406811,"odate":1741166406811,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Scaling Flaws of Verifier-guided Search in Mathematical Reasoning"},"authors":{"value":["Fei Yu","Yingru Li","Benyou Wang"]},"authorids":{"value":["~Fei_Yu3","~Yingru_Li1","~Benyou_Wang2"]},"keywords":{"value":["inference-time compute scaling","verifiers","ovm","prm"]},"TLDR":{"value":"scaling flaws hinder the effectiveness of verifier-guided search approaches"},"abstract":{"value":"Large language models (LLMs) struggle with multi-step reasoning, where inference-time scaling has emerged as a promising strategy for performance improvement. Verifier-guided search outperforms repeated sampling when sample size is limited by selecting and prioritizing valid reasoning paths. However, we identify a critical limitation: scaling flaws, prevalent across different models (Mistral 7B and DeepSeekMath 7B), benchmarks (GSM8K and MATH), and verifiers (outcome value models and process reward models). As sample size increases, verifier-guided search exhibits diminishing advantages and eventually underperforms repeated sampling. Our analysis attributes this to verifier failures, where imperfect verifiers misrank candidates and erroneously prune all valid paths. These issues are further exacerbated in challenging and out-of-distribution problems, restricting search effectiveness. To mitigate verifier failures, we explore reducing reliance on verifiers and conduct preliminary investigations using two simple methods. Our findings reveal fundamental limitations in verifier-guided search and suggest future directions."},"pdf":{"value":"/pdf/ca35dc3d3d36087ac0465329037b3562372a6303.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nyu2025scaling,\ntitle={Scaling Flaws of Verifier-guided Search in Mathematical Reasoning},\nauthor={Fei Yu and Yingru Li and Benyou Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=9vFB91wspX}\n}"},"paperhash":{"value":"yu|scaling_flaws_of_verifierguided_search_in_mathematical_reasoning"}},"id":"9vFB91wspX","forum":"9vFB91wspX","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission55/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission55/Authors"],"number":55,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission55/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738486314584,"cdate":1738486314584,"tmdate":1742441975353,"mdate":1742441975353,"pdate":1741166406731,"odate":1741166406731,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels"},"authors":{"value":["Zhe Xu","Jiasheng Ye","Xiaoran Liu","Xiangyang Liu","Tianxiang Sun","Zhigeng Liu","Qipeng Guo","Linlin Li","Qun Liu","Xuanjing Huang","Xipeng Qiu"]},"authorids":{"value":["~Zhe_Xu10","~Jiasheng_Ye1","~Xiaoran_Liu1","~Xiangyang_Liu3","~Tianxiang_Sun1","~Zhigeng_Liu1","~Qipeng_Guo1","~Linlin_Li3","~Qun_Liu1","~Xuanjing_Huang1","~Xipeng_Qiu1"]},"keywords":{"value":["benchmarking;NLP datasets;evaluation methodologies; evaluation; metrics;"]},"TLDR":{"value":"We propose a benchmark for long text narrative reasoning called DetectiveQA, and present a variety of evaluation metric for assessing the ability of large language models, and find that existing models still have to be improved in this regard."},"abstract":{"value":"Recently, significant efforts have been devoted to enhancing the long-context capabilities of Large Language Models (LLMs), particularly in long-context reasoning. To facilitate this research, we propose **DetectiveQA**, a dataset specifically designed for narrative reasoning within long contexts. We leverage detective novels, averaging over 100K tokens, to create a dataset containing 1200 human-annotated questions in both Chinese and English, each paired with reference reasoning steps. Furthermore, we introduce a step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning processes. We validate our approach and evaluate the mainstream LLMs, including GPT-4, Claude, and LLaMA, revealing persistent long-context reasoning challenges and demonstrating their evidence-retrieval challenges and data contamination issues. Our findings offer valuable insights into the study of long-context reasoning and lay the base for more rigorous evaluations."},"pdf":{"value":"/pdf/92d8256bde00f4afe5945fec8f8b5a476263a183.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nxu2025detectiveqa,\ntitle={Detective{QA}: Evaluating Long-Context Reasoning on Detective Novels},\nauthor={Zhe Xu and Jiasheng Ye and Xiaoran Liu and Xiangyang Liu and Tianxiang Sun and Zhigeng Liu and Qipeng Guo and Linlin Li and Qun Liu and Xuanjing Huang and Xipeng Qiu},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=9ExIs5ELlk}\n}"},"paperhash":{"value":"xu|detectiveqa_evaluating_longcontext_reasoning_on_detective_novels"}},"id":"9ExIs5ELlk","forum":"9ExIs5ELlk","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission53/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission53/Authors"],"number":53,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission53/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738464450868,"cdate":1738464450868,"tmdate":1742195737808,"mdate":1742195737808,"pdate":1741166406601,"odate":1741166406601,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Disentangling Exploration of Large Language Models by Optimal Exploitation"},"authors":{"value":["Tim Grams","Patrick Betz","Christian Bartelt"]},"authorids":{"value":["~Tim_Grams1","~Patrick_Betz1","~Christian_Bartelt1"]},"keywords":{"value":["Large Language Model","Exploration","Sequential Decision Making"]},"TLDR":{"value":"Evaluating large language models based on an optimal exploitation measures exploration progress accurately and can provide insights for prompt-engineering."},"abstract":{"value":"Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains unclear if large language models can effectively explore the state-space within an unknown environment. This work isolates exploration as the sole objective, tasking the agent with delivering information that enhances future returns. Within this framework, we argue that measuring agent returns is not sufficient for a fair evaluation and decompose missing rewards into exploration and exploitation components based on the optimal achievable return. Comprehensive experiments with various models reveal that most struggle to sufficiently explore the state-space and weak exploration is insufficient. We found a positive correlation of exploration performance with language comprehension and reasoning capabilities. Furthermore, we show that our decomposition can provide insights into differences in behaviors driven by prompt engineering, offering a valuable tool for refining performance in exploratory tasks."},"pdf":{"value":"/pdf/853f5f9a2acf5e225a23c81fe132c587432d87b7.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\ngrams2025disentangling,\ntitle={Disentangling Exploration of Large Language Models by Optimal Exploitation},\nauthor={Tim Grams and Patrick Betz and Christian Bartelt},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=IHxbEEDSkL}\n}"},"paperhash":{"value":"grams|disentangling_exploration_of_large_language_models_by_optimal_exploitation"}},"id":"IHxbEEDSkL","forum":"IHxbEEDSkL","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission51/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission51/Authors"],"number":51,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission51/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738449295810,"cdate":1738449295810,"tmdate":1742031404352,"mdate":1742031404352,"pdate":1741166406530,"odate":1741166406530,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Optimizing Test-Time Compute via Meta Reinforcement Finetuning"},"authors":{"value":["Yuxiao Qu","Matthew Y. R. Yang","Amrith Setlur","Lewis Tunstall","Edward Emanuel Beeching","Ruslan Salakhutdinov","Aviral Kumar"]},"authorids":{"value":["~Yuxiao_Qu1","~Matthew_Y._R._Yang1","~Amrith_Setlur1","~Lewis_Tunstall1","~Edward_Emanuel_Beeching2","~Ruslan_Salakhutdinov1","~Aviral_Kumar2"]},"keywords":{"value":["meta-RL","test-time compute","information gain","self-correction","LLM optimization","cumulative regret","backtracking"]},"TLDR":{"value":"The paper proposes MRT, a meta-RL method that optimizes LLM test-time compute using rewards and information gain."},"abstract":{"value":"Training models to efficiently use test-time compute is crucial for improving the reasoning performance of LLMs. While current methods mostly do so via fine-tuning on search traces or running RL against the 0/1 outcome reward, do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute from the lens of exploration and exploitation. It also motivates the use of cumulative regret to measure the efficacy of test-time compute by viewing a long output stream as consisting of several episodes from the model. While current state-of-the-art models do not optimize regret, we show that regret can be minimized by running final 0/1 reward RL regularized by a dense reward bonus, given by the \"information gain\" from each subsequent block in the output stream. We prescribe an approach for quantifying information gain, which measures the utility of an intermediate segment of tokens towards improving accuracy of the final answer. We instantiate this idea to develop MRT, a new class of finetuning methods for optimizing test-time compute. Fine-tuning with MRT leads to substantial improvements in both performance and token efficiency on the AIME dataset."},"pdf":{"value":"/pdf/2230e0bd79d2ae319ef8594922f073664c4a8137.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nqu2025optimizing,\ntitle={Optimizing Test-Time Compute via Meta Reinforcement Finetuning},\nauthor={Yuxiao Qu and Matthew Y. R. Yang and Amrith Setlur and Lewis Tunstall and Edward Emanuel Beeching and Ruslan Salakhutdinov and Aviral Kumar},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=WGz4ytjo1h}\n}"},"paperhash":{"value":"qu|optimizing_testtime_compute_via_meta_reinforcement_finetuning"}},"id":"WGz4ytjo1h","forum":"WGz4ytjo1h","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission50/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission50/Authors"],"number":50,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738440417565,"cdate":1738440417565,"tmdate":1741166406484,"mdate":1741166406484,"pdate":1741166406464,"odate":1741166406464,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Rationalization Models for Text-to-SQL"},"authors":{"value":["Gaetano Rossiello","Nhan H Pham","Michael Glass","Junkyu Lee","Dharmashankar Subramanian"]},"authorids":{"value":["~Gaetano_Rossiello1","~Nhan_H_Pham1","~Michael_Glass1","~Junkyu_Lee1","~Dharmashankar_Subramanian1"]},"keywords":{"value":["text-to-sql","large language models","chain-of-thought","reasoning"]},"TLDR":{"value":"We introduce a framework designed to facilitate the automatic annotation of text-to-SQL datasets with CoT rationales."},"abstract":{"value":"We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability."},"pdf":{"value":"/pdf/fd713446a941b425c794ae5b2ec89b465f4d7d30.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nrossiello2025rationalization,\ntitle={Rationalization Models for Text-to-{SQL}},\nauthor={Gaetano Rossiello and Nhan H Pham and Michael Glass and Junkyu Lee and Dharmashankar Subramanian},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=QgUVtHcHzI}\n}"},"paperhash":{"value":"rossiello|rationalization_models_for_texttosql"}},"id":"QgUVtHcHzI","forum":"QgUVtHcHzI","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission47/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission47/Authors"],"number":47,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission47/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738430713972,"cdate":1738430713972,"tmdate":1741714762178,"mdate":1741714762178,"pdate":1741166406324,"odate":1741166406324,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization"},"authors":{"value":["Yuchi Liu","Jaskirat Singh","Gaowen Liu","Ali Payani","Liang Zheng"]},"authorids":{"value":["~Yuchi_Liu1","~Jaskirat_Singh1","~Gaowen_Liu4","~Ali_Payani1","~Liang_Zheng4"]},"keywords":{"value":["prompt","large language models"]},"abstract":{"value":"Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications. Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly. Therefore, recent works have developed many strategies for improving the prompt, including both manual crafting and in-domain optimization. However, their efficacy in unrestricted scenarios remains questionable, as the former depends on human design for specific questions and the latter usually generalizes poorly to unseen scenarios. To address these problems, we give LLMs the freedom to design the best prompts according to themselves. Specifically, we include a hierarchy of LLMs, first constructing a prompt with precise instructions and accurate wording in a hierarchical manner, and then using this prompt to generate the final answer to the user query. We term this pipeline Hierarchical Multi-Agent Workflow, or HMAW. In contrast with prior works, HMAW imposes no human restriction and requires no training, and is completely task-agnostic while capable of adjusting to the nuances of the underlying task. Through both quantitative and qualitative experiments across multiple benchmarks, we verify that despite its simplicity, the proposed approach can create detailed and suitable prompts, further boosting the performance of current LLMs."},"pdf":{"value":"/pdf/70597acbddf1be77e39cf9d8664783efc41c4bb2.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nliu2025towards,\ntitle={Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization},\nauthor={Yuchi Liu and Jaskirat Singh and Gaowen Liu and Ali Payani and Liang Zheng},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=RVvXOrP2qm}\n}"},"paperhash":{"value":"liu|towards_hierarchical_multiagent_workflows_for_zeroshot_prompt_optimization"}},"id":"RVvXOrP2qm","forum":"RVvXOrP2qm","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission45/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission45/Authors"],"number":45,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission45/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738415261086,"cdate":1738415261086,"tmdate":1742386182452,"mdate":1742386182452,"pdate":1741166406169,"odate":1741166406169,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Large Language Models to Diffusion Finetuning"},"authors":{"value":["Edoardo Cetin","Tianyu Zhao","Yujin Tang"]},"authorids":{"value":["~Edoardo_Cetin1","~Tianyu_Zhao1","~Yujin_Tang1"]},"keywords":{"value":["language models","large language models","diffusion","finetuning","test-time scaling"]},"TLDR":{"value":"New finetuning method powering large language models with the ability to scale test-time compute and other key properties of the diffusion framework.."},"abstract":{"value":"We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is applicable to any foundation model pre-trained with cross-entropy and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method can be more effective and is fully compatible with traditional finetuning and search approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks."},"pdf":{"value":"/pdf/0e920fd2cc1755df2920553f7b4e8ae911c26f03.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\ncetin2025large,\ntitle={Large Language Models to Diffusion Finetuning},\nauthor={Edoardo Cetin and Tianyu Zhao and Yujin Tang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=Wu5RzXQ9Iy}\n}"},"paperhash":{"value":"cetin|large_language_models_to_diffusion_finetuning"}},"id":"Wu5RzXQ9Iy","forum":"Wu5RzXQ9Iy","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission43/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission43/Authors"],"number":43,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission43/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738402971120,"cdate":1738402971120,"tmdate":1742117041724,"mdate":1742117041724,"pdate":1741166406156,"odate":1741166406156,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction"},"authors":{"value":["Yiheng Xu","Zekun Wang","Junli Wang","Dunjie Lu","Tianbao Xie","Amrita Saha","Doyen Sahoo","Tao Yu","Caiming Xiong"]},"authorids":{"value":["~Yiheng_Xu1","~Zekun_Wang1","~Junli_Wang3","~Dunjie_Lu1","~Tianbao_Xie1","~Amrita_Saha2","~Doyen_Sahoo1","~Tao_Yu5","~Caiming_Xiong1"]},"keywords":{"value":["GUI Agent","Visual Language Model","Large Language Model","Grounding","Planning","Reasoning","Computer Using Agent"]},"abstract":{"value":"Automating GUI tasks remains challenging due to reliance on textual representations, platform-specific action spaces, and limited reasoning capabilities. We introduce Aguvis, a unified vision-based framework for autonomous GUI agents that directly operates on screen images, standardizes cross-platform interactions and incorporates structured reasoning via inner monologue. To enable this, we construct Aguvis Data Collection, a large-scale dataset with multimodal grounding and reasoning annotations, and develop a two-stage training pipeline that separates GUI grounding from planning and reasoning. Experiments show that Aguvisachieves state-of-the-art performance across offline and real-world online benchmarks, marking the first fully autonomous vision-based GUI agent that operates without closed-source models. We open-source all datasets, models, and training recipes to advance future research."},"pdf":{"value":"/pdf/c7aa145005a9d76fcae433a89f92626c067a6e86.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nxu2025aguvis,\ntitle={Aguvis: Unified Pure Vision Agents for Autonomous {GUI} Interaction},\nauthor={Yiheng Xu and Zekun Wang and Junli Wang and Dunjie Lu and Tianbao Xie and Amrita Saha and Doyen Sahoo and Tao Yu and Caiming Xiong},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=5OyOlBLUci}\n}"},"paperhash":{"value":"xu|aguvis_unified_pure_vision_agents_for_autonomous_gui_interaction"}},"id":"5OyOlBLUci","forum":"5OyOlBLUci","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission42/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission42/Authors"],"number":42,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission42/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738402401612,"cdate":1738402401612,"tmdate":1742412465224,"mdate":1742412465224,"pdate":1741166405954,"odate":1741166405954,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling"},"authors":{"value":["Runze Liu","Junqi Gao","Jian Zhao","Kaiyan Zhang","Xiu Li","Biqing Qi","Wanli Ouyang","Bowen Zhou"]},"authorids":{"value":["~Runze_Liu2","~Junqi_Gao1","~Jian_Zhao12","~Kaiyan_Zhang1","~Xiu_Li1","~Biqing_Qi1","~Wanli_Ouyang1","~Bowen_Zhou8"]},"keywords":{"value":["Large Language Model","Test-Time Scaling","Process Reward Model"]},"abstract":{"value":"Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs. Our website is available at https://ryanliu112.github.io/compute-optimal-tts."},"pdf":{"value":"/pdf/41ff29048ed51245000724c810c30d2700345c4f.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nliu2025can,\ntitle={Can 1B {LLM} Surpass 405B {LLM}? Rethinking Compute-Optimal Test-Time Scaling},\nauthor={Runze Liu and Junqi Gao and Jian Zhao and Kaiyan Zhang and Xiu Li and Biqing Qi and Wanli Ouyang and Bowen Zhou},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=CvjX9Lhpze}\n}"},"paperhash":{"value":"liu|can_1b_llm_surpass_405b_llm_rethinking_computeoptimal_testtime_scaling"}},"id":"CvjX9Lhpze","forum":"CvjX9Lhpze","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission41/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission41/Authors"],"number":41,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission41/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738390118143,"cdate":1738390118143,"tmdate":1742468010489,"mdate":1742468010489,"pdate":1741166405852,"odate":1741166405852,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Language Models Use Trigonometry to Do Addition"},"authors":{"value":["Subhash Kantamneni","Max Tegmark"]},"authorids":{"value":["~Subhash_Kantamneni1","~Max_Tegmark1"]},"keywords":{"value":["LLMs","Mechanistic Interpretability","Mathematics","Reasoning"]},"TLDR":{"value":"Language models represent numbers as helices and use the trigonometric \"Clock\" algorithm to manipulate these helices and compute addition."},"abstract":{"value":"Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the “Clock” algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability."},"pdf":{"value":"/pdf/1be1f70868b22ef41118b056edd97f2be434f0c5.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nkantamneni2025language,\ntitle={Language Models Use Trigonometry to Do Addition},\nauthor={Subhash Kantamneni and Max Tegmark},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=HH1hX4S0t8}\n}"},"paperhash":{"value":"kantamneni|language_models_use_trigonometry_to_do_addition"}},"id":"HH1hX4S0t8","forum":"HH1hX4S0t8","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission38/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission38/Authors"],"number":38,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission38/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738362831489,"cdate":1738362831489,"tmdate":1742421657395,"mdate":1742421657395,"pdate":1741166405736,"odate":1741166405736,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"TACO: Learning Multi-modal Models to Reason and Act with Synthetic Chains-of-Thought-and-Action"},"authors":{"value":["Zixian Ma","Jianguo Zhang","Zhiwei Liu","Jieyu Zhang","Juntao Tan","Manli Shu","Juan Carlos Niebles","Shelby Heinecke","Huan Wang","Caiming Xiong","Ranjay Krishna","silvio savarese"]},"authorids":{"value":["~Zixian_Ma1","~Jianguo_Zhang3","~Zhiwei_Liu3","~Jieyu_Zhang1","~Juntao_Tan1","~Manli_Shu1","~Juan_Carlos_Niebles1","~Shelby_Heinecke1","~Huan_Wang1","~Caiming_Xiong1","~Ranjay_Krishna1","~silvio_savarese2"]},"keywords":{"value":["multimodal models","reasoning","tool use"]},"TLDR":{"value":"TACO: Learning Multi-modal Models to Reason and Act with Synthetic Chains-of-Thought-and-Action"},"abstract":{"value":"While open-source multi-modal language models perform well on simple question\nanswering tasks, they often fail on complex questions that require multiple capa-\nbilities, such as fine-grained recognition, visual grounding, and reasoning, and\nthat demand multi-step solutions. We present TACO, a family of multi-modal\nlarge action models designed to improve performance on such complex, multi-\nstep and multi-modal tasks. During inference, TACO produces chains-of-thought-\nand–action (CoTA), executes intermediate steps by invoking external tools such as\nOCR, depth estimation and calculator, then integrates both the thoughts and action\noutputs to produce coherent responses. To train TACO, we create a large dataset\nof 1M+ synthetic CoTA traces generated with GPT-4o and Python programs. We\nthen experiment with various data filtering and mixing techniques and obtain a\nfinal subset of 293K high-quality CoTA examples. This dataset enables TACO to\nlearn complex reasoning and action paths, surpassing existing models trained on\ninstruct tuning data with only direct answers. Our model TACO outperforms the\ninstruction-tuned baseline across 8 benchmarks, achieving a 3.9% improvement\non average, with gains up to 20% in MMVet tasks involving OCR, mathematical\nreasoning and spatial reasoning."},"pdf":{"value":"/pdf/ed9c0205a7d79fbe0639ec449cbcd6602aca5ded.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nma2025taco,\ntitle={{TACO}: Learning Multi-modal Models to Reason and Act with Synthetic Chains-of-Thought-and-Action},\nauthor={Zixian Ma and Jianguo Zhang and Zhiwei Liu and Jieyu Zhang and Juntao Tan and Manli Shu and Juan Carlos Niebles and Shelby Heinecke and Huan Wang and Caiming Xiong and Ranjay Krishna and silvio savarese},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=8fty8ubFmm}\n}"},"paperhash":{"value":"ma|taco_learning_multimodal_models_to_reason_and_act_with_synthetic_chainsofthoughtandaction"}},"id":"8fty8ubFmm","forum":"8fty8ubFmm","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission37/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission37/Authors"],"number":37,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission37/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738361615149,"cdate":1738361615149,"tmdate":1742431070319,"mdate":1742431070319,"pdate":1741166405619,"odate":1741166405619,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"The in-context inductive biases of vision-language models differ across modalities"},"authors":{"value":["Kelsey R Allen","Eliza Kosoy","Ishita Dasgupta","Andrew Kyle Lampinen"]},"authorids":{"value":["~Kelsey_R_Allen1","~Eliza_Kosoy1","~Ishita_Dasgupta1","~Andrew_Kyle_Lampinen1"]},"keywords":{"value":["In-context learning","generalization","multimodality","vision-language models"]},"TLDR":{"value":"The way that vision-language models generalize categories learned in context depends on the modality and format in which they are presented."},"abstract":{"value":"Inductive biases are what allow learners to make guesses in the absence of conclusive evidence. These biases have often been studied in cognitive science using concepts or categories -- e.g. by testing how humans generalize a new category from a few examples that leave the category boundary ambiguous. We use these approaches to study generalization in foundation models during in-context learning. Modern foundation models can condition on both vision and text, and differences in how they interpret and learn from these different modalities is an emerging area of study. Here, we study how their generalizations vary by the modality in which stimuli are presented, and the way the stimuli are described in text. We study these biases with three different experimental paradigms, across three different vision-language models. We find that the models generally show some bias towards generalizing according to shape over color. This shape bias tends to be amplified when the examples are presented visually. By contrast, when examples are presented in text, the ordering of adjectives affects generalization. However, the extent of these effects vary across models and paradigms. These results help to reveal how vision-language models represent different types of inputs in context, and may have practical implications for the use of vision-language models."},"pdf":{"value":"/pdf/017386f6ecb38cae9aa7890d2248e140a3e9227e.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nallen2025the,\ntitle={The in-context inductive biases of vision-language models differ across modalities},\nauthor={Kelsey R Allen and Eliza Kosoy and Ishita Dasgupta and Andrew Kyle Lampinen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ymftzTut3a}\n}"},"paperhash":{"value":"allen|the_incontext_inductive_biases_of_visionlanguage_models_differ_across_modalities"}},"id":"ymftzTut3a","forum":"ymftzTut3a","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission36/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission36/Authors"],"number":36,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission36/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738359976458,"cdate":1738359976458,"tmdate":1741821001272,"mdate":1741821001272,"pdate":1741166405616,"odate":1741166405616,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"TRIG-Bench: A Benchmark for Text-Rich Image Grounding"},"authors":{"value":["Ming Li","Ruiyi Zhang","Jian Chen","Tianyi Zhou"]},"authorids":{"value":["~Ming_Li18","~Ruiyi_Zhang3","~Jian_Chen9","~Tianyi_Zhou1"]},"keywords":{"value":["Large Language Models","Multimodel Large Language Models","Vision Language Models","Visual Grounding"]},"TLDR":{"value":"We introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering."},"abstract":{"value":"Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark TRIG-Bench based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images."},"pdf":{"value":"/pdf/cd110cc29f465b40b8499853aa76c3031444162c.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nli2025trigbench,\ntitle={{TRIG}-Bench: A Benchmark for Text-Rich Image Grounding},\nauthor={Ming Li and Ruiyi Zhang and Jian Chen and Tianyi Zhou},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=IHvTfHasAj}\n}"},"paperhash":{"value":"li|trigbench_a_benchmark_for_textrich_image_grounding"}},"id":"IHvTfHasAj","forum":"IHvTfHasAj","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission35/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission35/Authors"],"number":35,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission35/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738358472513,"cdate":1738358472513,"tmdate":1742000283768,"mdate":1742000283768,"pdate":1741166405577,"odate":1741166405577,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Adaptive Self-improvement LLM Agentic System for ML Library Development"},"authors":{"value":["Genghan Zhang","Weixin Liang","Olivia Hsu","Kunle Olukotun"]},"authorids":{"value":["~Genghan_Zhang1","~Weixin_Liang1","~Olivia_Hsu1","~Kunle_Olukotun1"]},"keywords":{"value":["LLM agents","Self-improvement learning","Machine learning library"]},"TLDR":{"value":"How can we use LLMs to improve the efficiency of themselves? We introduce an LLM agentic system with self-improvement for ML library development using hardware architecture specific language, automatically implementing 25 of 26 key LLM operators."},"abstract":{"value":"ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM."},"pdf":{"value":"/pdf/a263ad5fbfb305140baff71681fbea79e8e1ee19.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhang2025adaptive,\ntitle={Adaptive Self-improvement {LLM} Agentic System for {ML} Library Development},\nauthor={Genghan Zhang and Weixin Liang and Olivia Hsu and Kunle Olukotun},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=ujMjxYUeyl}\n}"},"paperhash":{"value":"zhang|adaptive_selfimprovement_llm_agentic_system_for_ml_library_development"}},"id":"ujMjxYUeyl","forum":"ujMjxYUeyl","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission32/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission32/Authors"],"number":32,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission32/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738346608371,"cdate":1738346608371,"tmdate":1742313249205,"mdate":1742313249205,"pdate":1741166405439,"odate":1741166405439,"version":2,"details":{"writable":false,"replyCount":5,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations"},"authors":{"value":["Anian Ruoss","Fabio Pardo","Harris Chan","Bonnie Li","Volodymyr Mnih","Tim Genewein"]},"authorids":{"value":["~Anian_Ruoss1","~Fabio_Pardo1","~Harris_Chan1","~Bonnie_Li1","~Volodymyr_Mnih1","~Tim_Genewein1"]},"keywords":{"value":["in-context learning","imitation learning","multimodal","long context","reasoning","interactive decision-making"]},"abstract":{"value":"In this paper, we present a benchmark to pressure-test today's frontier models' multimodal decision-making capabilities in the very long-context regime (up to one million tokens) and investigate whether these models can learn from large numbers of expert demonstrations in their context. We evaluate the performance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0 Flash Experimental, GPT-4o, o1-mini, o1-preview, and o1 as policies across a battery of simple interactive decision-making tasks: playing tic-tac-toe, chess, and Atari, navigating grid worlds, solving crosswords, and controlling a simulated cheetah. We study increasing amounts of expert demonstrations in the context — from no demonstrations to 512 full episodes. Across our tasks, models rarely manage to fully reach expert performance, and often, presenting more demonstrations has little effect. Some models steadily improve with more demonstrations on a few tasks. We investigate the effect of encoding observations as text or images and the impact of chain-of-thought prompting. To help quantify the impact of other approaches and future innovations, we open-source our benchmark that covers the zero-, few-, and many-shot regimes in a unified evaluation."},"pdf":{"value":"/pdf/89d5811ba532327bd21c9a20d158b7385611f0e7.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nruoss2025a,\ntitle={A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations},\nauthor={Anian Ruoss and Fabio Pardo and Harris Chan and Bonnie Li and Volodymyr Mnih and Tim Genewein},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=0xvSZPcZdT}\n}"},"paperhash":{"value":"ruoss|lmact_a_benchmark_for_incontext_imitation_learning_with_long_multimodal_demonstrations"}},"id":"0xvSZPcZdT","forum":"0xvSZPcZdT","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission31/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission31/Authors"],"number":31,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission31/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738338579747,"cdate":1738338579747,"tmdate":1742438391955,"mdate":1742438391955,"pdate":1741166405211,"odate":1741166405211,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"LogitGaze: Predicting Human Attention Using Semantic Information from Vision-Language Models"},"authors":{"value":["Dmitry Lvov","Ilya Pershin"]},"authorids":{"value":["~Dmitry_Lvov1","~Ilya_Pershin1"]},"keywords":{"value":["Scanpath prediction","visual attention","vision-language models","LLMs","multimodal reasoning","semantic priors","gaze modeling","logit-lens","fixation prediction","interpretability"]},"TLDR":{"value":"We enhance scanpath prediction by integrating semantic cues from VLMs. This multimodal approach refines fixation modeling, improves prediction accuracy by ~15%, and deepens our understanding of visual attention dynamics"},"abstract":{"value":"Modeling human scanpaths remains a challenging task due to the complexity of visual attention dynamics. Traditional approaches rely on low-level visual features, but they often fail to capture the semantic and contextual factors that guide human gaze. To address this, we propose a novel method that integrates LLMs and VLMs to enrich scanpath prediction with semantic priors. By leveraging word-level representations extracted through interpretability tools like the logit lens, our approach aligns spatial-temporal gaze patterns with high-level scene semantics. Our method establishes a new state of the art, improving all key scanpath prediction metrics by approximately 15% on average, demonstrating the effectiveness of integrating linguistic and visual knowledge for enhanced gaze modeling."},"pdf":{"value":"/pdf/acbc2a01772799516ea8d703f5f88fd8b57744d8.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nlvov2025logitgaze,\ntitle={LogitGaze: Predicting Human Attention Using Semantic Information from Vision-Language Models},\nauthor={Dmitry Lvov and Ilya Pershin},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=Om4j6IVb42}\n}"},"paperhash":{"value":"lvov|logitgaze_predicting_human_attention_using_semantic_information_from_visionlanguage_models"}},"id":"Om4j6IVb42","forum":"Om4j6IVb42","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission30/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission30/Authors"],"number":30,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission30/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738333817630,"cdate":1738333817630,"tmdate":1742389598916,"mdate":1742389598916,"pdate":1741166405131,"odate":1741166405131,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks"},"authors":{"value":["Fangru Lin","Shaoguang Mao","Emanuele La Malfa","Valentin Hofmann","Adrian de Wynter","Xun Wang","Si-Qing Chen","Michael J. Wooldridge","Janet B. Pierrehumbert","Furu Wei"]},"authorids":{"value":["~Fangru_Lin1","~Shaoguang_Mao1","~Emanuele_La_Malfa2","~Valentin_Hofmann1","~Adrian_de_Wynter1","~Xun_Wang5","~Si-Qing_Chen1","~Michael_J._Wooldridge1","~Janet_B._Pierrehumbert1","~Furu_Wei1"]},"keywords":{"value":["LLM","reasoning","planning","robustness","dialect"]},"TLDR":{"value":"LLMs are brittle to reasoning and planning prompts when they are expressed in non-standardized dialect"},"abstract":{"value":"Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language variation, and thus fail to model the experience of speakers of non-standard dialects. \nFocusing on African American Vernacular English (AAVE), we present the first study aimed at objectively assessing the fairness and robustness of LLMs in handling dialects in canonical reasoning tasks, including algorithm, math, logic, and integrated reasoning. We introduce \\textbf{ReDial} (\\textbf{Re}asoning with \\textbf{Dial}ect Queries), a benchmark containing 1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE speakers, including experts with computer science backgrounds, to rewrite seven popular benchmarks,\nsuch as HumanEval and GSM8K. With ReDial, we evaluate widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model families. Our findings reveal that \\textbf{almost all of these widely used models show significant brittleness and unfairness to queries in AAVE}. Our work establishes a systematic and objective framework for analyzing LLM bias in dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair service to dialect speakers in reasoning tasks, laying a critical foundation for relevant future research."},"pdf":{"value":"/pdf/9c62372e2b55a472df6c8b1c67c336abb5fc0cea.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nlin2025assessing,\ntitle={Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks},\nauthor={Fangru Lin and Shaoguang Mao and Emanuele La Malfa and Valentin Hofmann and Adrian de Wynter and Xun Wang and Si-Qing Chen and Michael J. Wooldridge and Janet B. Pierrehumbert and Furu Wei},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=3YyyiyV4B6}\n}"},"paperhash":{"value":"lin|assessing_dialect_fairness_and_robustness_of_large_language_models_in_reasoning_tasks"}},"id":"3YyyiyV4B6","forum":"3YyyiyV4B6","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission29/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission29/Authors"],"number":29,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission29/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738327405232,"cdate":1738327405232,"tmdate":1742297328982,"mdate":1742297328982,"pdate":1741166405129,"odate":1741166405129,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Benchmarking Agentic Workflow Generation"},"authors":{"value":["Shuofei Qiao","Runnan Fang","Zhisong Qiu","Xiaobin Wang","Ningyu Zhang","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"]},"authorids":{"value":["~Shuofei_Qiao1","~Runnan_Fang1","~Zhisong_Qiu1","~Xiaobin_Wang1","~Ningyu_Zhang1","~Yong_Jiang1","~Pengjun_Xie2","~Fei_Huang2","~Huajun_Chen1"]},"keywords":{"value":["workflow generation","graph structured planning","large language model","agent"]},"TLDR":{"value":"A unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures."},"abstract":{"value":"Large Language Models (LLMs), with their remarkable task-handling capabilities, have catalyzed significant achievements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference."},"pdf":{"value":"/pdf/e92691ffd925449f1b85df69febc1ad1a21fb301.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nqiao2025benchmarking,\ntitle={Benchmarking Agentic Workflow Generation},\nauthor={Shuofei Qiao and Runnan Fang and Zhisong Qiu and Xiaobin Wang and Ningyu Zhang and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=1VU7zLtQyW}\n}"},"paperhash":{"value":"qiao|benchmarking_agentic_workflow_generation"}},"id":"1VU7zLtQyW","forum":"1VU7zLtQyW","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission26/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission26/Authors"],"number":26,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission26/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738308919820,"cdate":1738308919820,"tmdate":1742388026941,"mdate":1742388026941,"pdate":1741166404948,"odate":1741166404948,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"EcoAct: Economic Agent Determines When to Register What Action"},"authors":{"value":["Shaokun Zhang","Jieyu Zhang","Dujian Ding","Jiale Liu","Mirian Del Carmen Hipolito Garcia","Ankur Mallick","Daniel Madrigal","Menglin Xia","Victor Rühle","Qingyun Wu","Chi Wang"]},"authorids":{"value":["~Shaokun_Zhang2","~Jieyu_Zhang1","~Dujian_Ding1","~Jiale_Liu2","~Mirian_Del_Carmen_Hipolito_Garcia1","~Ankur_Mallick1","~Daniel_Madrigal1","~Menglin_Xia1","~Victor_Rühle1","~Qingyun_Wu2","~Chi_Wang3"]},"keywords":{"value":["LLM agents"]},"abstract":{"value":"Recent advancements have enabled Large Language Models (LLMs) to function as agents that can perform actions using external tools. \nThis requires registering, i.e. integrating tool information into the LLM context prior to taking actions. Current methods indiscriminately incorporate all candidate tools into the agent’s context and retain them across multiple reasoning steps. This process remains opaque to LLM agents and is not integrated into their reasoning procedures, leading to inefficiencies due to increased context length from irrelevant tools.  To address this, we introduce EcoAct, a simple but effective tool-using algorithm that allows LLMs to selectively register tools as needed, optimizing context use. By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50\\% in multi-step reasoning tasks while maintaining performance, as demonstrated through extensive experiments. \nMoreover, it can be plugged into any reasoning pipeline with only minor modifications to the prompt, making it universally applicable to LLM agents now and in the future."},"pdf":{"value":"/pdf/040f3c72bbbfcd6a8e98fc5a705f8b3d445b4895.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nzhang2025ecoact,\ntitle={EcoAct: Economic Agent Determines When to Register What Action},\nauthor={Shaokun Zhang and Jieyu Zhang and Dujian Ding and Jiale Liu and Mirian Del Carmen Hipolito Garcia and Ankur Mallick and Daniel Madrigal and Menglin Xia and Victor R{\\\"u}hle and Qingyun Wu and Chi Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=IHgVuYwhnz}\n}"},"paperhash":{"value":"zhang|ecoact_economic_agent_determines_when_to_register_what_action"}},"id":"IHgVuYwhnz","forum":"IHgVuYwhnz","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission25/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission25/Authors"],"number":25,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission25/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738304246357,"cdate":1738304246357,"tmdate":1742441241704,"mdate":1742441241704,"pdate":1741166404904,"odate":1741166404904,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context Learning"},"authors":{"value":["Chengsong Huang","Langlin Huang","Jiaxin Huang"]},"authorids":{"value":["~Chengsong_Huang1","~Langlin_Huang1","~Jiaxin_Huang1"]},"keywords":{"value":["efficient inference","in-context learning","efficient reasoning"]},"TLDR":{"value":"We propose a non-gradient method to improve the performance and reduce the memory usage of ICL by reweighting the given examples"},"abstract":{"value":"In-Context Learning (ICL) emerges as a key feature for Large Language Models (LLMs), allowing them to adapt to new tasks by leverageing task-specific examples without updating model parameters. However, ICL faces challenges with increasing numbers of examples due to performance degradation and quadratic computational costs. In this paper, we propose Logit Arithmetic Reweighting Approach (LARA), a novel framework that enhances ICL by using logit-based ensembling of multiple demonstrations. Our approach divides long input demonstrations into parallelizable shorter inputs to significantly reduce memory requirements, and then effectively aggregate the information by reweighting logits of each group via a non-gradient optimization approach. We further introduce Bi- nary LARA (B-LARA), a variant that constrains weights to binary values to simplify the search space and reduces memory usage by filtering out less informative demonstration groups. Experiments on BBH and MMLU demonstrate that LARA and B-LARA outperform all baseline methods in both accuracy and memory efficiency. We also conduct extensive analysis to show that LARA generalizes well to scenarios of varying numbers of examples from limited to many-shot demonstrations. Our codes can be found in https://anonymous.4open.science/r/LARA-F55B."},"pdf":{"value":"/pdf/ee0a186a10bed61cade7b94a7dc58f3ab9aeccb9.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nhuang2025divide,\ntitle={Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context Learning},\nauthor={Chengsong Huang and Langlin Huang and Jiaxin Huang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=6YWcXXAPe6}\n}"},"paperhash":{"value":"huang|divide_reweight_and_conquer_a_logit_arithmetic_approach_for_incontext_learning"}},"id":"6YWcXXAPe6","forum":"6YWcXXAPe6","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission24/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission24/Authors"],"number":24,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission24/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738299291767,"cdate":1738299291767,"tmdate":1742236086252,"mdate":1742236086252,"pdate":1741166404807,"odate":1741166404807,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization"},"authors":{"value":["Zishun Yu","Tengyu Xu","Di Jin","Karthik Abinav Sankararaman","Yun He","Wenxuan Zhou","Zhouhao Zeng","Eryk Helenowski","Chen Zhu","Sinong Wang","Hao Ma","Han Fang"]},"authorids":{"value":["~Zishun_Yu1","~Tengyu_Xu1","~Di_Jin1","~Karthik_Abinav_Sankararaman1","~Yun_He2","~Wenxuan_Zhou1","~Zhouhao_Zeng1","~Eryk_Helenowski1","~Chen_Zhu2","~Sinong_Wang1","~Hao_Ma1","~Han_Fang4"]},"keywords":{"value":["large language models","reasoning","inference compute management","reinforcement learning"]},"abstract":{"value":"Solving mathematics problems has been an intriguing capability of language models, and many efforts have been made to improve reasoning by extending reasoning length, such as through self-correction and extensive long chain-of-thoughts. While promising in problem-solving, advanced long reasoning chain models exhibit an undesired uni-modal behavior, where trivial questions require unnecessarily tedious long chains of thought. In this work, we propose a way to allow models to be aware of inference budgets by formulating it as utility maximization with respect to an inference budget constraint, hence naming our algorithm Inference Budget-Constrained Policy Optimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to ``understand'' the difficulty of queries and allocate inference budgets to harder ones. With different inference budgets, our best models are able to have a $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative) on MATH500 using $2.16$x and $4.32$x inference budgets respectively, relative to LLaMA3.1 8B Instruct. These improvements are approximately $2$x those of self-consistency under the same budgets."},"pdf":{"value":"/pdf/44dbf67f9b4cf4693f474c30ce1e3bece8b31ef5.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nyu2025think,\ntitle={Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization},\nauthor={Zishun Yu and Tengyu Xu and Di Jin and Karthik Abinav Sankararaman and Yun He and Wenxuan Zhou and Zhouhao Zeng and Eryk Helenowski and Chen Zhu and Sinong Wang and Hao Ma and Han Fang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=GA9Jbu5wQH}\n}"},"paperhash":{"value":"yu|think_smarter_not_harder_adaptive_reasoning_with_inference_aware_optimization"}},"id":"GA9Jbu5wQH","forum":"GA9Jbu5wQH","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission23/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission23/Authors"],"number":23,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission23/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738297533596,"cdate":1738297533596,"tmdate":1742159944766,"mdate":1742159944766,"pdate":1741166404803,"odate":1741166404803,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning"},"authors":{"value":["Kai Yan","Zhan Ling","Kang Liu","Yifan Yang","Ting-Han Fan","Lingfeng Shen","Zhengyin Du","Jiecao Chen"]},"authorids":{"value":["~Kai_Yan1","~Zhan_Ling2","~Kang_Liu17","~Yifan_Yang26","~Ting-Han_Fan1","~Lingfeng_Shen1","~Zhengyin_Du1","~Jiecao_Chen1"]},"keywords":{"value":["inductive reasoning","many-shot in-context learning","large language model"]},"TLDR":{"value":"We propose a novel benchmark for many-shot inductive reasoning for large language models."},"abstract":{"value":"Inductive Reasoning (IR), the ability to summarize rules from existing examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they all focus on few-shot (typically \n10) setting, which lacks evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) are more of tracking tasks instead of ones that require intelligence. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse input-output format. Based on such a benchmark, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings."},"pdf":{"value":"/pdf/02f378ab2910f45e9e5829d5421fa75cd12bfeed.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nyan2025mirbench,\ntitle={{MIR}-Bench: Benchmarking {LLM}'s Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning},\nauthor={Kai Yan and Zhan Ling and Kang Liu and Yifan Yang and Ting-Han Fan and Lingfeng Shen and Zhengyin Du and Jiecao Chen},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=dwQ0AYwT0q}\n}"},"paperhash":{"value":"yan|mirbench_benchmarking_llms_longcontext_intelligence_via_manyshot_incontext_inductive_reasoning"}},"id":"dwQ0AYwT0q","forum":"dwQ0AYwT0q","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission19/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission19/Authors"],"number":19,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission19/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738196338471,"cdate":1738196338471,"tmdate":1742254116365,"mdate":1742254116365,"pdate":1741166404490,"odate":1741166404490,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search"},"authors":{"value":["Zongyu Lin","Yao Tang","Xingcheng Yao","Da Yin","Ziniu Hu","Yizhou Sun","Kai-Wei Chang"]},"authorids":{"value":["~Zongyu_Lin1","~Yao_Tang2","~Xingcheng_Yao1","~Da_Yin2","~Ziniu_Hu1","~Yizhou_Sun1","~Kai-Wei_Chang1"]},"keywords":{"value":["Agent","process reward model","Q-learning"]},"abstract":{"value":"Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing an exploration tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data."},"pdf":{"value":"/pdf/6e697f7cb97f85f5009524f38c25b43ac6d1b7ae.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nlin2025qlass,\ntitle={{QLASS}: Boosting Language Agent Inference via Q-Guided Stepwise Search},\nauthor={Zongyu Lin and Yao Tang and Xingcheng Yao and Da Yin and Ziniu Hu and Yizhou Sun and Kai-Wei Chang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=fPXDrhI9d6}\n}"},"paperhash":{"value":"lin|qlass_boosting_language_agent_inference_via_qguided_stepwise_search"}},"id":"fPXDrhI9d6","forum":"fPXDrhI9d6","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission18/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission18/Authors"],"number":18,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission18/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738187373213,"cdate":1738187373213,"tmdate":1742135616814,"mdate":1742135616814,"pdate":1741166404388,"odate":1741166404388,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Spectral Journey: How Transformers Predict the Shortest Path"},"authors":{"value":["Andrew Cohen","Andrey Gromov","Kaiyu Yang","Yuandong Tian"]},"authorids":{"value":["~Andrew_Cohen4","~Andrey_Gromov1","~Kaiyu_Yang1","~Yuandong_Tian1"]},"keywords":{"value":["Language models","Graph algorithms","interpretability"]},"TLDR":{"value":"We train 2-layer transformers to predict shortest paths on simple connected graphs and reverse-engineer the algorithm they learn."},"abstract":{"value":"Decoder-only transformers lead to a step-change in capability of large language models. However, opinions are mixed as to whether they are really planning or reasoning. A path to making progress in this direction is to study the model's behavior in a setting with carefully controlled data. Then interpret the learned representations and reverse-engineer the computation performed internally. We study decoder-only transformer language models trained from scratch to predict shortest paths on simple, connected and undirected graphs. In this setting the representations and the algorithm learned by the model are completely interpretable. We present three major results: (1) Two-layer decoder-only language models can learn to predict shortest paths on simple, connected graphs containing up to $10$ nodes. (2) Models learn a graph embedding that is correlated with the spectral decomposition of the \\emph{line graph}. (3) A new, approximate path-finding algorithm Spectral Line Navigator that relies on the spectral decomposition of the line graph to compute shortest paths."},"pdf":{"value":"/pdf/f2eae0fa7fbe8100aff118fb99ba7c0ac118e513.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\ncohen2025spectral,\ntitle={Spectral Journey: How Transformers Predict the Shortest Path},\nauthor={Andrew Cohen and Andrey Gromov and Kaiyu Yang and Yuandong Tian},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=mAEsGkITgG}\n}"},"paperhash":{"value":"cohen|spectral_journey_how_transformers_predict_the_shortest_path"}},"id":"mAEsGkITgG","forum":"mAEsGkITgG","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission17/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission17/Authors"],"number":17,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission17/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1738176559554,"cdate":1738176559554,"tmdate":1742429304091,"mdate":1742429304091,"pdate":1741166404352,"odate":1741166404352,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension"},"authors":{"value":["Xiyao Wang","Zhengyuan Yang","Linjie Li","Hongjin Lu","Yuancheng Xu","Chung-Ching Lin","Kevin Lin","Furong Huang","Lijuan Wang"]},"authorids":{"value":["~Xiyao_Wang1","~Zhengyuan_Yang1","~Linjie_Li1","~Hongjin_Lu1","~Yuancheng_Xu1","~Chung-Ching_Lin2","~Kevin_Lin3","~Furong_Huang1","~Lijuan_Wang1"]},"keywords":{"value":["Descriptive captioning; Inference-time search; Vision language model; Reward Model; Value Network"]},"abstract":{"value":"Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs."},"pdf":{"value":"/pdf/cb72e41291835bd65bfc8c3b117684790e8c8a15.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nwang2025scaling,\ntitle={Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension},\nauthor={Xiyao Wang and Zhengyuan Yang and Linjie Li and Hongjin Lu and Yuancheng Xu and Chung-Ching Lin and Kevin Lin and Furong Huang and Lijuan Wang},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=yBNkBwt1Tw}\n}"},"paperhash":{"value":"wang|scaling_inferencetime_search_with_vision_value_model_for_improved_visual_comprehension"}},"id":"yBNkBwt1Tw","forum":"yBNkBwt1Tw","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission6/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission6/Authors"],"number":6,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission6/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1736284674658,"cdate":1736284674658,"tmdate":1742450713464,"mdate":1742450713464,"pdate":1741166403798,"odate":1741166403798,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"A Simple Model of Inference Scaling Laws"},"authors":{"value":["Noam Itzhak Levi"]},"authorids":{"value":["~Noam_Itzhak_Levi1"]},"keywords":{"value":["Scaling Laws","Inference Scaling","Maths","Coding","LLMs."]},"TLDR":{"value":"We provide a simple predictive model for the functional behavior of inference with increasing number of attempts."},"abstract":{"value":"Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. \nWe then define an \"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws."},"pdf":{"value":"/pdf/29e5a3cdf140b5106f1386f31e243e05d9cc816d.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nlevi2025a,\ntitle={A Simple Model of Inference Scaling Laws},\nauthor={Noam Itzhak Levi},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=Xcwye2tYnB}\n}"},"paperhash":{"value":"levi|a_simple_model_of_inference_scaling_laws"}},"id":"Xcwye2tYnB","forum":"Xcwye2tYnB","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission4/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission4/Authors"],"number":4,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission4/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1736076918391,"cdate":1736076918391,"tmdate":1741705379337,"mdate":1741705379337,"pdate":1741166403636,"odate":1741166403636,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}},{"content":{"title":{"value":"Reasoning3D - Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models"},"authors":{"value":["Tianrun Chen","Chunan Yu","Jing Li","Jianqi Zhang","Lanyun Zhu","Deyi Ji","Yong Zhang","Ying Zang","Lingyun Sun","Zejian Li"]},"authorids":{"value":["~Tianrun_Chen1","~Chunan_Yu2","~Jing_Li40","~Jianqi_Zhang4","~Lanyun_Zhu1","~Deyi_Ji2","~Yong_Zhang15","~Ying_Zang2","~Lingyun_Sun1","~Zejian_Li1"]},"keywords":{"value":["Reasoning Segmentation","3D Segmentation","3D Model Parsing","3D Part Understanding","Large Language Model","Large Vision-Language Model","Computer-Human Interaction"]},"abstract":{"value":"In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation, a new paradigm in 3D segmentation that goes beyond traditional category-specific methods. We propose a baseline method, Reasoning3D, that leverages pre-trained 2D segmentation networks powered by Large Language Models (LLMs) to interpret user queries and segment 3D meshes with contextual awareness. This approach enables fine-grained part segmentation and generates natural language explanations without requiring extensive 3D datasets. Experiments demonstrate that Reasoning3D can effectively localize and highlight parts of 3D objects. Our training-free method allows rapid deployment and serves as a universal baseline for future research in various fields such as robotics, object manipulation, autonomous driving, AR/VR, and medical applications. The code and the user interface have been released publicly."},"pdf":{"value":"/pdf/b3a797f4a0504be6aba49b7f3d768cb8f7b23aef.pdf"},"venue":{"value":"Reasoning and Planning for LLMs @ ICLR2025"},"venueid":{"value":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan"},"_bibtex":{"value":"@inproceedings{\nchen2025reasoningd,\ntitle={Reasoning3D - Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models},\nauthor={Tianrun Chen and Chunan Yu and Jing Li and Jianqi Zhang and Lanyun Zhu and Deyi Ji and Yong Zhang and Ying Zang and Lingyun Sun and Zejian Li},\nbooktitle={Workshop on Reasoning and Planning for Large Language Models},\nyear={2025},\nurl={https://openreview.net/forum?id=CHwCmiYUi5}\n}"},"paperhash":{"value":"chen|reasoning3d_grounding_and_reasoning_in_3d_finegrained_zeroshot_openvocabulary_3d_reasoning_part_segmentation_via_large_visionlanguage_models"}},"id":"CHwCmiYUi5","forum":"CHwCmiYUi5","license":"CC BY 4.0","signatures":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission1/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission1/Authors"],"number":1,"invitations":["ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Post_Submission","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/-/Edit","ICLR.cc/2025/Workshop/LLM_Reason_and_Plan/Submission1/-/Revision"],"domain":"ICLR.cc/2025/Workshop/LLM_Reason_and_Plan","tcdate":1736005975338,"cdate":1736005975338,"tmdate":1742284736801,"mdate":1742284736801,"pdate":1741166403635,"odate":1741166403635,"version":2,"details":{"writable":false,"replyCount":0,"presentation":[{"name":"title","order":1,"type":"string"},{"name":"keywords","order":4,"type":"string[]"},{"name":"TLDR","order":5,"type":"string","fieldName":"TL;DR"},{"name":"abstract","order":6,"type":"string","input":"textarea","markdown":true},{"name":"pdf","order":7,"type":"file"},{"name":"authors"},{"name":"authorids"},{"name":"venue","hidden":true},{"name":"venueid","hidden":true},{"name":"_bibtex","type":"string","input":"textarea"}]}}],"count":110,"fromCache":true}